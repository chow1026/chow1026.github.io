<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title> =^..^= MEH (descriptive-statistics)</title><link>https://chowy1026.github.io/</link><description></description><atom:link type="application/rss+xml" rel="self" href="https://chowy1026.github.io/tags/descriptive-statistics.xml"></atom:link><language>en</language><lastBuildDate>Tue, 20 Sep 2016 07:41:56 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Descriptive Statistics - Sampling Distributions</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-7/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 7: Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;The Central Limit Theorem allows us to know where any sample mean falls on a distribution of sample means.  The Central Limit Theorem is used to help us understand the following facts regardless of
whether the population distribution is normal or not:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The mean of the sample means is the same as the population mean, i.e.
\[ M = \mu} \] where M is mean of sample means, \(\mu\) is mean of population&lt;/li&gt;
&lt;li&gt;The Standard Error, which is the standard deviation of the Sample Means (also known as Sampling Distribution) is estimated by the following formula:
\[ SE = \frac{\sigma}{ \sqrt{n}} \]&lt;/li&gt;
&lt;li&gt;The distribution of sample means will become increasingly more normal as the sample size, n,  increases.&lt;/li&gt;
&lt;li&gt;Sampling Distribution often form a normal distribution.  &lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>descriptive-statistics</category><category>sampling distribution</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-7/</guid><pubDate>Fri, 19 Aug 2016 02:36:43 GMT</pubDate></item><item><title>Descriptive Statistics - Standardizing</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-5/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 5: Standardizing&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;If we are concerned about a proportion less than or greater than a certain value on a distribution, we should use RELATIVE FREQUENCIES.&lt;/li&gt;
&lt;li&gt;When a bin-size is too big, we can't tell for sure the detailed values of the bin; when a bin-size is too small, the distribution becomes more and more of a uniform distribution.  A predicament: We want a small bin size to have as much details as possible about the location of data values, relative to the rest of the distribution.  &lt;/li&gt;
&lt;li&gt;We solve this conundrum with a theoretical model of our distribution.&lt;/li&gt;
&lt;li&gt;Z score: how many standard deviation any number, x, is away from the mean.  We can convert any value from a normal distribution to a z score.  When we do this we standardize the distribution.  &lt;ul&gt;
&lt;li&gt;\[ z = \frac{ x - \mu }{\sigma} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A negative z score means:&lt;ul&gt;
&lt;li&gt;The original value, \( x \) is less than the mean, \( \mu \).&lt;/li&gt;
&lt;li&gt;The original value, \( x \) minus the mean, \( \mu \) is negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;On a standardized distribution, the mean, \( \mu \) is 0.  &lt;/li&gt;
&lt;li&gt;On a standardized distribution, the standard deviation, \( \sigma \) is 1.&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>descriptive-statistics</category><category>standardizing</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-5/</guid><pubDate>Thu, 18 Aug 2016 02:09:35 GMT</pubDate></item><item><title>Descriptive Statistics - Variability</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-4/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 4: Variability&lt;/h3&gt;
&lt;h4&gt;General Ideas of Variability&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Range&lt;/strong&gt;: The maximum data point minus the minimum data point.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wide Spread&lt;/strong&gt;: The data set has shorter and wider distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: When a data is more consistent, it forms taller and narrower distribution.  &lt;/li&gt;
&lt;li&gt;Outlier(s) usually widen the range of distribution and INCREASES variability.  Statisticians usually cut off outliers.&lt;/li&gt;
&lt;li&gt;1st Quartile, &lt;strong&gt;\( Q_{1} \)&lt;/strong&gt;, is the 25% of the distribution, while 3rd Quartile, &lt;strong&gt;\( Q_{3} \)&lt;/strong&gt;, is the 75% of the distribution.  2nd Quartile, also known as Median, is the 50% of the distribution.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interquartile Range&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;\(IQR =  Q_{3} - Q_{1} \)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facts about &lt;strong&gt;\(IQR \)&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;About 50% of data lies between IQR.&lt;/li&gt;
&lt;li&gt;IQR may or may not be affected by every value in the dataset.  &lt;/li&gt;
&lt;li&gt;IQR is NOT affected by outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A data point is considered an outlier if any of the below is true:&lt;ul&gt;
&lt;li&gt;\[ x_{outlier} &amp;lt; Q_{1} - 1.5*IQR \]&lt;/li&gt;
&lt;li&gt;\[ x_{outlier} &amp;gt; Q_{3} - 1.5*IQR \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Box Plots&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Key Elements of Box Plots:&lt;ul&gt;
&lt;li&gt;Max, Min (two handles of the box)&lt;/li&gt;
&lt;li&gt;\( Q_{1} \), \( Q_{3} \), \( Q_{2} \) (aka Median) forms the box, with Median in the center "splitting" the box.&lt;/li&gt;
&lt;li&gt;Outliers, drawn as dots next to /outside of Max and/or Min.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Box plots sometimes don't tell about the distribution model of data.  The box plot for normal, bimodal, or uniform distribution are the same.&lt;/li&gt;
&lt;li&gt;Although usually the mean lies between \( Q_{1} \) and \( Q_{3} \), it may lie outside of it, if there are outliers.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Measuring Variability&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;To measure variability, we find the average distance between each data value and the mean.&lt;/li&gt;
&lt;li&gt;\[ Deviation, \delta = x_{i} - \bar{x} \]&lt;/li&gt;
&lt;li&gt;Average Deviation, \(\bar{\delta}\):&lt;ul&gt;
&lt;li&gt;\[ \bar{\delta} = \frac{ \sum_{i=0}^N{  \lvert{x_{i} - \bar{x} } \rvert }}{N}\]&lt;/li&gt;
&lt;li&gt;\[ \bar{\delta} = \frac{ \sum_{i=0}^N{\lvert\bar{x} - x_{i}\rvert}}{N} \]&lt;/li&gt;
&lt;li&gt;\[ \bar{\delta} = \sum_{i=0}^N{\frac{ \lvert \bar{x} - x_{i} \rvert }{N}} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Squared Deviation, \( \delta^2 \):&lt;ul&gt;
&lt;li&gt;\[ \delta^2 = {\lvert{x_{i} - \bar{x} } \rvert}^2 \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sum of Squares, \( \sum_{i=0}^N(\delta^2) \), aka \( SS \):&lt;ul&gt;
&lt;li&gt;\[ SS = \sum_{i=0}^N{({x_{i} - \bar{x}})^2} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Variance, \( \sigma^2 \), which is simply the Mean of \( SS \), \( \bar{SS} \) is:&lt;ul&gt;
&lt;li&gt;\[ \sigma^2 = \frac{\sum_{i=0}^N{{({x_{i} - \bar{x}})}^2}}{N} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Standard Deviation, \( \sigma \), is the square root of variance.  &lt;ul&gt;
&lt;li&gt;\[ \sigma = \sqrt{\frac{\sum_{i=0}^N{{({x_{i} - \bar{x}})}^2}}{N}} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why Standard Deviation? In a perfectly normal distribution where mean equals to median equals to mode, 68% of the data values would fall between \( -1 \sigma \) and \( +1 \sigma \), 95% of the data values will fall between \( -2 \sigma \) and \( +2 \sigma \)&lt;/li&gt;
&lt;li&gt;When a sample is taken from a population, the sample standard deviation is usually smaller than true standard deviation, \( \sigma \), of the population.  To correct that, we apply Bessel's Correction.  The corrected/estimation sample standard deviation, denoted as \( s \), and corresponding variance, will have the formulas below:&lt;ul&gt;
&lt;li&gt;\[ s^2 = \frac{\sum_{i=0}^N{{({x_{i} - \bar{x}})}^2}}{N -1} \]&lt;/li&gt;
&lt;li&gt;\[ s = \sqrt{\frac{\sum_{i=0}^N{{({x_{i} - \bar{x}})}^2}}{N-1}} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>descriptive-statistics</category><category>variability</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-4/</guid><pubDate>Wed, 17 Aug 2016 03:29:46 GMT</pubDate></item><item><title>Descriptive Statistics - Central Tendency</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-3/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 3: Central Tendency&lt;/h3&gt;
&lt;h4&gt;Some Definitions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Mode&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Mode is always on x-axis&lt;/li&gt;
&lt;li&gt;Mode can be used to describe any type of data we have, be it numerical or categorical&lt;/li&gt;
&lt;li&gt;All scores in a data set affects the mode.&lt;/li&gt;
&lt;li&gt;Even if we take a lot of samples from the same population, the mode will NOT be the same in each sample.&lt;/li&gt;
&lt;li&gt;There is NO equation for mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mean&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;All scores in a distribution affects a mean&lt;/li&gt;
&lt;li&gt;The mean can be describe with a formula.  As notated below:&lt;ul&gt;
&lt;li&gt;\[ \mu  = \frac{ \sum{x} }{N} \]&lt;/li&gt;
&lt;li&gt;\[ \bar{x} = \frac{ \sum_{i=0}^{n}{x} }{n}\]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Many samples from the same population will have similar means.&lt;/li&gt;
&lt;li&gt;The means of a sample can be used to make inferences of a population it came from.&lt;/li&gt;
&lt;li&gt;The mean will change if we add an extreme value to the data set.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Median&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Median is the middle of the data.&lt;/li&gt;
&lt;li&gt;When there is an even number of data points, the median is the average of the two middle scores, after arranged in ascending/descending order.  &lt;/li&gt;
&lt;li&gt;Median is generally Robust - Strong and Steady, Doesn't affected much by the outlier data.  &lt;/li&gt;
&lt;li&gt;Median serves as a better measurement for the center of the data when the data distribution is heavily skewed.  &lt;/li&gt;
&lt;li&gt;If dataset has EVEN number, n, of data,&lt;ul&gt;
&lt;li&gt;\[  \frac{X_{\frac{n}{2}} + X_{\frac{n}{2} + 1}}{2} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If dataset has ODD number, n, of data,&lt;ul&gt;
&lt;li&gt;\[ X_{\frac{n}{2} + 1} \]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conclusions of Measures of Center&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Mean, Mode and Median are measure of center for datasets.&lt;/li&gt;
&lt;li&gt;Mean has a simple equation.&lt;/li&gt;
&lt;li&gt;Mean will always change if any data value changes.&lt;/li&gt;
&lt;li&gt;While Mode IS affected by bin size, both Mean and Median are NOT affected by bin size.&lt;/li&gt;
&lt;li&gt;While Mean IS severely affected by outliers, Median and Mode are NOT severely affected by outliers.&lt;/li&gt;
&lt;li&gt;Mode is easy to find on histogram.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>central tendency</category><category>descriptive-statistics</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-3/</guid><pubDate>Tue, 16 Aug 2016 07:09:38 GMT</pubDate></item><item><title>Descriptive Statistics - Research Methods</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-1/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 1: Introduction to Research Methods&lt;/h3&gt;
&lt;h4&gt;Some Definitions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constructs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contruct&lt;/strong&gt;: Anything that is difficult to measure because it can be defined and measured in very many different ways.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operational Definition&lt;/strong&gt;: Operational Definitions {of a construct} is a unit of measurement for a construct.  Once an Operational Definitions is defined {for a construct}, it is no longer a construct.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extraneous Factors&lt;/strong&gt;: Also known as "Lurking Variables", these factors that could impact the outcome of an experiment or test.  Extraneous factors should be held constant as much as possible to maximize accuracy of experiments/tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Samples and Populations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Population Parameter&lt;/strong&gt;: denoted by \(  \mu \) (Î¼ pronounced mu) are values that describe the whole population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample statistics&lt;/strong&gt;: denoted by \( \overline{x} \) (pronounced as X-bar) are values that describe our sample.  Statistics are used to estimate/approximate the population parameter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling Error&lt;/strong&gt;: The difference of values between true population parameter and sample statistics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Size&lt;/strong&gt;: The size of the group that we choose to represent population.  The bigger the sample size, the smaller the sampling error generally.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Golden Arches Theory {of Conflict Prevention} by Thomas Friedman&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correlations doesn't mean causation: Just because two variables are relatable, it doesn't mean once causes the other to occur.  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Experiments vs Observational Studies&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To show relationships =&amp;gt; Perform observational studies i.e. surveys.&lt;ul&gt;
&lt;li&gt;(+) Easy way to get information of a population,&lt;/li&gt;
&lt;li&gt;(+) Relatively inexpensive&lt;/li&gt;
&lt;li&gt;(+) Can be conducted remotely&lt;/li&gt;
&lt;li&gt;(+) Anyone could access and analyze survey results&lt;/li&gt;
&lt;li&gt;(-) Untruthful responses&lt;/li&gt;
&lt;li&gt;(-) Biased responses&lt;/li&gt;
&lt;li&gt;(-) Respondents not understanding the questions (Response Bias)&lt;/li&gt;
&lt;li&gt;(-) Respondents refuse to answer (Non-response Bias)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To show causation =&amp;gt; Perform controlled experiments i.e.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Other Definitions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Treatments&lt;/strong&gt;: The manner in which researchers handle subjects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observational Studies&lt;/strong&gt;: When an experimenter watches a group of subjects and does not introduce a treatment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Independent Variables&lt;/strong&gt;: The variable that the experimenter chooses to manipulate, usually plotted along x-axis on a graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dependent Variables&lt;/strong&gt;: The variable that the experimenter chooses to measure, usually plotted along y-axis on a graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Treatment Group&lt;/strong&gt;: The group of a study that receives a varying levels of independent variable, used to measure the effect of a treatment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Control Group&lt;/strong&gt;: The group of study that doesn't receive treatment, used as a baseline when compared to treatment groups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Placebo&lt;/strong&gt;: Something that is given to the control group so they think they are getting the treatment, when in reality it causes no effect on them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blinding&lt;/strong&gt;: A technique used to reduce bias, i.e. placebo is blinding.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Double Blinding&lt;/strong&gt;: A technique to ensure those who administer treatments and those receiving treatments do not know who is actually receiving which treatment.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Controlled Factors&lt;/strong&gt;: Factors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random Assignments&lt;/strong&gt;: A technique used to minimize impact of the other uncontrolled factors to the results of an experiment. Randomization works best with larger groups.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Within-Subject Design&lt;/strong&gt;: A technique to see how a factor influences the impact we are measuring, for the same study subject, keeping other controlled and uncontrolled factors constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>descriptive-statistics</category><category>introduction</category><category>research methods</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-1/</guid><pubDate>Thu, 11 Aug 2016 13:10:06 GMT</pubDate></item><item><title>Descriptive Statistics - Visualizing Data</title><link>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-2/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;Lesson 2: Visualizing Data&lt;/h3&gt;
&lt;h4&gt;Some Definitions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Frequencies&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frequencies&lt;/strong&gt;: The number of times a certain outcome occurs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Relative Frequencies&lt;/strong&gt;: The probability (0.0 - 1.0) of an outcome happening in comparison to other outcomes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proportion&lt;/strong&gt;: A proportion is the fraction of counts over the total sample.A proportion can be turned into a percentage by multiplying the proportion by 100.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;: Histogram is a graphical representation of distribution of data.  Discrete intervals (bin size) are decided upon to form widths of our boxes.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bin Size and Number of Rows&lt;/strong&gt;: The two are inter-dependent.  Bin size is the discrete intervals set to accommodate the outcomes that falls within the defined intervals.  Bin size is also the intervals in which one is counting the frequencies of the outcomes.  The number of rows increases when bin size decreases; and vice versa.  Number of rows represent number of bars in a histogram diagram.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Histogram and Bar Graphs: Differences between them&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Histogram&lt;/strong&gt;: Data in histogram can be mixed, or distributed differently simply by changing the bin size.  The independent variables (x-axis) is usually numerical and quantitative.  The order of the bars are usually based on the incremental quantitative x-axis, small to big.  The shape of a histogram is very important as it represents the distribution curve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bar Graph&lt;/strong&gt;: Columns in bar graphs are spaced out. Each column in a bar graph represents one distinct categories. Distribution of data in a bar chart is less flexible for change because of its distinctive representation.  The x-axis is usually categorical or qualitative.  The order of how the bars are charted is solely dependent on how we want to sort it.  The shape of a bar chart is arbitrary.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;More on Histogram&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution&lt;/strong&gt;: How data falls into the different bin size.  Normal distribution forms a symmetrical bell-shape, with the highest frequencies in the middle.  The bin-size with highest frequencies is mod.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positively Skewed&lt;/strong&gt;: When most data falls onto the left side of the histogram than on the right.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Negatively Skewed&lt;/strong&gt;: When most values falls onto the right side of the histogram then the left.&lt;br&gt;
5.&lt;strong&gt;Math Symbols&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;\( \mathcal{f}  \)&lt;/strong&gt;: Frequencies&lt;/li&gt;
&lt;li&gt;\( \sum \): Total sum&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;\( \mathcal{p} \)&lt;/strong&gt;: Proportion (Probability)&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;</description><category>descriptive-statistics</category><category>visualizing data</category><guid>https://chowy1026.github.io/course-notes/descriptive-statistics/lesson-2/</guid><pubDate>Thu, 11 Aug 2016 13:04:36 GMT</pubDate></item></channel></rss>
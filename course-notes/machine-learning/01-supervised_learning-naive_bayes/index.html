<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH · Supervised Learning - Naive Bayes </title>
<link href="../../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://chowy1026.github.io/course-notes/machine-learning/01-supervised_learning-naive_bayes/">
<link rel="icon" href="../../../images/favicon.png" sizes="64x64">
<link rel="icon" href="../../../images/icon_512x512.png" sizes="512x512">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="cHoWy">
<link rel="prev" href="../00-supervised_learning-introduction/" title="Supervised Learning - Introduction" type="text/html">
<link rel="next" href="../02-supervised_learning-svm/" title="Supervised Learning - Support Vector Machine (SVM)" type="text/html">
<meta property="og:site_name" content=" =^..^= MEH">
<meta property="og:title" content="Supervised Learning - Naive Bayes">
<meta property="og:url" content="https://chowy1026.github.io/course-notes/machine-learning/01-supervised_learning-naive_bayes/">
<meta property="og:description" content="Naive Bayes Classifier / Algorithm
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assum">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-03-28T11:34:18+08:00">
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chowy1026.github.io/">
                      <h1 id="brand"><a href="https://chowy1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../../posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="../../index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="../../../links/index.html">Links</a>
        <a class="sidebar-nav-item" href="../../../books/index.html">Books</a>
        <a class="sidebar-nav-item" href="../../../archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="../../../tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="../../../images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="../../../images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2017 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">Supervised Learning - Naive Bayes</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2017-03-28T11:34:18+08:00" itemprop="datePublished" title="2017-03-28 11:34">2017-03-28 11:34</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h2>Naive Bayes Classifier / Algorithm</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning">machine learning</a>, naive Bayes classifiers are a family of simple <a href="https://en.wikipedia.org/wiki/Probabilistic_classification" title="Probabilistic Classifier">probabilistic classifier</a>s based on applying <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" title="Bayes' Theorem">Bayes' theorem</a> with strong (naive) <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)" title="Independence Probability Theory">independence</a> assumptions between the features.</p>
<p>Naive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s, and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis.</p>
<p>Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" title="Maximum Likelihood Training">Maximum-likelihood training</a> can be done by evaluating a closed-form expression,[1]:718 which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.</p>
<p>In the statistics and computer science literature, Naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.  All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.</p>
<p>Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers.  Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as <a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" title="Boosted Trees">boosted trees</a> or <a href="https://en.wikipedia.org/wiki/Random_forest" title="Random Forests">random forests</a>.  </p>
<p>An advantage of naive Bayes is that it only requires a small number of training data to estimate the parameters necessary for classification.  </p>
<h3>Bayes Rules:</h3>
<p>Prior Probability * Test Evidence (in Probability) = Posterior Probability</p>
<p><strong>Cancer Example:</strong>       <br>
Prior Probabilities {Prior Cancer Probability is 1%}: <br>
\begin{aligned}
P(C) &amp;= 0.01\\
\therefore\; P(NC) &amp;= 1 - P(C)\\
&amp;= 0.99
\end{aligned}</p>
<p>Test Evidence, Sensitivity and Specificity Probabilities:    <br><em>Test Sensitivity</em>
\begin{aligned}
P(Pos|C) &amp;= 0.9\\
\therefore\; P(Neg|C) &amp;= 1 - 0.9\\
&amp;= 0.1 <br>
\end{aligned}</p>
<p><em>Test specificity</em>
\begin{aligned}
P(Neg|NC) &amp;= 0.9\\<br>
\therefore\; P(Pos|NC) &amp;= 1 - 0.9\\
&amp;= 0.1<br>
\end{aligned}</p>
<p>Joint Probability:   <br>
\begin{aligned}
P(C, Pos) &amp;= P(C) * P(Pos|C)\\
&amp;= 0.009\\\\
P(NC, Pos) &amp;= P(NC) * P(Pos|NC)\\
&amp;= 0.099\\\\
P(Pos) &amp;= P(C|Pos) + P(NC|Pos)\\
&amp;= 0.009 + 0.099 \\
&amp;= 0.108
\end{aligned}</p>
<p>Normalized Posterior Probability:
\begin{aligned}
P(C|Pos) &amp;= P(C, Pos) / P(Pos)\\
&amp;=  0.009 / 0.108\\
&amp;= 0.0833
\\\\
P(NC|Pos) &amp;= P(NC, Pos) / P(Pos)\\
&amp;=  0.099 / 0.108\\
&amp;= 0.9167
\end{aligned}</p>
<p>Legends:   <br>
- \(P(C)\): Prior Cancer Probability, in this case 1%
- \(P(NC)\): Prior non-Cancer Probability, 1-P(C), in this case 99%  <br>
- \(P(C, Pos)\): Joint probability that one having cancer AND tested positive   <br>
- \(P(NC, Pos)\): Joint probability that one does not have cancer AND tested positive    <br>
- \(P(C|Pos)\): Posterior probability of having cancer given one is tested positive   <br>
- \(P(NC|Pos)\): Posterior probability of NOT having cancer given one is tested positive   <br>
- \(P(Pos|C)\): Test sensitivity, Probability of tested positive if one has cancer   <br>
- \(P(Neg|NC)\): Test specificity, Probability of tested negative if one has NO cancer.</p>
<h3>Common Usage</h3>
<p>Naive Bayes is a pretty good algorithm for text learning.  Naive Bayes is called "Naive" because it works with <em>words</em> at various <em>length</em>, but ignores <em>word order</em>.    </p>
<h3>Pros:</h3>
<ul>
<li>Easy to implement      </li>
<li>Simple and efficient to run      </li>
<li>Big / Great feature spaces      </li>
</ul>
<h3>Cons:</h3>
<ul>
<li>It could break when phrases that encompasses multiple words that have distinctive meanings don't really work well.  For example Chicago Bulls will be interpreted as "Chicago" and "Bulls", but that's not what Chicago Bulls is.</li>
</ul>
<h2>Other references:</h2>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes" title="Scikit-Learn Naive Bayes">Scikit-Learn Naive Bayes</a></li>
<li>
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" title="Scikit-Learn Gaussian Naive Bayes">Scikit-Learn Gaussian Naive Bayes</a>    </li>
</ul>
</div>
    </div>
    <aside class="postpromonav"><nav><div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../00-supervised_learning-introduction/" rel="prev" title="Supervised Learning - Introduction">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
                <a href="../02-supervised_learning-svm/" rel="next" title="Supervised Learning - Support Vector Machine (SVM)">
Next post
              </a>
            </span>

        </div>

    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="chowygit",
            disqus_url="https://chowy1026.github.io/course-notes/machine-learning/01-supervised_learning-naive_bayes/",
        disqus_title="Supervised Learning - Naive Bayes",
        disqus_identifier="cache/course-notes/machine-learning/01-supervised_learning-naive_bayes.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH · Supervised Learning - Support Vector Machine (SVM) </title>
<link href="../../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://chow1026.github.io/course-notes/machine-learning/02-supervised_learning-svm/">
<link rel="icon" href="../../../images/favicon.png" sizes="64x64">
<link rel="icon" href="../../../images/icon_512x512.png" sizes="512x512">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="cHoWy">
<link rel="prev" href="../01-supervised_learning-naive_bayes/" title="Supervised Learning - Naive Bayes" type="text/html">
<link rel="next" href="../03-supervised_learning-decision_tree/" title="Supervised Learning - Decision Tree" type="text/html">
<meta property="og:site_name" content=" =^..^= MEH">
<meta property="og:title" content="Supervised Learning - Support Vector Machine (SVM)">
<meta property="og:url" content="https://chow1026.github.io/course-notes/machine-learning/02-supervised_learning-svm/">
<meta property="og:description" content="Support Vector Machine (SVM)
In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-04-03T10:11:55+08:00">
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chow1026.github.io/">
                      <h1 id="brand"><a href="https://chow1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../../posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="../../index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="../../../links/index.html">Links</a>
        <a class="sidebar-nav-item" href="../../../books/index.html">Books</a>
        <a class="sidebar-nav-item" href="../../../archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="../../../tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="../../../images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="../../../images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2017 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">Supervised Learning - Support Vector Machine (SVM)</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2017-04-03T10:11:55+08:00" itemprop="datePublished" title="2017-04-03 10:11">2017-04-03 10:11</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <div>
<h2>Support Vector Machine (SVM)</h2>
<p>In <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning">machine learning</a>, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for <a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical Classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression Analysis">regression analysis</a>. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-<a href="https://en.wikipedia.org/wiki/Probabilistic_classification" title="Probabilistic Classification">probabilistic</a> <a href="https://en.wikipedia.org/wiki/Binary_classification" title="Binary Classification">binary</a> <a href="https://en.wikipedia.org/wiki/Linear_classifier" title="Linear Classifier">linear classifier</a>. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.</p>
<p>In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.</p>
<p>When data are not labeled, supervised learning is not possible, and an <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised Learning">unsupervised learnin</a>g approach is required, which attempts to find natural <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster Analysis">clustering of the data</a> to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often[citation needed] used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.</p>
<h3>Motivation</h3>
<p><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical Classification">Classifying data</a> is a common task in <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning">machine learning</a>. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a \( p \)-dimensional vector (a list of \( p \) numbers), and we want to know whether we can separate such points with a \( (p-1) \)-dimensional <a href="https://en.wikipedia.org/wiki/Hyperplane_separation_theorem" title="Hyperplane Separation Theorem">hyperplane</a>. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or <a href="https://en.wikipedia.org/wiki/Margin_(machine_learning)" title="Margin (Machine Learning)">margin</a>, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum <a href="https://en.wikipedia.org/wiki/Margin_classifier" title="Margin Classifier">margin classifier</a>; or equivalently, the <a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">perceptron</a> of optimal stability.</p>
<h3>Definition</h3>
<p>More formally, a support vector machine constructs a <a href="https://en.wikipedia.org/wiki/Hyperplane_separation_theorem" title="Hyperplane Separation Theorem">hyperplane</a> or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin the lower the <a href="https://en.wikipedia.org/wiki/Generalization_error" title="Generalization Error">generalization error</a> of the classifier.</p>
<p>Whereas the original problem may be stated in a finite dimensional space, it often happens that the sets to discriminate are not <a href="https://en.wikipedia.org/wiki/Linear_separability" title="Linear Separability">linearly separable</a> in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products may be computed easily in terms of the variables in the original space, by defining them in terms of a <a href="https://en.wikipedia.org/wiki/Positive-definite_kernel" title="Kernel Function">kernel function</a> \( k(x,y) \) k(x,y) selected to suit the problem.</p>
<h3>Applications</h3>
<p>SVMs can be used to solve various real world problems:</p>
<ul>
<li>SVMs are helpful in <a href="https://en.wikipedia.org/wiki/Document_classification" title="Text/Document Classification">text and hypertext categorization</a> as their application can significantly reduce the need for labeled training instances in both the standard inductive and <a href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)" title="Transduction (Machine Learning)">transductive</a> settings.</li>
<li>
<a href="https://en.wikipedia.org/wiki/Computer_vision#Recognition" title="Image Recognition">Classification of images</a> can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true of <a href="https://en.wikipedia.org/wiki/Image_segmentation" title="Image Segmentation">image segmentation</a> systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.</li>
<li>
<a href="https://en.wikipedia.org/wiki/Handwriting_recognition" title="Handwriting Recognition">Hand-written characters can be recognized</a> using SVM.</li>
<li>The SVM algorithm has been widely applied in the biological and other sciences. They have been used to classify proteins with up to 90% of the compounds classified correctly. <a href="https://en.wikipedia.org/wiki/Resampling_(statistics)#Permutation_tests" title="Permutation Tests">Permutation tests</a> based on SVM weights have been suggested as a mechanism for interpretation of SVM models.  Support vector machine weights have also been used to interpret SVM models in the past.  Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.</li>
</ul>
<h3>History</h3>
<p>The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the <a href="https://en.wikipedia.org/wiki/Kernel_method" title="Kernel Trick">kernel trick</a> to maximum-margin hyperplanes.  The current standard incarnation (soft margin) was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.    </p>
<h2>Other Useful Resources</h2>
<ul>
<li>
<a href="http://scikit-learn.org/stable/modules/svm.html" title="Scikit-Learn SVM">Scikit-Learn Support Vector Machine</a>     </li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="Scikit-Learn Support Vector Classifier">Scikit-Learn Support Vector Classifier</a></li>
<li><a href="http://scikit-learn.org/stable/modules/svm.html#svm-classification" title="SVC User Guide">Scikit-Learn SVC User Guide</a></li>
<li><a href="http://scikit-learn.org/stable/modules/svm.html#svm-kernels" title="SVM Kernels">Scikit-Learn Kernel Functions</a></li>
</ul>
</div>
    </div>
    <aside class="postpromonav"><nav><div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../01-supervised_learning-naive_bayes/" rel="prev" title="Supervised Learning - Naive Bayes">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
                <a href="../03-supervised_learning-decision_tree/" rel="next" title="Supervised Learning - Decision Tree">
Next post
              </a>
            </span>

        </div>

    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="chowygit",
            disqus_url="https://chow1026.github.io/course-notes/machine-learning/02-supervised_learning-svm/",
        disqus_title="Supervised Learning - Support Vector Machine (SVM)",
        disqus_identifier="cache/course-notes/machine-learning/02-supervised_learning-svm.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH · Supervised Learning - Decision Tree </title>
<link href="../../../assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://chow1026.github.io/course-notes/machine-learning/03-supervised_learning-decision_tree/">
<link rel="icon" href="../../../images/favicon.png" sizes="64x64">
<link rel="icon" href="../../../images/icon_512x512.png" sizes="512x512">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="cHoWy">
<link rel="prev" href="../02-supervised_learning-svm/" title="Supervised Learning - Support Vector Machine (SVM)" type="text/html">
<link rel="next" href="../05-supervised_learning-random_forest/" title="Supervised Learning - Random Forest" type="text/html">
<meta property="og:site_name" content=" =^..^= MEH">
<meta property="og:title" content="Supervised Learning - Decision Tree">
<meta property="og:url" content="https://chow1026.github.io/course-notes/machine-learning/03-supervised_learning-decision_tree/">
<meta property="og:description" content="A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one wa">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-04-03T17:14:29+08:00">
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chow1026.github.io/">
                      <h1 id="brand"><a href="https://chow1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../../posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="../../index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="../../../links/index.html">Links</a>
        <a class="sidebar-nav-item" href="../../../books/index.html">Books</a>
        <a class="sidebar-nav-item" href="../../../archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="../../../tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="../../../images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="../../../images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2017 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">Supervised Learning - Decision Tree</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2017-04-03T17:14:29+08:00" itemprop="datePublished" title="2017-04-03 17:14">2017-04-03 17:14</time></span>

    
    

    <div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>A <a href="https://en.wikipedia.org/wiki/Decision_tree" title="Decision Tree">decision tree</a> is a <a href="https://en.wikipedia.org/wiki/Decision_support_system" title="Decision Support System">decision support</a> tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm.</p>
<p>Decision trees are commonly used in <a href="https://en.wikipedia.org/wiki/Operations_research" title="Operations Research">operations research</a>, specifically in <a href="https://en.wikipedia.org/wiki/Decision_analysis" title="Decision Analysis">decision analysis</a>, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.</p>
<h2>Overview</h2>
<p>A decision tree is a <a href="https://en.wikipedia.org/wiki/Flowchart" title="Flowchart">flowchart</a>-like structure in which each internal node represents a "test" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.</p>
<p>In <a href="https://en.wikipedia.org/wiki/Decision_analysis" title="Decision Analysis">decision analysis</a>, a decision tree and the closely related <a href="https://en.wikipedia.org/wiki/Influence_diagram" title="Influence Diagram">influence diagram</a> are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.</p>
<p>A decision tree consists of three types of nodes:</p>
<p>Decision nodes – typically represented by squares
Chance nodes – typically represented by circles
End nodes – typically represented by triangles
Decision trees are commonly used in operations research and <a href="https://en.wikipedia.org/wiki/Operations_management" title="Operation Management">operations management</a>. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating <a href="https://en.wikipedia.org/wiki/Conditional_probability" title="Conditional Probability">conditional probabilities</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Decision_tree" title="Decision Tree">Decision trees</a>, <a href="https://en.wikipedia.org/wiki/Influence_diagram" title="Influence Diagram">influence diagrams</a>, <a href="https://en.wikipedia.org/wiki/Utility#Utility_functions" title="Utility Functions">utility functions</a>, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods.</p>
<h2>Data Impurity and Entropy</h2>
<p>A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.
\begin{aligned}
p &amp;= 0.5\\
\therefore\; q &amp;= 1 - p\\
&amp;= 0.5\\
\end{aligned}</p>
<p>\begin{aligned}
Entropy &amp;= -p\,log_2p -q\,log_2q\\
&amp;= -0.5\,log_20.5 -0.5\,log_20.5\\
&amp;= 1\\
\end{aligned}</p>
<p>To build a decision tree, we need to calculate two types of entropy using frequency tables as follows:   <br>
a) Entropy using the frequency table of one attribute:
\[
E(S) = - \sum_{i=1}^c\;p_i\,log_2(p_i)
\]
b) Entropy using the frequency table of two attributes:  <br>
\[
E(T,X) = - \sum_{c\in X}\;P(c)\,E(c)
\]</p>
<h2>Information Gain</h2>
<p>The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).    </p>
<p>Step 1: Calculate entropy of the target.    </p>
<p>Step 2: The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.    <br>
\begin{aligned}
Gain(T, X) &amp;= Entropy(T) - Entropy(T,X)\\
&amp;= E(T) - E(T,X)
\end{aligned}</p>
<p>Step 3: Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.     </p>
<p>Step 4a: A branch with entropy of 0 is a leaf node.     </p>
<p>Step 4b: A branch with entropy more than 0 needs further splitting.     </p>
<p>Step 5: The ID3 algorithm is run recursively on the non-leaf branches, until all data is classified.      </p>
<h2>Decision Tree Strengths and Weaknesses</h2>
<h3>Strengths</h3>
<ul>
<li>Easy to use,</li>
<li>Beautiful to draw/grow</li>
<li>Graphically describes the data</li>
<li>Bigger, custom classifiers can be built out of decision tree via ensemble methods.</li>
</ul>
<h3>Weaknesses</h3>
<ul>
<li>Prone to overfitting, esp too many features</li>
<li>
<h3>Needs to pay attention to tuning parameters, stop the growth of the tree (by tuning min samples split) at the appropriate time</h3>
</li>
</ul>
<h2>Other useful references:</h2>
<ul>
<li>[<a href="https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria" title="Gini vs Entropy">Gini vs Entropy</a>]     </li>
<li>
<a href="http://www.saedsayad.com/decision_tree.htm" title="Decision Tree">Decision Tree</a> by Saed Sayad     </li>
<li>
<a href="http://scikit-learn.org/stable/modules/tree.html#tree" title="Scikit-Learn Decision Tree">Scikit-Learn Decision Tree</a>     </li>
<li>
<a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="Scikit-Learn Decision Tree Classifier">Scikit-Learn DT Classifier</a>     </li>
</ul>
</div>
    </div>
    <aside class="postpromonav"><nav><div class="pager hidden-print pagination">

            <span class="previous pagination-item older">
                <a href="../02-supervised_learning-svm/" rel="prev" title="Supervised Learning - Support Vector Machine (SVM)">
                Previous post
                </a>
            </span>


            <span class="next pagination-item newer">
                <a href="../05-supervised_learning-random_forest/" rel="next" title="Supervised Learning - Random Forest">
Next post
              </a>
            </span>

        </div>

    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="chowygit",
            disqus_url="https://chow1026.github.io/course-notes/machine-learning/03-supervised_learning-decision_tree/",
        disqus_title="Supervised Learning - Decision Tree",
        disqus_identifier="cache/course-notes/machine-learning/03-supervised_learning-decision_tree.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

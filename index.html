<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH</title>
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://chowy1026.github.io/">
<link rel="icon" href="images/favicon.png" sizes="64x64">
<link rel="icon" href="images/icon_512x512.png" sizes="512x512">
<link rel="next" href="index-3.html" type="text/html">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/multiple-github-accounts/" type="text/html">
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chowy1026.github.io/">
                      <h1 id="brand"><a href="https://chowy1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="course-notes/index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="links/index.html">Links</a>
        <a class="sidebar-nav-item" href="books/index.html">Books</a>
        <a class="sidebar-nav-item" href="archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2016 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/multiple-github-accounts/" class="u-url">Setting Up Multiple GitHub User Account on One Machine</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-15T12:39:45+08:00" title="2016-11-15 12:39">2016-11-15 12:39</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>It is not unusual that developers need multiple github accounts on a single machine.  I was looking through the resources on the internet and found <a href="https://gist.github.com/jexchan/2351996/">this</a> the most useful.  None the less, I decided to write a post about this, just so for my own references.  </p>
<p>Here are the steps:
- Create the users (however many you need) on <a href="https://github.com/">github</a> first.     </p>
<ul>
<li>Use <code>ssh-keygen</code> to generate various ssh keys.  When prompted, make sure the keys are named appropriately so they can be identified easily.     </li>
</ul>
<pre class="code literal-block"><span></span>ssh-keygen -t rsa -C "{email1@youremail1.com}"
ssh-keygen -t rsa -C "{email1@youremail1.com}"
</pre>


<p>Make sure emails used are the ones you used for creating the github accounts.    </p>
<p>Usually ssh keys are stored under <code>home/{username}/.ssh</code> folder (or <code>/Users/{username}/.ssh</code> if you are on mac).  For example, the following keys are generated:    </p>
<pre class="code literal-block"><span></span>~/.ssh/id_rsa_{git_username1}
~/.ssh/id_rsa_{git_username2}
</pre>


<ul>
<li>Add the keys to SSH to the SSH Agent on the system:     </li>
</ul>
<pre class="code literal-block"><span></span>ssh-add ~/.ssh/id_rsa_{git_username1}
ssh-add ~/.ssh/id_rsa_{git_username2}
</pre>


<p>To delete ALL previously added keys:    </p>
<pre class="code literal-block"><span></span>ssh-add -D
</pre>


<p>To delete a previously added key:    </p>
<pre class="code literal-block"><span></span>ssh-add -d {id_rsa_keyname}
</pre>


<p>To list ALL previously added keys:    </p>
<pre class="code literal-block"><span></span>ssh-add -l
</pre>


<ul>
<li>The public keys need to be added to the github accounts accordingly.  </li>
</ul>
<pre class="code literal-block"><span></span>pbcopy &lt; ~/.ssh/id_rsa_{git_username1}.pub
</pre>


<p>Paste the content to corresponding <a href="https://github.com/settings/keys">github SSH key management</a>.   <br>
Repeat for all other keys for other accounts.     </p>
<ul>
<li>Configure SSH config</li>
</ul>
<pre class="code literal-block"><span></span>cd ~/.ssh/
nano config
</pre>


<ul>
<li>Add the following lines, modify accordingly:</li>
</ul>
<pre class="code literal-block"><span></span># github_{user1} account
Host github.com-{user1}
    HostName github.com
    User git
    IdentityFile ~/.ssh/id_rsa_{git_username1}
    IdentitiesOnly yes

# github_{user2} account
Host github.com-{user2}
    HostName github.com
    User git
    IdentityFile ~/.ssh/id_rsa_{git_username2}
    IdentitiesOnly yes
</pre>


<ul>
<li>Manage Global Git Configs.  You may either define those in command line or store them in a .gitconfig_global under     </li>
</ul>
<pre class="code literal-block"><span></span><span class="k">[core]</span>
       <span class="na">editor</span> <span class="o">=</span> <span class="s">atom -n -w</span>
<span class="s">       excludesfile = {filepath}/.gitignore_global</span>
<span class="k">[push]</span>
       <span class="na">default</span> <span class="o">=</span> <span class="s">upstream</span>
<span class="k">[merge]</span>
       <span class="na">conflictstyle</span> <span class="o">=</span> <span class="s">diff3</span>
<span class="k">[color]</span>
       <span class="na">ui</span> <span class="o">=</span> <span class="s">true</span>
<span class="k">[user]</span>
       <span class="na">name</span> <span class="o">=</span> <span class="s">{leave null, define this locally}</span>
<span class="s">       email = {leave null, define this locally}</span>
</pre>


<ul>
<li>Manage Local Git Configs  <br>
For example, github user1</li>
</ul>
<pre class="code literal-block"><span></span><span class="k">[user]</span>
        <span class="na">name</span> <span class="o">=</span> <span class="s">{github_username1}</span>
<span class="s">        email = {github_email1}</span>
<span class="k">[remote "origin"]</span>
        <span class="na">url</span> <span class="o">=</span> <span class="s">git@github.com-{user1}:{github_username}/{github_repo}</span>
</pre>


<p>Note that the host in URL has to be the right host defined earlier in the <code>~/.ssh/config</code> file.   </p>
<p>The same precaution has to be taken when cloning.  ie when executing <code>git clone</code> command, make sure the git host in 'copied and pasted' repo url is edited accordingly to the right user/host.  </p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="posts/matplotlib-on-osx-virtualenv/" class="u-url">Matplotlib on OSX with VirtualEnv/VirtualEnvWrapper (and Pip)</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-14T14:17:26+08:00" title="2016-11-14 14:17">2016-11-14 14:17</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>I like keeping my mini projects separate, mostly under its own virtual environment set up.  I use <a href="https://virtualenv.pypa.io/en/stable/">VirtualEnv</a> and <a href="https://virtualenvwrapper.readthedocs.io/en/latest/">VirtualEnvWrapper</a>.  </p>
<p>As I am working on code for data analysis and data visualization, inevitably I need the matplotlib library.  Reading various resources online, I managed to get it to work.  And with this post, I hope to document the steps for future reference.  </p>
<p>My environments:    <br>
- MacBookPro, OS macOS Sierra (10.12.1)   <br>
- VirtualEnv v.15.0.3   <br>
- python v.3.5.1  <br>
- Pip v.9.0.1    </p>
<p>Steps:  <br>
1. Activate the virtual environment: <code>workon {env}</code>  <br>
2. Show pip packages: <code>pip3 list</code>  <br>
3. Install necessary packages for data analysis and plotting: <code>pip install numpy scipy matplotlib pandas sympy nose</code>  <br>
4. Install PyQt5: <code>pip install PyQt5</code>   <br>
5. Modify matplotlibrc file: <code>nano {path to env}/lib/{pythonX.X}/site-packages/matplotlib/mpl-data/matplotlibrc</code>   <br>
      - At the  top of the configurations, define backend: <code>backend    :   Qt5Agg</code>  <br>
      - Then, define backend binding: <code>backend.qt5    :    PyQt5</code>  <br>
      - Save file.   </p>
<p>You are all set now.</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/descriptive-statistics/final_project/" class="u-url">Descriptive Statistics Final Project - Random Draws of Card Deck</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-14T11:39:38+08:00" title="2016-11-14 11:39">2016-11-14 11:39</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>Instead of working on this project on a google sheet files, I decided to write my own python script to simulate the experiments.  </p>
<p>1) Histogram generated shown as below.   <br><img alt="Original Cards" src="images/descriptive-statistics/original52.png"><br>
Histogram for original card values are negatively skewed (towards right).    </p>
<p>2) Randomly Drawing 3 Cards   <br>
I wrote python scripts with pandas and matplotlib for the rest of the project.  </p>
<p>3) Distribution of Sum of 3 Randomly Drawn Cards   <br><strong>30 Samples of Sum of 3 Randomly Drawn Cards</strong>   <br><img alt="30 Samples of Sum of 3 Randomly Drawn Cards" src="images/descriptive-statistics/sample30.png"></p>
<p>Statistical Description of this sample with n = 30:  <br>
count  30.000000  <br>
mean   19.866667  <br>
std     5.550603  <br>
min     9.000000  <br>
25%    15.000000  <br>
50%    20.000000  <br>
75%    24.000000  <br>
max    30.000000    </p>
<p><strong>500 Samples of Sum of 3 Randomly Drawn Cards</strong>      <br><img alt="500 Samples of Sum of 3 Randomly Drawn Cards" src="images/descriptive-statistics/sample500.png"></p>
<p>Statistical Description of this sample with n = 500:  <br>
count  500.000000  <br>
mean    19.542000  <br>
std      5.337529  <br>
min      5.000000  <br>
25%     16.000000  <br>
50%     20.000000  <br>
75%     24.000000  <br>
max     30.000000    </p>
<p>4) As sample size increases, the curve gets more normalized (bell-shaped).     </p>
<p>5) Q:  Within what range will you expect approximately 90% of your draw values to fall? What is the approximate probability that you will get a draw value of at least 20? Make sure you justify how you obtained your values.    </p>
<p>To determine the range where 90% of the values fall, I found the closest corresponding z-value to be 1.28.   Therefore, with the following formula, I determined the range for our sample with 30 draws, and sample with 500 draws.   </p>
<p>\[<br>
  z = \frac{ x - \mu }{\sigma} \\
  x_{30, 90%} = 26.971442   \\
  x_{500, 90%} = 26.374   \\
\]  </p>
<p>To draw at least 20 from each of the samples, the corresponding z values are:    </p>
<p>\[
  z_{30, &gt;20} = \frac{20 - 19.866667}{5.550603} = 0.024021
  \\
  z_{500, &gt;20} = \frac{20 - 19.542000}{5.337529} = 0.085807 \\
\]</p>
<p>For sample with sample size 30, at z = 0.024021, the probability is 0.51 (51%).  That means the change of drawing 3 cards with sum greater than 20 from this sample is 49%.      </p>
<p>For sample with sample size 500, at z = 0.024021, the probability is 0.534 (53.4%).  That means the change of drawing 3 cards with sum greater than 20 from this sample is 46.6%.    </p>
<p>Finally, here's a <a href="http://www.stat.ufl.edu/~athienit/Tables/Ztable.pdf">link to the z-table</a> for reference.  </p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-16/" class="u-url">Inferential Statistics - Chi-Squared Tests</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-09T13:10:55+08:00" title="2016-11-09 13:10">2016-11-09 13:10</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Types of Data</h3>
<ul>
<li>
<strong>Ordinal Data</strong> - ranks with no fixed intervals nor zeros   </li>
<li>
<strong>Interval Data</strong> - ranks with equal intervals     </li>
<li>
<strong>Ratio Data</strong> - ranks with equal intervals and an absolute zero   </li>
<li>
<strong>Nominal/Categorical Data</strong> - data with no numerical values (typically yes/no, in/out, successful/unsuccessful)    </li>
</ul>
<h3>Types of Tests</h3>
<ul>
<li>
<strong>Parametric</strong> - hypothesis testing that make assumptions about the parameters of the populations, \( \mu \) and \( \sigma \).</li>
<li>
<strong>Non-Parametric</strong> - hypothesis testing that do not require population parameters</li>
</ul>
<h3>Characteristics of Non-Parametric Testings</h3>
<ul>
<li>There is no way to calculate a mean or standard deviation      </li>
<li>The data is based on frequencies or proportions    </li>
<li>The data is nominal (successful vs unsuccessful, 1 or 0, yes or no, mountain vs beach etc.)    </li>
<li>The data are not based on Normal distribution    </li>
</ul>
<h3>\( \chi^2 \) Goodness of Fit Test</h3>
<p>How well our observed frequencies 'fit' our expected frequencies? <br>
\[
  \chi ^2 = \sum{ \frac{(f_0 - f_e)^2}{f_e} }
\]
\( \chi^2 \) is smaller when the observed value is closer to the expected value.   <br>
\( \chi^2 \) is NEVER negative and therefore \( \chi^2 \) statistic is one-directional.     </p>
<p>For each category, we have one \( \chi^2 \) statistics.  When we have more categories, \( \chi^2 \) statistics get bigger with the number of categories.    </p>
<h3>Degrees of Freedom</h3>
<p>For n x m, 2 dimensional nominal data:
\[
  df = (N_n - 1) * (N_m - 1) \\
  \text{where } N_n \text{ and }N_m \text{ are number of columns, and number of rows respectively. }
\]</p>
<h3>Cramer's V \( (\phi_c) \)</h3>
<p>\[
  \phi_c = \sqrt{\frac{\chi^2}{n(k-1)}}  \\
  k \text{ is the smaller number between number of rows or number of columns}\\
  n \text{ is the total sample size regardless of treatments}
\]</p>
<h3>Assumptions and Restrictions for \(\chi^2\) Tests</h3>
<ul>
<li>Avoid dependent observations.  Independence will be violated if any participants were given two treatments instead of one.     </li>
<li>Avoid small expected frequencies, therefore in general have a larger number of participants.   Sample size should be at least 20, and each expected cell frequencies should be at least 5.    </li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-15/" class="u-url">Inferential Statistics - Regressions</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-08T15:00:58+08:00" title="2016-11-08 15:00">2016-11-08 15:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Linear Regressions</h3>
<p>Line of best fit forms a line to help us:  <br>
- describe data  <br>
- make predictions     </p>
<p><strong>Observed Y vs Expected Y</strong>   <br><strong>Expected Y</strong> \( \hat{y} \) are the ones calculated/predicted based on the best fit regression line.  <br><strong>Observed Y</strong> \( y \) are collected data or real-life data.  <br><strong>Residual</strong> is the difference between Observed value and Expected value.  </p>
<p>A few ways to find the line of best fit:  <br>
- Find a line to minimize the sum of residuals.   The problem with this approach is sometimes, negative and positive residuals cancel each other out.     <br>
- Find a line that minimize the sum of <strong>absolute</strong> residuals.  <br>
- Find a line that minimize the sum of squared residuals, \( \sum{(y - \hat{y})} \)</p>
<p>When we use calculus to determine the slope, b:<br>
\[
  b = \frac{\sum{(y_i - \bar{y})(x_i - \bar{x})}}{\sum{(x_i - \bar{x})^2}} \\
  = r (\frac{S_y}{S_x}) \\
  \text{where } r \text{ is Pearson's Correlation Coefficient and }\\
   S \text{ are standard deviations of } x \text{ and } y.
\]</p>
<p>We have decided to symbolize the regression line by y = bx + a, where b represents the slope and a represents the y-intercept.
Since \( b = r(\frac{S_y}{S_x}) \), we can also symbolize the regression line like this:  <br>
\[ y = r(\frac{S_y}{S_x})x + a \]</p>
<p><strong>Pearson's Correlation Coefficient \( r \)</strong>  <br>
A high \(r\) value indicates a strong correlation. This could contribute to high \(r^2 \) value, which indicates the percentage of differences in Y is due to differences in X.  </p>
<p><strong>Standard Error of Estimates</strong>  <br>
Standard error of estimates: <br>
\[
SE = \sqrt{ \frac{\sum (y - \hat{y})^2}{N-2}}
\]</p>
<p><strong>Factors that Affect the Regressions</strong> <br>
- outliers affect the value of r, correlation Coefficient
- outliers also affect the linear regression line
-</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-14/" class="u-url">Inferential Statistics - Correlations</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-07T13:33:58+08:00" title="2016-11-07 13:33">2016-11-07 13:33</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p><strong>Relationships</strong>   <br>
How one variable is related to the other?</p>
<p><strong>Variable X and Y</strong>    <br>
X is often referred to as the predictor, explanatory, independent variable.      </p>
<p>Y is often referred as the outcome, response, dependent variable.  </p>
<p>Scatterplot is a popular/ most common way to show relationship of X and Y variables.      </p>
<p><strong>Strong Relationships</strong>    <br>
Strong relationships usually have less scattered plots.  If we draw an eclipse surrounding the data points, the smaller the ratio of minor axis to the major axis, the stronger the relationship is.  </p>
<p><strong>Direction of Relationships</strong>       <br>
Positively related - Y responses in the same direction as X changes; Negatively related - Y responses in opposite directions of X changes.</p>
<p><strong>Correlation Coefficient, (r)</strong>    <br>
Also known as Pierson's r.  </p>
<p>r is a fraction, with the covariance of x and y (how much do they vary together) as the numerator, and the product of standard deviation of x  and standard deviation of y as the denominator.<br>
\[
  r = \frac{cov(x,y)}{ S_x * S_y } = \frac{cov_{x,y}}{ S_x * S_y }
\]  <br>
r measures the strength of a relationship, by measuring how closely the data falls along a straight line.      </p>
<p>Even though r is a ratio, it is not interpreted as a percentage.  However, \(r^2\) is a percentage of the variation in y explained by variation in x.  \(r^2\) is called the coefficient determination.       </p>
<p><strong>True Correlation of Population, rho, \( \rho \)</strong>     <br>
We usually perform hypothesis testing with t-tests.
\[
  H_{0} : \rho = 0 \\
  H_{1} : \rho \gt 0 \\
  H_{1} : \rho \lt 0 \\
  H_{1} : \rho \ne 0
\]</p>
<p><strong>Causation vs Correlations</strong>    <br><strong>Causation</strong> - One variable caused another to happen.     </p>
<p><strong>Correlation</strong> - There is a relationship between two variables.  But there are lots of lurking variables.  For example, there are two variables X and Y.  They could have a relationship because both of them are influenced by variable A, or Y is influenced by X <strong>through</strong> variable A.  In this case, variable A is called the mediating variable.  </p>
<p>To make a causal statement: <br>
     - the independent variable would have to occur BEFORE the dependent variable.<br>
     - have to rule out other lurking variables too</p>
<p><strong>Fallacies</strong>  <br>
Ambiguous Temporal Precedence - we don't know which variable happens first.   <br>
Third variable problem    <br>
Post-hoc fallacy      </p>
<p><strong>t value and Correlation Coefficient, (r)</strong>   <br>
t value and r:
\[
  t = \frac{r  \sqrt{N-2}}{\sqrt{1-r^2}}
\]
where \(df\) is \( N - 2 \)</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-13/" class="u-url">Inferential Statistics - ANOVA Continued.</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-03T21:55:35+08:00" title="2016-11-03 21:55">2016-11-03 21:55</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>One of the popular Multiple Comparison Tests is Tukey's Honestly Significant Differences.    <br><strong>Tukey's Honestly Significant Differences (HSD)</strong>  <br>
Tukey's HSD is calculated as the following:
\[
\text{Tukey's HSD} = q * \sqrt{\frac{MS_{within}}{n}} \\
q \text{ is looked up with } df_{within} \text{ and } k \text{, the number of treatments/sample groups}   \\
n \text{ is the number of samples in one sample group}
\]   </p>
<p>If the mean difference between/among treatments are greater than Tukey's HSD, the difference is significant.    </p>
<p>Note this is very similar to Z test and T test. For Z tests, the margin of error is:
\[
\text{Margin of Error} = z * \frac{\sigma}{\sqrt{n}}
\]</p>
<p>Whereas for t tests, the margin of error is:
\[
\text{Margin of Error} = t * \frac{s}{\sqrt{n}}
\]</p>
<p><strong>Cohen's D for Multiple Comparisons</strong>  <br>
For normal comparisons, Cohen's D is calculated by
\[
\text{Cohen's D, } d = \frac{\bar{X_1} - \bar{X_2}}{SD_{pooled}}
\]   </p>
<p>In multiple comparisons, Cohen's D is calculated by
\[
\text{Cohen's D, } d = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{MS_{within}}}
\]</p>
<p>Cohen's D is calculated per pair comparisons.  </p>
<p><strong>\( \mathbf{\eta ^ 2} \)</strong>  <br>
\( \eta ^2 \) is defined as the proportion of total variance that is due to between-group differences (explained variation).  </p>
<p>\[
\eta ^2 = \frac{SS_{between}}{SS_{total}} \\
      = \frac{SS_{between}}{SS_{between} + SS_{within}}
\]    </p>
<p><strong>Reporting Reports of Anova</strong>  <br>
We report the results of F Statistics as the following:   <br>
\[
   F(df_{between}, df_{within}) = 27 \quad p &lt; 0.05 \quad \eta ^2 = 0.90 \quad \\
   \text{p is estimated, by hand} \\
   F(df_{between}, df_{within}) = 27 \quad p = 0.001 \quad \eta ^2 = 0.90 \quad \\
   \text{exact p value calculated by software} \\
\]</p>
<p><strong>ANOVA for Groups with Different Sample Sizes</strong>  <br>
Grand mean
\[
  \text{Grand mean, } \bar{X_G} = \frac{ \sum_{j=0}^k n_j (\bar{x_j}) }{\sum_{j=0}^k n_j } \\
  n_j \text{ is sample size for each sample} \\<br>
  k \text{ is number of sample groups}      \\
\]    </p>
<p>SS (Sum of Squares) Between   <br>
\[
  \text{sum of squares, } SS_{between} = \sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 \\
  n_j \text{ is sample size for each sample}   \\
\]    </p>
<p>SS (Sum of Squares) Within   <br>
\[
  \text{sum of squares, } SS_{within} = \sum_{i=0}^N (x_i - \bar{x_k})^2 \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]    </p>
<p>DF (Degress of Freedom) Between   <br>
\[
  \text{degrees of freedom, } df_{between} = k - 1 \\
  k \text{ is number of sample groups}    \\
\]    </p>
<p>DF (Degress of Freedom) Within   <br>
\[
  \text{degrees of freedom, } df_{within} = N - k \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]    </p>
<p>MS (Mean Squares) Between   <br>
\[
  \text{Mean square, } MS_{between} = \frac{SS_{between}}{df_{between}}  \\
  = \frac{\sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2}{k - 1}
\]    </p>
<p>MS (Mean Squares) Within   <br>
\[
\text{Mean square, } MS_{within} = \frac{SS_{within}}{df_{within}}  \\
= \frac{\sum_{i=0}^N (x_i - \bar{x_k})^2}{N - k}
\]    </p>
<p>F Stats
\[
\text{F Statistics, } F = \frac{MS_{between}}{MS_{within}}  \\
= \frac{\sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 / (k - 1)}{\sum_{i=0}^N (x_i - \bar{x_k})^2 / (N - k)}
\]   </p>
<p><strong>\( \mathbf{\eta ^ 2} \)</strong><br>
\[
\eta ^2 = \frac{SS_{between}}{SS_{total}} \\
      = \frac{SS_{between}}{SS_{between} + SS_{within}}
\]  </p>
<p><strong>ANOVA Power</strong>  <br>
Increase POWER in order to avoid Type II statistical error where we fail to reject the null when there is a treatment effect.   </p>
<p>In the case of drug testing, we want to:  <br>
- test <strong>more people</strong>  <br>
- give each drug to <strong>very similar</strong> groups of people  <br>
- test with a <strong>strong</strong> dosage    </p>
<p><strong>ANOVA Assumptions &amp; Conclusions</strong>   <br>
We assume:  <br>
- <strong>Normality</strong>: the population of which our samples are from are all normally distributed.  <br>
- <strong>Homogeneity of Variance</strong>: the samples come from populations that have equal amount of variability.   <br>
- <strong>Independence of Observations</strong>: The results found from one samples won't affect the others.   </p>
<p>We could have the following exceptions:  <br>
- violate the normality if the sample is large  <br>
- violate the homogeneity of variance if:  <br>
        - almost equal sample sizes  <br>
        - ratio of any two variances doesn't exceed 4     <br>
-</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/intro-data-analysis/1-data_analysis_process/" class="u-url">Intro to DA - Data Analysis Process</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-14T13:57:55+08:00" title="2016-10-14 13:57">2016-10-14 13:57</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>Data Analysis Process</h2>
<ol>
<li>
<strong>Question</strong>  <br>
    Problem begins with a question you want answer or problem you wanna solve.  For example:    <ul>
<li>Characters of students who pass projects</li>
<li>How can I stock stores with products that most people want to buy?</li>
</ul>
</li>
<li>
<strong>Wrangler</strong>    <ul>
<li>Data Acquisition   </li>
<li>Data Cleaning   </li>
</ul>
</li>
<li>
<strong>Explore</strong>   <ul>
<li>Building intuition</li>
<li>Finding patterns</li>
</ul>
</li>
<li>
<strong>Draw Conclusions</strong>   <br>
    This requires statistics and machine learning (that is beyond the scope of this course)<ul>
<li>Draw Conclusion =&gt; Users less likely to XXX</li>
<li>Make Prediction =&gt; Predict what products a user would like</li>
</ul>
</li>
<li>
<strong>Communicate</strong>  <ul>
<li>Blog post, paper, email, powerpoint, in-person conversation</li>
</ul>
</li>
</ol>
<p>The steps aren't always sequential.  We might go back and forth between data wrangling and exploration.  Along the process, one may go back to the question to further refine the question.  And data acquisition might occur before the question arises.</p>
<h2>Data Wrangling</h2>
<p>Data Types: <br>
Usually data acquired are in JSON or CSV (comma separated value) format.    </p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-12/" class="u-url">Inferential Statistics - One-Way ANOVA</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-14T10:25:32+08:00" title="2016-10-14 10:25">2016-10-14 10:25</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In lesson 11, we learned to perform t-tests for two independent samples.  However, in real statistical studies, there are a lot of times we need to compare more than two independent samples.  When we have \( n \) independent samples, the number of t-tests we need to perform are:
<!-- {n+1 \choose 2k} == \binom{n+1}{2k} -->
\[
  \text{no of t-tests with n samples} = \binom{n}{2} = \frac{n!}{2!(n-2)!}
\]</p>
<p>However, the same concept of t-test applies here.  Remember t is defined by a function of the distance apart from each other and the variability of each sample.
\[
  t_{statistic} = \frac{ \bar{X_1} - \bar{X_2}}{ \sqrt{ \frac{ S_p^2 }{ n_1 } + \frac{ S_p^2 }{ n_2 }}}
\]
When we compare 3 or more samples, we compare distance/variability between means (as numerator) and some kind of sample error (as the denominator)</p>
<p><strong>Think about this:</strong> <br>
Q: How can we compare three or more samples? <br>
A: Find the average squared deviation of each sample means.</p>
<p>Q: Will the <strong>Grand Mean</strong>, mean of the sample means be the same as the mean of all values in each sample? <br>
A: Sometimes.  Only when the sample sizes are equal for each sample, that the Grand Mean will be the same.</p>
<p><strong>Between Group Variability</strong> <br>
Between group variability is the variability between/among samples.   <br>
Q: What conclusions can we draw from the deviation of each sample mean from the mean of the means? <br>
A: The smaller the distance between sample means, the less likely the population means will defer significantly.  Vice versa the greater the distance between sample means, the more likely population means will differ significantly.  </p>
<p><strong>Within Group Variability</strong> <br>
Within group variability is the variability of the individual samples within a sample.  <br>
The greater the variability of each individual sample, the less likely population means will differ significantly.  (thinner, non overlapping normal distribution.) <br>
The smaller the variability of each individual sample, the more likely population means will differ significantly.  (wider, overlapping normal distribution)</p>
<p>Since we are analyzing the variabilities between samples and within samples, we call this Analysis of Variability (ANOVA).  We have one way ANOVA when we have one independent variable (sometimes called a factor)  </p>
<p><strong>F Statistics</strong> <br>
F statistics is the ratio of between group variability (numerator) to the within group variability (denominator)
If the between group variability is big, it constitutes to big F statistics, which results in rejecting the null hypothesis and accepting the alternative hypothesis.<br>
Whereas if the within group variability is large, it makes the F statistics small, which results in accepting the null hypothesis and rejecting the alternative hypothesis.</p>
<p>\[
F = \frac{ \text{between group variability}}{ \text{within group variability}} \\<br>
  = \frac{ \sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 / (k-1)}{\sum_{i=0}^N  (x_i - \bar{x_k})^2 / (N-k) } \\
  n_j \text{ is sample size for each sample}   \\
  = \frac{ n \sum_{j=0}^k (\bar{x_j} - \bar{x_G})^2 / (k-1)}{\sum_{i=0}^N (x_i - \bar{x_k})^2 / (N-k) } \text{when sample size is same for all samples}  \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]</p>
<p>F can also be formulated as
\[
F = \frac{SS_{between} / df_{between}}{SS_{within} / df_{within}} \text{ where } SS \text{ stands for Sum of Squares }      \\
  = \frac{MS_{between}}{MS_{within}} \text{ where } MS \text{ stands for Mean Square }
\]</p>
<p>Note that \(df_{between}\) is \( k - 1 \) while \(df_{within}\) is \( N - k \)</p>
<p>If we add \(df_{between}\) and \(df_{within}\) up, we get \( N - 1\) which is the total degree of freedom \(df_{total}\)
\[
df_{total} = df_{between} + df_{within} \\
= N - 1
\]</p>
<p>Similarly, the total variation \(SS_{total}\) is the sum of \(SS_{between}\) and \(SS_{within}\).<br>
\[
SS_{total} = SS_{between} + SS_{within} \\
= \sum{(x_i - \bar{x_G}) ^ 2}
\]</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-11/" class="u-url">Inferential Statistics - T-Test, Independent Samples</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-11T12:35:53+08:00" title="2016-10-11 12:35">2016-10-11 12:35</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p><strong>Dependent Samples (Repeated Measures)</strong> deals with within-subject designs.<br>
Types of Dependent Samples:</p>
<ul>
<li>Two conditions (a control group and a treatment group, OR two groups with two types of treatment)</li>
<li>Longitudinal (same subject group measured two different points in time)</li>
<li>Pre-test, Post-test (subject group measure before and after treatment)</li>
</ul>
<p>Pros:</p>
<ul>
<li>Controls for individual differences:<ul>
<li>Use fewer subjects</li>
<li>More cost-effective</li>
<li>Less time-consuming</li>
<li>Less expensive</li>
</ul>
</li>
</ul>
<p>Cons:</p>
<ul>
<li>Carry-over effect:<ul>
<li>The second measurement might be affected by the first treatment</li>
</ul>
</li>
<li>The order in which treatments were given might influence the results</li>
</ul>
<h2>Independent Samples:</h2>
<p>Independent Samples deals with between-subject designs.
Types of Independent Samples:</p>
<ul>
<li>Experimental</li>
<li>Observational</li>
</ul>
<p>Pros:</p>
<ul>
<li>Control for carry-over effect:<ul>
<li>The second measurement less likely be affected by the first treatment</li>
</ul>
</li>
<li>The order in which treatments no longer influence the results</li>
</ul>
<p>Cons:</p>
<ul>
<li>Little/no control for individual differences:</li>
<li>Need more subjects</li>
<li>Less cost-effective</li>
<li>More time-consuming</li>
<li>More expensive</li>
</ul>
<h2>Two-Sample Tests</h2>
<p>Considering two independent normally distributed samples collected, when we subtract those two data, we get a new dataset.<br>
\[
    N ( \mu_{1}, \sigma_{1} ) - N ( \mu_{2}, \sigma_{2} ) = N ( \mu_{1} - \mu_{2}, \sqrt{ \sigma_{1}^2 + \sigma_{2}^2 } )
\]</p>
<p>The standard deviation, \( SD \) would be:
\[
    SD = \sqrt{ SD_{1}^2 + SD_{2}^ 2 }
\]</p>
<p>The standard error, \( SE \) would be:
\[
    SE = \sqrt{ \frac{SD_{1}^2}{n_{1}} + \frac{SD_{2}^2}{n_2}}
\]</p>
<p>The t statistic, \( t_{statistic} \) would be:
\[
    t_{statistic} = \frac{ (\bar{x_{1}} - \bar{x_{2}} ) - (\mu_{1} - \mu_{2})}{ SE }
\]</p>
<p>The degree of freedom, \( df \) would be:
\[
    df = (n_{1} - 1) + (n_{2} - 1) = n_{1} + n_{2} - 2
\]
or the smaller value between
\(
(n_{1} - 1) \text{ and } (n_{2} - 1)
\).</p>
<h2>Pooled Variance</h2>
<p>Pooled variance is a method for estimating variance
given several different samples taken in different circumstances where the mean may vary
between samples but the true variance is assumed to remain the same. The pooled variance is
computed by using
The Pooled Variance, \( S_{p}^2 \) would be:
\[
    S_{p}^2 = \frac{(SS_{x} + SS_{y})}{df_{x} + df_{y}}
\]
where \( SS_{x} = \sum (x_{i} - \bar{x})^ 2 \)
and \( SS_{y} = \sum (y_{i} - \bar{y})^ 2 \)</p>
<p>The Standard Error, \( SE_{(\bar{x}-\bar{y})} \), using Pooled Variance is:</p>
<p>\[
    SE_{(\bar{x}-\bar{y})} = \sqrt{ \frac{S_{p}^2}{n_x} + \frac{S_{p}^2}{n_y} }
\]</p>
<p>The full t statistic \( t_{statistic} \) formula would be:
\[
    t_{statistic} = \frac{ (\bar{x} - \bar{y}) - \delta }{ SE_{(\bar{x}-\bar{y})} }
\]
\(\bar{x} - \bar{y} \) is called the observed difference.
The \( \delta \)expected_diff is derived from the Null Hypothesis, \( H_0 \), when
\[
    H_0: \mu_x - \mu_y = \delta
\]
There are lots of cases where \( \delta \) was assumed 0.  </p>
<p>When we use Pooled Variance, we hold the following assumptions:
1. X and Y should be two random samples from two independent populations. <br>
2. Populations that X and Y come from, should be approximately normal.  This is less important when sample size is really large ( &gt;30).<br>
3. Sample data can be used to estimate population variances.
4. Population variances should be roughly equal.</p>
</div>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="next">
                <a href="index-3.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

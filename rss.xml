<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title> =^..^= MEH</title><link>https://chow1026.github.io/</link><description></description><atom:link href="https://chow1026.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 18 Aug 2017 06:53:20 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Back to Anaconda</title><link>https://chow1026.github.io/posts/back-to-anaconda/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;It was last year that I first heard of Anaconda, iPython (currently known as Jupyter) Notebook, etc... It didn't make sense to me back then, mind you it is a mighty and powerful platform.  I felt a bit overwhelmed then.  But today as I revisit the &lt;a href="https://www.continuum.io/"&gt;Anaconda&lt;/a&gt; website and &lt;a href="http://jupyter.readthedocs.io/en/latest/"&gt;Jupyter&lt;/a&gt; (formerly iPython) website, reading the documents again, it sort of makes sense now.  &lt;/p&gt;
&lt;p&gt;What prompted me to revisit Anaconda, and Jupyter Notebook?  I am kind of looking for a "one tool does all" environment for data science project that accommodate the use of python and R.  I have been using homebrew with pip, virtualenv, virtualenvwrapper to manage development environment and packages for python projects; and RStudio for R projects.  It is sort of redundant to set up different environments, use different package manager and learn two different tools to do any kind of data science work at all.&lt;/p&gt;
&lt;p&gt;So here we are.  The set up for Anaconda is pretty straight forward and there are tons of documentations out there.  Here is a simplified short list of steps:      &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href="https://www.continuum.io/downloads"&gt;Download&lt;/a&gt; page on &lt;a href="https://www.continuum.io/"&gt;Anaconda&lt;/a&gt; home page.&lt;/li&gt;
&lt;li&gt;Choose the download that is suitable for your platform:    &lt;/li&gt;
&lt;li&gt;Install by double clicking the installer package, or type &lt;code&gt;bash AnacondaX.Y.Z-XXX.sh&lt;/code&gt; if you downloaded the bash installer script instead.  &lt;/li&gt;
&lt;li&gt;Once installation is completed, we could move on to using the terminal/bash. Type in &lt;code&gt;conda update conda&lt;/code&gt; and &lt;code&gt;conda update anaconda&lt;/code&gt; to make sure packages at root are up-to-date.  &lt;/li&gt;
&lt;li&gt;Sometimes packages or library that comes with anaconda might be out of date, you can update by &lt;code&gt;conda update scikit-learn&lt;/code&gt; or &lt;code&gt;conda install -c anaconda scikit-learn=X.XX.X&lt;/code&gt;.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One thing I noticed was the project-env hook that virtualenvwrapper offers.  When I typed &lt;code&gt;workon XXXenv&lt;/code&gt;, the virtual environment of XXX will be activated, and I was also redirected to the project directory.  This is convenient, if we have 1-to-1 match of project and virtualenv setups, which was mostly the case for me.  Note that this is not a fixed/default setup, I just happened to like it this way.  &lt;/p&gt;
&lt;p&gt;I guess the advantage of anaconda is one could set up an environment once and share it across projects that uses similar libraries or packages.  &lt;/p&gt;
&lt;p&gt;TODO: Explore anaconda-project.  &lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/posts/back-to-anaconda/</guid><pubDate>Tue, 15 Aug 2017 02:20:58 GMT</pubDate></item><item><title>dyld: Library Not Loaded: @executable_path/../.Python</title><link>https://chow1026.github.io/posts/python-library-not-loaded/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;I have always been a fan of virtualenv and virtualenvwrapper.  It is nice to be able to isolate development environments for each of my python projects.  &lt;/p&gt;
&lt;p&gt;However some of the projects I work on aren't python projects.  It could be R, it could be iPython Jupyter notebook.   That means I could be away from meddling with any of the python virtual environments for a while.  And if I 'accidentally' updated my native python version on my MacOS machine, I would see the error as such "dyld: Library Not Loaded: @executable_path/../.Python".  &lt;/p&gt;
&lt;p&gt;After googling around, the solutions out there are either to start from scratch and recreate a new virtual environments or to go through the some steps to recreate the links from the virtual environments to the new version and paths of python.  &lt;/p&gt;
&lt;p&gt;I tried both ways, and I really can't tell which is easier.  Ultimately, after recreating the links or recreate a project, one will still have to re-install all the packages needed for the project.  I would love the ability to freeze requirements into a requirements.txt, but even that will require a working python virtualenv and pip.  Typical chicken and egg problem.  &lt;/p&gt;
&lt;p&gt;Anyway, for the sake of future reference, I thought it would be good to type out all the command lines to recreate the links:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;python virtualenv folder&lt;span class="o"&gt;}&lt;/span&gt;
$ find &lt;span class="o"&gt;{&lt;/span&gt;broken virtualenv&lt;span class="o"&gt;}&lt;/span&gt;/ -type l         &lt;span class="c1"&gt;## to list out all the links&lt;/span&gt;
$ deactivate &lt;span class="c1"&gt;## Optional, deactivate if virtualenv is active&lt;/span&gt;
$ find &lt;span class="o"&gt;{&lt;/span&gt;broken virtualenv&lt;span class="o"&gt;}&lt;/span&gt;/ -type l -delete &lt;span class="c1"&gt;## to delete the broken links&lt;/span&gt;
$ virtualenv &lt;span class="o"&gt;{&lt;/span&gt;broken virtualenv&lt;span class="o"&gt;}&lt;/span&gt; --python&lt;span class="o"&gt;=&lt;/span&gt;python3   &lt;span class="c1"&gt;## recreate links to OS's python&lt;/span&gt;
$ workon &lt;span class="o"&gt;{&lt;/span&gt;broken virtualenv&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;## activate and workon the fixed virtualenv&lt;/span&gt;
$ pip3 install  ... &lt;span class="o"&gt;{&lt;/span&gt;other packages required &lt;span class="k"&gt;for&lt;/span&gt; the project&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;


&lt;p&gt;All that said, &lt;strong&gt;NOTE TO SELF&lt;/strong&gt;:&lt;/p&gt;
&lt;h4&gt;REMEMBER TO PIP FREEZE REQUIREMENTS.TXT BEFORE UPGRADING SYSTEM'S PYTHON.&lt;/h4&gt;
&lt;p&gt;PS: For Nikola, to rebuild site after the virtualenv fix, Nikola Static site builder somehow failed.  Error message was "dbm.error: db type is dbm.gnu, but the module is not available".  After some googling around, I found out we need to clear cache and previously built .db file type &lt;code&gt;nikola clean&lt;/code&gt; and &lt;code&gt;nikola forget&lt;/code&gt;.  Then run &lt;code&gt;nikola auto -b&lt;/code&gt; to build a new build. &lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/posts/python-library-not-loaded/</guid><pubDate>Mon, 07 Aug 2017 08:39:43 GMT</pubDate></item><item><title>Visualization with Tableau</title><link>https://chow1026.github.io/posts/tableau/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;Since Udacity changed the Data Visualization course module in the DAND program (they have chosen to partner with Tableau), I was given the options to finish my last project (the data visualization project) either with D3 (old syllabus) or with Tableau (new syllabus).  &lt;/p&gt;
&lt;p&gt;Thinking that Tableau was just a software, and given the fact I hadn't had great experience with Javascript programming even back in my frontend days, I decided to switch to the new module and give Tableau a try.  &lt;/p&gt;
&lt;p&gt;However I soon learn that it is quite a terrible mistake.  First off, the new Visualization with Tableau tutorial was awful.  It barely covers the essence of visualization designs.  The parts that it is supposed to demonstrate Tableau features, it is not even half as good or thorough as the tutorials on Tableau website itself.  It is rather apparent that Udacity launched this switch without having the course fully ready.  &lt;/p&gt;
&lt;p&gt;Even with the tutorials on Tableau website, Tableau desktop hasn't proven itself to be the best software.  There are lots of non self-explanatory error messages such as "Your connection has probably timed out.  Try to reconnect to Tableau Server.  "  or "Internal Error - An unexpected error has occurred and the operations could not be completed.  ".  It took a fair bit of puddling around, trial and errors, and observations, that I finally figured out what those messages mean, and how to work around it by logging out of the previous session, and re-logging back in to create a new session.  Note that this is really merely a workaround.  It is NOT a real solution as it wastes so much time to log out and log in while you work on a project.&lt;/p&gt;
&lt;p&gt;Another disappointment was that all the students on Data Analyst Nanodegree programme who agreed to the switch found out that we are not given the student license of Tableau, but those license was available for those who are on Frontend developer project.  Without the license, we are only allowed certain size of datasets.  The limit was somewhat around 15,000,000.  This is truly frustrating as I had all the ideas in mind on how to analysis the data and what kind of storyboard I wanted to create, only to find out later I can't tell the story I want.  &lt;/p&gt;
&lt;p&gt;This makes me feel highly dissatisfied with Udacity at the last couple months of my program.  I feel this is a very dishonest conduct of business, by launching something that's so far from ready.  Because of the lack of structural support and guidance for the last course, and license limitation with Tableau, I was at least delayed a month from graduation.  For someone who is paying the course with her own out-of-pocket money, this is just absurd.  Now will I be able to get refund for last month's subscription fees?  I doubt it ...... &lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/posts/tableau/</guid><pubDate>Sat, 29 Jul 2017 06:45:47 GMT</pubDate></item><item><title>Overview of A/B Testing</title><link>https://chow1026.github.io/course-notes/ab-testing/lesson1-overview/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h3&gt;What is AB Testing:&lt;/h3&gt;
&lt;p&gt;AB Testing is general methodology used online when one wants to test a new product or feature.  &lt;/p&gt;
&lt;h4&gt;Example Business Use Case:&lt;/h4&gt;
&lt;p&gt;Experiment the user's steps on Audacity, explore the customer funnel.  Experiment changing the color of a "Start Now" button from Orange to Pink.  See if the change has any impact on how many student explore  Audacity's courses.  &lt;/p&gt;
&lt;p&gt;Therefore, Our initial hypothesis 0: Changing the "Start Now" button &lt;em&gt;from Orange to Pink&lt;/em&gt; will increase the number of student exploring the Audacity courses.  &lt;/p&gt;
&lt;h5&gt;Metrics:&lt;/h5&gt;
&lt;p&gt;What audacity ultimately cares about is: How many total number of students actually complete the courses:&lt;/p&gt;
&lt;p&gt;Common Metrics
- Total Number of Courses Completed  (Takes long time to complete course, too long for experiment and for practical business feedback)
- Total number of Clicks (However this doesn't reflect the proportion of it to total visit)
- Click Through Rate [the number of clicks/number of page views] (this is slightly better button doesn't account for users who just visited without clicks, or users who clicks multiple times)
- Click Through Probability [unique visitors who clicks/unique visitors who visits] (This is the best for this use case)&lt;/p&gt;
&lt;p&gt;Generally we use "rate" when we want to measure the usability, whereas when we want to measure the total impact we use the "probability".  "Rate" measures usability of a button, ie. how often a user finds and clicks a button when placed on a site (with other buttons and links); ""&lt;/p&gt;
&lt;p&gt;Now, our upgraded hypothesis:
Changing the "Start Now" button from &lt;em&gt;Orange to Pink&lt;/em&gt; will &lt;strong&gt;increase the click-through probability&lt;/strong&gt; of the button.  &lt;/p&gt;
&lt;h4&gt;Estimating Click-Through Probability&lt;/h4&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/ab-testing/lesson1-overview/</guid><pubDate>Mon, 08 May 2017 05:37:59 GMT</pubDate></item><item><title>D3 Building Blocks</title><link>https://chow1026.github.io/course-notes/data-visualization/02-d3_building_blocks/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;h4&gt;1. D3 Version Notice&lt;/h4&gt;
&lt;p&gt;There's a new version of D3!
A new version of D3.js (version 4.0) was released on June 28th, 2016. This version changed the names of many of the functions in the library. From the release notes:&lt;/p&gt;
&lt;p&gt;... one unavoidable consequence of adopting ES6 modules: every symbol in D3 4.0 now shares a flat namespace rather than the nested one of D3 3.x. For example, d3.scale.linear is now d3.scaleLinear, and d3.layout.treemap is now d3.treemap.
This means that the D3 code we have in this material will not work with version 4 of D3.js. We're working on updating the code, but for now, please keep this in mind as you are working through the course. Feel free to use either version 3 or version 4 for the final project.&lt;/p&gt;
&lt;h4&gt;2. Technical Interlude&lt;/h4&gt;
&lt;p&gt;Scott Murray provides an excellent overview of these fundamentals on his website, &lt;a href="http://alignedleft.com/tutorials/d3/fundamentals" title="Scott Murray's Aligned Left"&gt;alignedleft.com&lt;/a&gt;. (time estimate: 10 minutes)&lt;/p&gt;
&lt;p&gt;To learn the basics of HTML, CSS, and Javascript, we encourage you to work through Project 1 and Project 2 at https://dash.generalassemb.ly/. (time estimate: 10-20 hours)&lt;/p&gt;
&lt;p&gt;Scott Murray also provides an excellent &lt;a href="http://alignedleft.com/tutorials/d3" title="Scott Murray's D3 Tutorial"&gt;tutorial of creating a visualization using D3.js&lt;/a&gt; if you'd like to get a preview of what's to come in later lessons. (time estimate: 10-15 hours)&lt;/p&gt;
&lt;p&gt;If you have more time or want to get more comfortable with HTML and CSS prior to continuing in this course, consider taking our &lt;a href="https://www.udacity.com/course/ud304" title="Udacity's Intro to HTML and CSS course"&gt;Intro to HTML and CSS&lt;/a&gt;d course. (time estimate: 18+ hours)&lt;/p&gt;
&lt;h4&gt;3.&lt;/h4&gt;
&lt;h4&gt;28. Server Requests And D3&lt;/h4&gt;
&lt;p&gt;A simple &lt;a href="http://chimera.labs.oreilly.com/books/1230000000345/ch03.html#_the_web" title="Overview of Requests and Responses"&gt;overview of requests and responses&lt;/a&gt; from Scott Murray's Interactive Data Visualization for the Web. Be sure to scroll to the top of the page to the section titled The Web.&lt;/p&gt;
&lt;p&gt;More &lt;a href="http://eloquentjavascript.net/17_http.html" title="Javascript Requests"&gt;detail on requests and javascript&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;@5:17 Jonathan mentions that the server can send an AJAX request.&lt;/p&gt;
&lt;p&gt;With &lt;a href="http://en.wikipedia.org/wiki/Ajax_(programming)" title="AJAX"&gt;AJAX&lt;/a&gt;, web applications can send data to, and retrieve data from, a server asynchronously (in the background) without interfering with the display and behavior of the existing page. The data sent and returned is usually in the format of JSON (see &lt;a href="http://en.wikipedia.org/wiki/AJAJ" title="AJAJ"&gt;AJAJ&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt;29.  Let's Make A Bar Chart&lt;/h4&gt;
&lt;p&gt;In the following videos, Jonathan will be working through part II of Mike Bostock's &lt;a href="http://bost.ocks.org/mike/bar/" title="Bar Chart Tutorial"&gt;Let's Make a Bar Chart Tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We recommend going through Part I and Part II of the tutorial.&lt;/p&gt;
&lt;p&gt;To examine the code files, we recommend using a text editor such as Sublime Text 3. You are welcome to use any text editor.&lt;/p&gt;
&lt;h4&gt;30. Code Structure And JavaScript&lt;/h4&gt;
&lt;p&gt;The code that Jonathan outlines is an adaptation from &lt;a href="http://bost.ocks.org/mike/bar/2/" title="Part II of Bar Chart Tutorial"&gt;Part II of Mike Bostock's Let's Make a Bar Chart&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can follow along with Jonathan by downloading the associated files and from the Downloadables section.&lt;/p&gt;
&lt;h4&gt;31. Layout And Scales&lt;/h4&gt;
&lt;p&gt;Mike Bostock's &lt;a href="http://bost.ocks.org/mike/bar/" title="Bar Chart Tutorial"&gt;Let's Make a Bar Chart Tutorial&lt;/a&gt;.  &lt;/p&gt;
&lt;p&gt;@2:10 Jonathan discusses adding the domain to the linear scale, x. Keep in mind that you can set the domain and range of the linear scale separately or in a few lines of code using the method chaining syntax.&lt;/p&gt;
&lt;p&gt;@3:14 Jonathan discusses passing a general function to the d3.max function. This "unnamed" function returns the information stored in the "value" property for each row of data in the data.tsv file. Keep in mind that each row of thedata.tsv file will be represented as a JavaScript object.&lt;/p&gt;
&lt;p&gt;For example, take a look at the data.tsv file. Each row of the data.tsv file is a JavaScript object inside the data array. The first Javascript object then is {"name": "Locke", "value": 4}. In our case, the "unnamed" function(d) is defined to return d.value so the first data object returns the value 4. The function d3.max returns the largest value of all the values that are stored under the "value" property in the JavaScript objects.&lt;/p&gt;
&lt;p&gt;The beauty of d3 is the ability to generalize though! We could define the "unnamed" function to be any function. Instead of pulling information from the "value" property, we could use the length of each name from the "name" property. Depending on what data you are trying to represent in your scale, you may write a different function(d) to go inside of the d3.max().&lt;/p&gt;
&lt;p&gt;You don't have to pass a function into d3.max(). If you don't pass a function, then d3.max(array) returns the maximum element in the array based on "natural order". See the documentation for more information. &lt;a href="https://github.com/mbostock/d3/wiki/Arrays#d3_max"&gt;D3 Arrays&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;32. Binding Data&lt;/h4&gt;
&lt;p&gt;Jonathan will cover the details of how exactly D3 binds data later in the course, but if you are interested in learning more now, Mike Bostock's &lt;a href="http://bost.ocks.org/mike/selection/" title="How Selection Work"&gt;How Selections Work&lt;/a&gt; provides a great overview.&lt;/p&gt;
&lt;h4&gt;33. Adding Bars And Text&lt;/h4&gt;
&lt;p&gt;Much of this code might leave you confused. At this point in the course, you should not understand every line of code. Instead, you should focus on the structure of the code and any common functions or patterns that you see.&lt;/p&gt;
&lt;p&gt;Throughout this code, you might notice the pattern function(d) { ... some code ...}.&lt;/p&gt;
&lt;p&gt;These are anonymous accessor functions that access a data value using the parameter d to return a calculated value or string. You will learn about &lt;a href="https://www.udacity.com/course/viewer#!/c-ud507/l-3069149263/m-3071138998" title="Anonymous Accessor Functions"&gt;anonymous accessor functions&lt;/a&gt; and &lt;a href="https://www.udacity.com/course/viewer#!/c-ud507/l-3069149263/m-3071139018" title="How D3 Binds Data to SVG Elements"&gt;how D3 binds data to SVG elements&lt;/a&gt; in Lesson 3.&lt;/p&gt;
&lt;h4&gt;34. From Source Code To Graphic&lt;/h4&gt;
&lt;p&gt;If you would like to run these visualizations locally, you will need to start an HTTP sever. There will be instructions for doing this later in &lt;a href="https://www.udacity.com/course/viewer#!/c-ud507/l-3168988586/m-3063989000" title="Lesson 2b"&gt;Lesson 2b&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/data-visualization/02-d3_building_blocks/</guid><pubDate>Sat, 22 Apr 2017 06:33:29 GMT</pubDate></item><item><title>Visualization Fundamentals</title><link>https://chow1026.github.io/course-notes/data-visualization/01-visualization_fundamentals/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Data visualization&lt;/strong&gt; or &lt;strong&gt;data visualisation&lt;/strong&gt; is viewed by many disciplines as a modern equivalent of &lt;a href="https://en.wikipedia.org/wiki/Visual_communication" title="Visual Communication"&gt;visual communication&lt;/a&gt;. It involves the creation and study of the visual representation of data, meaning "information that has been abstracted in some schematic form, including attributes or variables for the units of information".&lt;/p&gt;
&lt;p&gt;A primary goal of data visualization is to communicate information clearly and efficiently via &lt;a href="https://en.wikipedia.org/wiki/Statistical_graphics" title="Statistical Graphics"&gt;statistical graphics&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Plot_(graphics)" title="Plots"&gt;plots&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Infographic" title="Infographics"&gt;information graphics&lt;/a&gt;. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message. Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding &lt;a href="https://en.wikipedia.org/wiki/Causality" title="Causality"&gt;causality&lt;/a&gt;, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.&lt;/p&gt;
&lt;p&gt;Data visualization is both an art and a science. It is viewed as a branch of &lt;a href="https://en.wikipedia.org/wiki/Descriptive_statistics" title="Descriptive Statistics"&gt;descriptive statistics&lt;/a&gt; by some, but also as a &lt;a href="https://en.wikipedia.org/wiki/Grounded_theory" title="Grounded Theory"&gt;grounded theory&lt;/a&gt; development tool by others. Increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as "&lt;a href="https://en.wikipedia.org/wiki/Big_data" title="Big Data"&gt;big data&lt;/a&gt;" or &lt;a href="https://en.wikipedia.org/wiki/Internet_of_things" title="Internet of Things"&gt;Internet of things&lt;/a&gt;. Processing, analyzing and communicating this data present ethical and analytical challenges for data visualization. The field of &lt;a href="https://en.wikipedia.org/wiki/Data_science" title="Data Science"&gt;data science&lt;/a&gt; and practitioners called &lt;a href="https://en.wikipedia.org/wiki/Data_scientists" title="Data Scientists"&gt;data scientists&lt;/a&gt; help address this challenge.&lt;/p&gt;
&lt;p&gt;Contents  [hide]
1   Overview
2   Characteristics of effective graphical displays
3   Quantitative messages
4   Visual perception and data visualization
4.1 Human perception/cognition and data visualization
5   History of Data Visualization
6   Terminology
7   Examples of diagrams used for data visualization
8   Other perspectives
9   Data presentation architecture
9.1 Objectives
9.2 Scope
9.3 Related fields
10  See also
10.1    People (Historical)
10.1.1  People (active today)
11  References
12  Further reading
13  External links&lt;/p&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Data visualization is one of the steps in analyzing data and presenting it to users.
Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in &lt;a href="https://en.wikipedia.org/wiki/Data_analysis" title="Data Analysis"&gt;data analysis&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Data_science" title="Data Science"&gt;data science&lt;/a&gt;. According to Friedman (2008) the "main goal of data visualization is to communicate information clearly and effectively through graphical means. It doesn't mean that data visualization needs to look boring to be functional or extremely sophisticated to look beautiful. To convey ideas effectively, both aesthetic form and functionality need to go hand in hand, providing insights into a rather sparse and complex data set by communicating its key-aspects in a more intuitive way. Yet designers often fail to achieve a balance between form and function, creating gorgeous data visualizations which fail to serve their main purpose — to communicate information".&lt;/p&gt;
&lt;p&gt;Indeed, Fernanda Viegas and Martin M. Wattenberg have suggested that an ideal visualization should not only communicate clearly, but stimulate viewer engagement and attention.[6]&lt;/p&gt;
&lt;p&gt;Data visualization is closely related to &lt;a href="https://en.wikipedia.org/wiki/Infographic" title="Infographics"&gt;information graphics&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Information_visualization" title="Information Visualization"&gt;information visualization&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Scientific_visualization" title="Scientific Visualization"&gt;scientific visualization&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" title="Exploratory Data Analysis"&gt;exploratory data analysis&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Statistical_graphics" title="Statistical Graphics"&gt;statistical graphics&lt;/a&gt;. In the new millennium, data visualization has become an active area of research, teaching and development. According to Post et al. (2002), it has united scientific and information visualization.&lt;/p&gt;
&lt;h3&gt;Characteristics of effective graphical displays&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Minards Map" src="https://commons.wikimedia.org/wiki/File:Minard%27s_Map_(vectorized"&gt;.svg#/media/File:Minard%27s_Map_(vectorized).svg)
"Charles Joseph Minard's 1869 diagram of Napoleon's March - an early example of an information graphic."    &lt;/p&gt;
&lt;p&gt;"The greatest value of a picture is when it forces us to notice what we never expected to see."  by John Tukey&lt;/p&gt;
&lt;p&gt;Professor Edward Tufte explained that users of information displays are executing particular analytical tasks such as making comparisons or determining causality. The design principle of the information graphic should support the analytical task, showing the comparison or causality.[9]&lt;/p&gt;
&lt;p&gt;In his 1983 book &lt;em&gt;The Visual Display of Quantitative Information&lt;/em&gt;, Edward Tufte defines 'graphical displays' and principles for effective graphical display in the following passage: "Excellence in statistical graphics consists of complex ideas communicated with clarity, precision and efficiency. Graphical displays should:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;show the data    &lt;/li&gt;
&lt;li&gt;induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else    &lt;/li&gt;
&lt;li&gt;avoid distorting what the data has to say     &lt;/li&gt;
&lt;li&gt;present many numbers in a small space     &lt;/li&gt;
&lt;li&gt;make large data sets coherent    &lt;/li&gt;
&lt;li&gt;encourage the eye to compare different pieces of data    &lt;/li&gt;
&lt;li&gt;reveal the data at several levels of detail, from a broad overview to the fine structure     &lt;/li&gt;
&lt;li&gt;serve a reasonably clear purpose: description, exploration, tabulation or decoration    &lt;/li&gt;
&lt;li&gt;be closely integrated with the statistical and verbal descriptions of a data set.    &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations."&lt;/p&gt;
&lt;p&gt;For example, the Minard diagram shows the losses suffered by Napoleon's army in the 1812–1813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, direction of movement, and temperature. The line width illustrates a comparison (size of the army at points in time) while the temperature axis suggests a cause of the change in army size. This multivariate display on a two dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility. Tufte wrote in 1983 that: "It may well be the best statistical graphic ever drawn."[10]&lt;/p&gt;
&lt;p&gt;Not applying these principles may result in &lt;a href="https://en.wikipedia.org/wiki/Misleading_graphs" title="Misleading Graphs"&gt;misleading graphs&lt;/a&gt;, which distort the message or support an erroneous conclusion. According to Tufte, &lt;a href="https://en.wikipedia.org/wiki/Chartjunk" title="Chart Junk"&gt;chartjunk&lt;/a&gt;to extraneous interior decoration of the graphic that does not enhance the message, or gratuitous three dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of "administrative debris." The ratio of "data to ink" should be maximized, erasing non-data ink where feasible.[10]&lt;/p&gt;
&lt;p&gt;The Congressional Budget Office summarized several best practices for graphical displays in a June 2014 presentation. These included:
a) Knowing your audience;   &lt;br&gt;
b) Designing graphics that can stand alone outside the context of the report; and   &lt;br&gt;
c) Designing graphics that communicate the key messages in the report.&lt;/p&gt;
&lt;h3&gt;Quantitative messages&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Total Revenue and Outlays Time Series" src="https://commons.wikimedia.org/wiki/File:Total_Revenues_and_Outlays_as_Percent_GDP_2013.png#/media/File:Total_Revenues_and_Outlays_as_Percent_GDP_2013.png"&gt;
A time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time.&lt;/p&gt;
&lt;p&gt;&lt;img alt="US Phillips Curve" src="https://commons.wikimedia.org/wiki/File:U.S._Phillips_Curve_2000_to_2013.png#/media/File:U.S._Phillips_Curve_2000_to_2013.png"&gt;
A scatterplot illustrating negative correlation between two variables (inflation and unemployment) measured at points in time.&lt;/p&gt;
&lt;p&gt;Author Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message:      &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A &lt;a href="https://en.wikipedia.org/wiki/Line_chart" title="Line Chart"&gt;line chart&lt;/a&gt; may be used to demonstrate the trend.     &lt;/li&gt;
&lt;li&gt;Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the &lt;em&gt;measure&lt;/em&gt;) by sales persons (the &lt;em&gt;category&lt;/em&gt;, with each sales person a &lt;em&gt;categorical subdivision&lt;/em&gt;) during a single period. A &lt;a href="https://en.wikipedia.org/wiki/Bar_chart" title="Bar Chart"&gt;bar chart&lt;/a&gt; may be used to show the comparison across the sales persons.     &lt;/li&gt;
&lt;li&gt;Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A &lt;a href="https://en.wikipedia.org/wiki/Pie_chart" title="Pie Chart"&gt;pie chart&lt;/a&gt; or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.      &lt;/li&gt;
&lt;li&gt;Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.       &lt;/li&gt;
&lt;li&gt;Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A &lt;a href="https://en.wikipedia.org/wiki/Histogram" title="Histogram"&gt;histogram&lt;/a&gt;, a type of bar chart, may be used for this analysis. A &lt;a href="https://en.wikipedia.org/wiki/Boxplot" title="Boxplot"&gt;boxplot&lt;/a&gt; helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.      &lt;/li&gt;
&lt;li&gt;Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A &lt;a href="https://en.wikipedia.org/wiki/Scatter_plot" title="Scatter Plot"&gt;scatter plot&lt;/a&gt; is typically used for this message.     &lt;/li&gt;
&lt;li&gt;Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.       &lt;/li&gt;
&lt;li&gt;Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A &lt;a href="https://en.wikipedia.org/wiki/Cartogram" title="Cartogram"&gt;cartogram&lt;/a&gt; is a typical graphic used.     &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Analysts reviewing a set of data may consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of &lt;a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" title="Exploratory Data Analysis"&gt;exploratory data analysis&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Visual perception and data visualization&lt;/h3&gt;
&lt;p&gt;A human can distinguish differences in line length, shape, orientation, and color (hue) readily without significant processing effort; these are referred to as "&lt;a href="https://en.wikipedia.org/wiki/Pre-attentive_processing" title="Pre-attentive Processing"&gt;pre-attentive attributes&lt;/a&gt;". For example, it may require significant time and effort ("attentive processing") to identify the number of times the digit "5" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.&lt;/p&gt;
&lt;p&gt;Effective graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).&lt;/p&gt;
&lt;h4&gt;Human perception/cognition and data visualization&lt;/h4&gt;
&lt;p&gt;Almost all data visualizations are created for human consumption. Knowledge of human perception and cognition is necessary when designing intuitive visualizations.  Cognition refers to processes in human beings like perception, attention, learning, memory, thought, concept formation, reading, and problem solving. Human visual processing is efficient in detecting changes and making comparisons between quantities, sizes, shapes and variations in lightness. When properties of symbolic data are mapped to visual few properties, humans can browse through large amounts of data efficiently.  &lt;a href="https://en.wikipedia.org/wiki/Data_exploration" title="Data Exploration"&gt;It&lt;/a&gt; is estimated that 2/3 of the brain's neurons can be involved in visual processing.  Proper visualization provides a different approach to show potential connections, relationships, etc. which are not as obvious in non-visualized quantitative data. Visualization can become a means of data exploration.&lt;/p&gt;
&lt;h3&gt;History of Data Visualization&lt;/h3&gt;
&lt;p&gt;There is a history of data visualization: beginning in the 2nd century C.E. with data arrangement into columns and rows and evolving to the initial quantitative representations in the 17th century.[14] According to the Interaction Design Foundation, French philosopher and mathematician René Descartes laid the ground work for Scotsman William Playfair. Descartes developed a two-dimensional coordinate system for displaying values, which in the late 18th century Playfair saw potential for graphical communication of quantitative data.[14] In the second half of the 20th century, Jacques Bertin used quantitative graphs to represent information "intuitively, clearly, accurately, and efficiently".[14]&lt;/p&gt;
&lt;p&gt;John Tukey and Edward Tufte pushed the bounds of data visualization; Tukey with his new statistical approach of exploratory data analysis and Tufte with his book "The Visual Display of Quantitative Information" paved the way for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand drawn visualizations and evolving into more technical applications – including interactive designs leading to software visualization.[17] Programs like &lt;a href="https://en.wikipedia.org/wiki/SAS_(software)" title="SAS"&gt;SAS&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/SOFA_Statistics" title="SOFA Statistics"&gt;SOFA&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/R_(programming_language)" title="R"&gt;R&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Minitab" title="Minitab"&gt;Minitab&lt;/a&gt;, and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility.&lt;/p&gt;
&lt;h3&gt;Terminology&lt;/h3&gt;
&lt;p&gt;Data visualization involves specific terminology, some of which is derived from statistics. For example, author Stephen Few defines two types of data, which are used in combination to support a meaningful analysis or visualization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Categorical: Text labels describing the nature of the data, such as "Name" or "Age". This term also covers qualitative (non-numerical) data.    &lt;/li&gt;
&lt;li&gt;Quantitative: Numerical measures, such as "25" to represent the age in years.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two primary types of information displays are tables and graphs.     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A table contains quantitative data organized into rows and columns with categorical labels. It is primarily used to look up specific values. In the example above, the table might have categorical column labels representing the name (a qualitative variable) and age (a quantitative variable), with each row of data representing one person (the sampled experimental unit or category subdivision).      &lt;/li&gt;
&lt;li&gt;A graph is primarily used to show relationships among data and portrays values encoded as visual objects (e.g., lines, bars, or points). Numerical values are displayed within an area delineated by one or more axes. These axes provide scales (quantitative and categorical) used to label and assign values to the visual objects. Many graphs are also referred to as charts.      &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;KPI Library has developed the "&lt;a href="http://www.visual-literacy.org/periodic_table/periodic_table.html" title="Periodic Table of Visualization Methods"&gt;Periodic Table of Visualization Methods&lt;/a&gt;", an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound.&lt;/p&gt;
&lt;h3&gt;Other perspectives&lt;/h3&gt;
&lt;p&gt;There are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008) presented it. In this way Friendly (2008) presumes two main parts of data visualization: &lt;a href="https://en.wikipedia.org/wiki/Statistical_graphics" title="Statistical Graphics"&gt;statistical graphics&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Thematic_map" title="Thematic Cartography"&gt;thematic cartography&lt;/a&gt;. In this line the "Data Visualization: Modern Approaches" (2007) article gives an overview of seven subjects of data visualization:         &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Articles &amp;amp; resources     &lt;/li&gt;
&lt;li&gt;Displaying connections    &lt;/li&gt;
&lt;li&gt;Displaying data    &lt;/li&gt;
&lt;li&gt;Displaying news   &lt;/li&gt;
&lt;li&gt;Displaying websites     &lt;/li&gt;
&lt;li&gt;Mind maps    &lt;/li&gt;
&lt;li&gt;Tools and services     &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these subjects are closely related to &lt;a href="https://en.wikipedia.org/wiki/Graphic_design" title="Graphic Design"&gt;graphic design&lt;/a&gt; and information representation.&lt;/p&gt;
&lt;p&gt;On the other hand, from a computer science perspective, Frits H. Post (2002) categorized the field into a number of sub-fields:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Information_visualization" title="Information Visualization"&gt;Information visualization&lt;/a&gt;    &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Interaction_techniques" title="Interaction Techniques"&gt;Interaction techniques&lt;/a&gt; and architectures      &lt;/li&gt;
&lt;li&gt;Modelling techniques    &lt;/li&gt;
&lt;li&gt;Multiresolution methods      &lt;/li&gt;
&lt;li&gt;Visualization algorithms and techniques      &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Volume_visualization" title="Volume Visualization"&gt;Volume visualization&lt;/a&gt;     &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data presentation architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Facebook Network" src="https://upload.wikimedia.org/wikipedia/commons/9/90/Kencf0618FacebookNetwork.jpg"&gt;
A data visualization from social media&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data presentation architecture (DPA)&lt;/strong&gt; is a skill-set that seeks to identify, locate, manipulate, format and present data in such a way as to optimally communicate meaning and proper knowledge.&lt;/p&gt;
&lt;p&gt;Historically, the term data presentation architecture is attributed to Kelly Lautt:[22] "Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of &lt;a href="https://en.wikipedia.org/wiki/Business_Intelligence" title="Business Intelligence"&gt;Business Intelligence&lt;/a&gt;. Data presentation architecture weds the science of numbers, data and statistics in &lt;a href="https://en.wikipedia.org/wiki/Information_discovery" title="Information Discovery"&gt;discovering valuable information&lt;/a&gt; from data and making it usable, relevant and actionable with the arts of data visualization, communications, &lt;a href="https://en.wikipedia.org/wiki/Organizational_psychology" title="Organization Psychology"&gt;organizational psychology&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Change_management" title="Change Management"&gt;change management&lt;/a&gt; in order to provide business intelligence solutions with the data scope, delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen. Data visualization skills are one element of DPA."&lt;/p&gt;
&lt;h4&gt;Objectives&lt;/h4&gt;
&lt;p&gt;DPA has two main objectives:     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To use data to provide knowledge in the most efficient manner possible (minimize noise, complexity, and unnecessary data or detail given each audience's needs and roles)       &lt;/li&gt;
&lt;li&gt;To use data to provide knowledge in the most effective manner possible (provide relevant, timely and complete data to each audience member in a clear and understandable manner that conveys important meaning, is actionable and can affect understanding, behavior and decisions)     &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Scope&lt;/h4&gt;
&lt;p&gt;With the above objectives in mind, the actual work of data presentation architecture consists of:       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating effective delivery mechanisms for each audience member depending on their role, tasks, locations and access to technology     &lt;/li&gt;
&lt;li&gt;Defining important meaning (relevant knowledge) that is needed by each audience member in each context     &lt;/li&gt;
&lt;li&gt;Determining the required periodicity of data updates (the currency of the data)     &lt;/li&gt;
&lt;li&gt;Determining the right timing for data presentation (when and how often the user needs to see the data)     &lt;/li&gt;
&lt;li&gt;Finding the right data (subject area, historical reach, breadth, level of detail, etc.)       &lt;/li&gt;
&lt;li&gt;Utilizing appropriate analysis, grouping, visualization, and other presentation formats     &lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Related fields&lt;/h4&gt;
&lt;p&gt;DPA work shares commonalities with several other fields, including:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Business analysis in determining business goals, collecting requirements, mapping processes.     &lt;/li&gt;
&lt;li&gt;Business process improvement in that its goal is to improve and streamline actions and decisions in furtherance of business goals      &lt;/li&gt;
&lt;li&gt;Data visualization in that it uses well-established theories of visualization to add or highlight meaning or importance in data presentation.      &lt;/li&gt;
&lt;li&gt;Graphic or user design: As the term DPA is used, it falls just short of design in that it does not consider such detail as colour palates, styling, branding and other aesthetic concerns, unless these design elements are specifically required or beneficial for communication of meaning, impact, severity or other information of business value. For example:      &lt;ul&gt;
&lt;li&gt;choosing locations for various data presentation elements on a presentation page (such as in a company portal, in a report or on a web page) in order to convey hierarchy, priority, importance or a rational progression for the user is part of the DPA skill-set.      &lt;/li&gt;
&lt;li&gt;choosing to provide a specific colour in graphical elements that represent data of specific meaning or concern is part of the DPA skill-set       &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Information architecture, but information architecture's focus is on unstructured data and therefore excludes both analysis (in the statistical/data sense) and direct transformation of the actual content (data, for DPA) into new entities and combinations.        &lt;/li&gt;
&lt;li&gt;Solution architecture in determining the optimal detailed solution, including the scope of data to include, given the business goals        &lt;/li&gt;
&lt;li&gt;Statistical analysis or data analysis in that it creates information and knowledge out of data       &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See also[edit]
Analytics
Balanced scorecard
Business analysis
Business intelligence
Data analysis
Data profiling
Data warehouse
Exploratory data analysis
Infographic
Information architecture
Information design
Information visualization
Interaction design
Interaction techniques
Scientific visualization
Software visualization
Statistical analysis
Statistical graphics
Visual analytics
People (Historical)[edit]
Charles Joseph Minard
John Tukey
John Snow
Otto Neurath
Florence Nightingale
William Playfair
People (active today)[edit]
Alberto Cairo
Edward Tufte
Ola Rosling - Rosling developed the scatter-plot graphing tool used on Gapminder.org.
Hans Rosling
Aaron Koblin
Manuel Lima
Max Roser - Roser is an economist at the University of Oxford and author of the online data visualisation publication Our World In Data.
Moritz Stefaner
Ben Shneiderman
Fernanda Viégas
Martin M. Wattenberg
Mona Chalabi - Data journalist at FiveThirtyEight. Previously at the Guardian, the Bank of England, and the Economist Intelligence Unit.
George Furnas
Branko Milanovic
Mike Bostock - Bostock is one of the key developers of the Javascript library d3.js.
Adrien Segal - Oakland, CA based artist known for her sculptures based on tidal and snow data.
Jeffrey Heer
Leland Wilkinson
Hadley Wickham&lt;/p&gt;
&lt;h3&gt;Video Lesson Notes:&lt;/h3&gt;
&lt;h4&gt;1. What is Data Visualization&lt;/h4&gt;
&lt;h4&gt;2. Defining Data Visualization&lt;/h4&gt;
&lt;p&gt;You can find Scott Murray's book, &lt;a href="http://chimera.labs.oreilly.com/books/1230000000345/" title="Scott Murray's Interactive Data Visualization for the Web"&gt;Interactive Data Visualization for the Web: An Introduction to Designing with D3&lt;/a&gt;, here.      &lt;/p&gt;
&lt;p&gt;Cole Nussbaumer published an excellent book, &lt;a href="http://www.amazon.com/gp/product/1119002257?ie=UTF8&amp;amp;creativeASIN=1119002257&amp;amp;linkCode=xm2&amp;amp;tag=storytellingwithdata-20" title="Cole Nussbaumer's Storytelling with Data"&gt;Storytelling with Data: a Data Visualization Guide for Business Professionals&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Storytelling with Data teaches you the fundamentals of data visualization and how to communicate effectively with data. You'll discover the power of storytelling and the way to make data a pivotal point in your story. The lessons in this illuminative text are grounded in theory, but made accessible through numerous real-world examples—ready for immediate application to your next graph or presentation.&lt;/p&gt;
&lt;h4&gt;3. Good Data Visualization&lt;/h4&gt;
&lt;p&gt;Cole Nussbaumer published an excellent book, Storytelling with Data: a Data Visualization Guide for Business Professionals.     &lt;/p&gt;
&lt;p&gt;Storytelling with Data teaches you the fundamentals of data visualization and how to communicate effectively with data. You'll discover the power of storytelling and the way to make data a pivotal point in your story. The lessons in this illuminative text are grounded in theory, but made accessible through numerous real-world examples—ready for immediate application to your next graph or presentation.&lt;/p&gt;
&lt;p&gt;You can also learn more about data visualization by reading Cole's blog, &lt;a href="http://www.storytellingwithdata.com/" title="Storytelling with Data"&gt;storytellingwithdata.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you aren't sure where to start looking, try the following links.    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;https://twitter.com/nytgraphics  &lt;br&gt;
 http://www.smallmeans.com/new-york-times-infographics/    &lt;/li&gt;
&lt;li&gt;http://flowingdata.com/    &lt;/li&gt;
&lt;li&gt;http://infosthetics.com/    &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find the &lt;a href="https://discussions.udacity.com/t/lesson-1a-good-data-visualizations-dand/25155/30" title="Good Data Visualization Discussion"&gt;Good Data Visualizations&lt;/a&gt; discussion topic here.&lt;/p&gt;
&lt;h4&gt;4. The Functional Art: Obesity And Education&lt;/h4&gt;
&lt;p&gt;Blog Post
Alberto Cairo's Three Steps to Become a Visualization Designer by Andy Kriebel&lt;/p&gt;
&lt;h4&gt;5. Slope Graphs&lt;/h4&gt;
&lt;p&gt;@0:42 Cole misspeaks by saying "and a positive relationship here in the red states". She meant to say "and an inverse relationship in the red states". @2:00 it is stated that the slope of the lines can show the percent difference between two groups, which is not precisely true. The slope graph does indicate direction and magnitude of change.&lt;/p&gt;
&lt;p&gt;Click the following link to view an image of the &lt;a href="http://www.storytellingwithdata.com/2014/03/more-on-slopegraphs.html" title="Slope Graph"&gt;slope graph&lt;/a&gt;. You can learn more about data visualization from Cole's blog, Storytelling with Data.&lt;/p&gt;
&lt;h4&gt;6. Words of Wisdom from Zipfian Academy&lt;/h4&gt;
&lt;h4&gt;7. Design Code Tell&lt;/h4&gt;
&lt;h4&gt;8. Data Visualization and You&lt;/h4&gt;
&lt;h4&gt;9. The Data Science Process&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Computer Science&lt;/th&gt;
&lt;th&gt;Statistics and Data Mining&lt;/th&gt;
&lt;th&gt;Graphic Design&lt;/th&gt;
&lt;th&gt;InfoVis and HCI&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;acquire     parse&lt;/td&gt;
&lt;td&gt;filter                mine&lt;/td&gt;
&lt;td&gt;represent   refine&lt;/td&gt;
&lt;td&gt;interact&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;10. Details of the Data Science Process&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Data_wrangling" title="Data Wrangling"&gt;Data Munging or Data Wrangling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;InfoVis stands for &lt;a href="http://en.wikipedia.org/wiki/Information_visualization" title="Information Visualization"&gt;Information Visualization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;HCI stands for &lt;a href="http://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction" title="Human Computer Interaction"&gt;Human-Computer Interaction&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;11. DataViz in Data Science&lt;/h4&gt;
&lt;h4&gt;12. Iterating through Visualization&lt;/h4&gt;
&lt;h4&gt;13. Exploratory vs Explainatory&lt;/h4&gt;
&lt;h4&gt;14. Anscombe’s Quartet&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/Anscombe's_quartet" title="Anscombe’s Quartet"&gt;Anscombe’s Quartet &lt;/a&gt;
The plots were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties.&lt;/p&gt;
&lt;h4&gt;15. Why Even Create Graphics?&lt;/h4&gt;
&lt;p&gt;@0:44 Chris discusses the percentage 0.7%. He means to say that our awareness is only 0.7% of our total "bandwidth."&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0002NC" title="Edward Tufte on Visual Perceptions"&gt;Edward Tufte elaborates on the speed of visual perception on his website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Penn researchers calculate how much the eye tells the brain in &lt;a href="http://www.eurekalert.org/pub_releases/2006-07/uops-prc072606.php" title="Penn State Paper on Visual Perceptions"&gt;this paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;16. Data Types&lt;/h4&gt;
&lt;p&gt;Hans Rosling’s 200 Countries, 200 Years, 4 Minutes https://www.youtube.com/watch?v=jbkSRLYSojo&lt;/p&gt;
&lt;p&gt;Different Data Types 1 - Numerical Data https://classroom.udacity.com/courses/ud359/lessons/692548568/concepts/6785689350923&lt;/p&gt;
&lt;p&gt;Different Data Types 2 - Categorical Data https://www.udacity.com/course/viewer#!/c-ud359/l-692548568/m-678568936&lt;/p&gt;
&lt;p&gt;Different Data Types 3 - Time Series Data https://classroom.udacity.com/courses/ud359/lessons/692548568/concepts/6785689370923&lt;/p&gt;
&lt;h4&gt;17. Identifying Data Types&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://charliepark.org/images/slopegraphs/natgeo_scatter.jpg" title="OECD Health Data 2009 Plot"&gt;Healthcare spending and life expectancy from OECD Health Data 2009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;@0:46 Chris says "health insurance" instead of "universal health coverage".&lt;/p&gt;
&lt;h4&gt;18. Visual Encodings&lt;/h4&gt;
&lt;p&gt;@0:34 spending per person should be income per person&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.gapminder.org/world/" title="Gapminder"&gt;Gapminder World Visualization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stephen Few's article on &lt;a href="http://www.perceptualedge.com/articles/b-eye/encoding_values_in_graph.pdf" title="Stephen Few's Rules for Visual Encodings"&gt;Rules for Using Encodings in Graphs&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;19.  World Cup Top Scores&lt;/h4&gt;
&lt;h4&gt;20.  Win/Lose/Draw&lt;/h4&gt;
&lt;h4&gt;21. Rankings Of Visual Encodings&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://web.cs.dal.ca/~sbrooks/csci4166-6406/seminars/readings/Cleveland_GraphicalPerception_Science85.pdf" title="Graphical Perception Paper by Cleveland and McGill"&gt;Graphical Perception Paper by Cleveland and McGill&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://flowingdata.com/2010/03/20/graphical-perception-learn-the-fundamentals-first/" title="Summary of the Paper"&gt;Nathan's Yau summarizes the findings of the paper and provides sound advice&lt;/a&gt; for putting it into practice on his blog, Flowing Data.&lt;/p&gt;
&lt;p&gt;tl;dr
Keep in mind this list isn’t meant to be a definitive answer on what to use and what not to in your data graphics.&lt;/p&gt;
&lt;p&gt;Cleveland and McGill note, “The ordering does not result in a precise prescription for displaying data but rather is a framework within which to work.” -Nathan Yau&lt;/p&gt;
&lt;h4&gt;22.  Decomposing Visualizations&lt;/h4&gt;
&lt;p&gt;This is a chance for you to find more visualizations in the wild!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.targetprocess.com/articles/visual-encoding.html" title="Decomposing Visualization"&gt;Decomposing Visualizations Examples&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;23. The Facebook Offering&lt;/h4&gt;
&lt;h4&gt;24. The Facebook Offering Animation&lt;/h4&gt;
&lt;p&gt;The &lt;a href="http://charts.animateddata.co.uk/uktemperaturelines/" title="UK Temperature History"&gt;UK Temperature History&lt;/a&gt; is an interesting data visualization that uses the z-axis to animate and encode changes over time. Can you spot other interesting parts of the design? Post your thoughts in the discussions!&lt;/p&gt;
&lt;h4&gt;25. Transition to the Tech Portion&lt;/h4&gt;
&lt;h4&gt;26. Ups and Downs of the Visualization Spectrum&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Visualization Spectrum" src="https://chow1026.github.io/course-notes/data-visualization/01-visualization_fundamentals/The_Visualization_Spectrum.png"&gt;
Some of the Visualization Technologies   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API" title="HTML5 Canvas"&gt;HTML5 Canvas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/WebGL" title="WebGL"&gt;WebGL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/SVG" title="SVG"&gt;SVG (Scalable Vector Graphics)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://d3js.org/" title="D3"&gt;D3.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nvd3.org/" title="NVD3"&gt;NVD3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dimplejs.org/" title="Dimple JS"&gt;Dimple.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://code.shutterstock.com/rickshaw/" title="Rickshaw"&gt;Rickshaw&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://chartio.com/" title="Chart IO"&gt;Chartio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://raw.densitydesign.org/" title="Raw JS"&gt;RAW&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;27. D3 Web Technologies&lt;/h4&gt;
&lt;p&gt;Based on HTML, Javascript, CSS and SVG.  &lt;/p&gt;
&lt;h4&gt;28. D3: Unpacking The Name&lt;/h4&gt;
&lt;p&gt;@1:12 Jonathan mentions the DOM.&lt;/p&gt;
&lt;p&gt;For more context and information about the DOM, please watch &lt;a href="https://classroom.udacity.com/courses/ud884/lessons/1464158642/concepts/15290985490923" title="Converting HTML to the DOM"&gt;Converting HTML to the DOM&lt;/a&gt;. (from &lt;a href="https://www.udacity.com/course/ud884" title="Website Performance Optimization"&gt;Website Performance Optimization&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;For more information about CSV and JSON files, please watch CSV Format and Intro to JSON. (from &lt;a href="https://www.udacity.com/course/ud032" title="Data Wrangling with Mongo DB"&gt;Data Wrangling with Mongo DB&lt;/a&gt;)&lt;/p&gt;
&lt;h4&gt;29. Why D3?&lt;/h4&gt;
&lt;p&gt;&lt;a href="http://education-portal.com/academy/lesson/web-scripting-client-side-and-server-side.html#lesson" title="Web Scripting: Client Side and Server-Side"&gt;Web Scripting: Client-Side and Server-Side&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;@0:23 seperation should be separation&lt;/p&gt;
&lt;p&gt;@0:48 To clarify, Jonathan could also have said "and D3 code can actually manipulate existing pages and html documents".&lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/data-visualization/01-visualization_fundamentals/</guid><pubDate>Fri, 21 Apr 2017 07:05:27 GMT</pubDate></item><item><title>Evaluation Metrics</title><link>https://chow1026.github.io/course-notes/machine-learning/14-evaluation_metrics/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;The simplest metric is Accuracy.  It is defined as :&lt;/p&gt;
&lt;p&gt;Accuracy = no. of items in a class labeled correctly / all items in that class&lt;/p&gt;
&lt;p&gt;There are shortcomings of accuracy:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;not ideal for skewed classes    &lt;/li&gt;
&lt;li&gt;may want to err on side of guessing innocent   &lt;/li&gt;
&lt;li&gt;may want to err of side of guessing guilty   &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Model Evaluation Metrics&lt;/h3&gt;
&lt;p&gt;There are 3 different approaches to evaluate the quality of predictions of a model:    &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Estimator score method&lt;/strong&gt;: Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve. This is not discussed on this page, but in each estimator’s documentation.     &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scoring parameter&lt;/strong&gt;: Model-evaluation tools using &lt;a href="http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" title="Cross Validation"&gt;cross-validation&lt;/a&gt; (such as model_selection.cross_val_score and model_selection.GridSearchCV) rely on an internal scoring strategy. This is discussed in the section &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter" title="Scoring Parameter"&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;.     &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metric functions&lt;/strong&gt;: The metrics module implements functions assessing prediction error for specific purposes. These metrics are detailed in sections on &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics" title="Classification Metrics"&gt;Classification metrics&lt;/a&gt;, &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics" title="Multilabel Ranking Metrics"&gt;Multilabel ranking metrics&lt;/a&gt;, &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" title="Regression Metrics"&gt;Regression metrics&lt;/a&gt; and &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#clustering-metrics" title="Clustering Metrics"&gt;Clustering metrics&lt;/a&gt;.     &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, Dummy estimators are useful to get a baseline value of those metrics for random predictions.&lt;/p&gt;
&lt;p&gt;For the most common use cases, you can designate a scorer object with the scoring parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;:       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;confusion matrix&lt;/li&gt;
&lt;li&gt;accuracy   &lt;/li&gt;
&lt;li&gt;average precision     &lt;/li&gt;
&lt;li&gt;precision score    &lt;/li&gt;
&lt;li&gt;recall score    &lt;/li&gt;
&lt;li&gt;f1 score (micro, macro, weighted, samples)    &lt;/li&gt;
&lt;li&gt;roc auc (area under curve)      &lt;/li&gt;
&lt;li&gt;log loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Clustering&lt;/strong&gt;:       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;adjusted rand score&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean absolute error&lt;/li&gt;
&lt;li&gt;Mean square error&lt;/li&gt;
&lt;li&gt;Median absolute error&lt;/li&gt;
&lt;li&gt;\(r^2\) score&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Confusion Matrix&lt;/h3&gt;
&lt;p&gt;In the field of &lt;a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning"&gt;machine learning&lt;/a&gt; and specifically the problem of &lt;a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical Classification"&gt;statistical classification&lt;/a&gt;, a &lt;strong&gt;confusion matrix&lt;/strong&gt;, also known as an error matrix,[4] is a specific table layout that allows visualization of the performance of an algorithm, typically a &lt;a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised Learning"&gt;supervised learning&lt;/a&gt; one (in &lt;a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised Learning"&gt;unsupervised learning&lt;/a&gt; it is usually called a &lt;strong&gt;matching matrix&lt;/strong&gt;). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice versa).   The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).&lt;/p&gt;
&lt;p&gt;It is a special kind of &lt;a href="https://en.wikipedia.org/wiki/Contingency_table" title="Contingency Table"&gt;contingency table&lt;/a&gt;, with two dimensions ("actual" and "predicted"), and identical sets of "classes" in both dimensions (each combination of dimension and class is a variable in the contingency table).&lt;/p&gt;
&lt;h4&gt;NOTES FROM UDACITY CLASS:&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt;: True Positive / (True Positive + False Negative). Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.      &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt;: True Positive / (True Positive + False Positive). Out of all the items labeled as positive, how many truly belong to the positive class.      &lt;/p&gt;
&lt;h4&gt;Terminology and derivations from a confusion matrix&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;condition positive (P)&lt;/strong&gt; :: the number of real positive cases in the data      &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;condition negatives (N)&lt;/strong&gt; :: the number of real negative cases in the data&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;true positive (TP)&lt;/strong&gt; :: eqv. with hit    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;true negative (TN)&lt;/strong&gt; :: eqv. with correct rejection    &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;false positive (FP)&lt;/strong&gt; :: eqv. with &lt;a href="https://en.wikipedia.org/wiki/False_alarm" title="False Alarm"&gt;false alarm&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Type_I_error" title="Type I Error"&gt;Type I error&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;false negative (FN)&lt;/strong&gt; :: eqv. with miss, &lt;a href="https://en.wikipedia.org/wiki/Type_II_error" title="Type II Error"&gt;Type II error&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sensitivity_(test)" title="Sensitivity"&gt;sensitivity&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Information_retrieval#Recall" title="Recall"&gt;recall&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Hit_rate" title="Hit Rate"&gt;hit rate&lt;/a&gt;, or &lt;a href="https://en.wikipedia.org/wiki/Sensitivity_(test)" title="True Positive Rate (TPR)"&gt;true positive rate (TPR)&lt;/a&gt; ::
\[
  \mathrm{TPR} = \frac{\mathrm{TP}}{P} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Specificity_(tests)" title="Speciticity"&gt;specificity&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Specificity_(tests)" title="True Negative Rate (TNR)"&gt;true negative rate (TNR)&lt;/a&gt; ::
\[
  \mathrm{TNR} = \frac{\mathrm{TN}}{N} = \frac{\mathrm{TN}}{\mathrm{TN} + \mathrm{FP}}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Information_retrieval#Precision" title="Precision"&gt;precision&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Positive_predictive_value" title="Positive Predictive Value"&gt;positive predictive value (PPV)&lt;/a&gt; :: &lt;br&gt;
\[
  \mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm {TP} +\mathrm {FP}}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Negative_predictive_value" title="Negative Predictive Value"&gt;negative predictive value (NPV)&lt;/a&gt; ::  &lt;br&gt;
\[
  \mathrm{NPV} = \frac{\mathrm{TN}}{\mathrm {TN} +\mathrm {FN}}
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;miss rate&lt;/strong&gt; or &lt;a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates" title="False Negative Rate"&gt;false negative rate (FNR)&lt;/a&gt; :: &lt;br&gt;
\[
  \mathrm{FNR} = \frac{\mathrm{FN}}{P} = \frac{\mathrm{FN}}{\mathrm{FN} + \mathrm{TP}} = 1 - \mathrm{TPR}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Information_retrieval#Fall-out" title="Fall Out"&gt;fall-out&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Information_retrieval#Fall-out" title="False Positive Rate"&gt;false positive rate (FPR)&lt;/a&gt; ::  &lt;br&gt;
\[
  \mathrm{FPR} = \frac{\mathrm{FP}}{N} = \frac{\mathrm{FP}}{\mathrm{FP} + \mathrm{TN}} = 1 - \mathrm{TNP}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/False_discovery_rate" title="False Discovery Rate"&gt;false discovery rate (FDR)&lt;/a&gt; ::   &lt;br&gt;
\[
  \mathrm{FDR} = \frac{\mathrm{FP}}{\mathrm {FP} +\mathrm {TP}} = 1 - \mathrm{PPV}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values" title="False Omission Rate"&gt;false omission rate (FOR)&lt;/a&gt;  ::    &lt;br&gt;
\[
  \mathrm{FOR} = \frac{\mathrm{FN}}{\mathrm {FN} +\mathrm {TN}} = 1 - \mathrm{NPV}
\]&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Accuracy" title="Accuracy"&gt;accuracy (ACC)&lt;/a&gt; ::  &lt;br&gt;
\[
  \mathrm{ACC} = \frac{\mathrm{TP} + \mathrm{TN}}{ {P} + {N}}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/F1_score" title="F1 Score"&gt;F1 score&lt;/a&gt; :: is the &lt;a href="https://en.wikipedia.org/wiki/Harmonic_mean#Harmonic_mean_of_two_numbers" title="Harmonic Mean"&gt;harmonic mea&lt;/a&gt;n of &lt;a href="https://en.wikipedia.org/wiki/Information_retrieval#Precision" title="Precision"&gt;precision&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Sensitivity_(test)" title="Sensitivity"&gt;sensitivity&lt;/a&gt;     &lt;br&gt;
\[
  F_{1} = 2 \cdot \frac{\mathrm{PPV} \cdot \mathrm{TPR}}{\mathrm{PPV} + \mathrm{TPR}} =  \frac{2\mathrm{TP}}{2\mathrm{TP} + \mathrm{FP} + \mathrm{FN}}
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient" title="Matthews Correlation Coefficient"&gt;Matthews correlation coefficient (MCC)&lt;/a&gt; ::   &lt;br&gt;
\[
  \mathrm{MCC} = \frac{ \mathrm{TP} \times \mathrm{TN} - \mathrm{FP} \times \mathrm{FN} }{ \sqrt{(\mathrm{TP} + \mathrm{FP} )(\mathrm{TP} + \mathrm{FN} )( \mathrm{TN} + \mathrm{FP} )( \mathrm{TN} + \mathrm{FN} )} }
\]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Informedness" title="Informedness"&gt;Informedness&lt;/a&gt; or &lt;strong&gt;Bookmaker Informedness (BM)&lt;/strong&gt; ::   &lt;br&gt;
\[
  \mathrm{BM}  = \mathrm{TPR} + \mathrm{TNR} -1
\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Markedness (MK)&lt;/strong&gt; ::    &lt;br&gt;
\[
  \mathrm{MK}  = \mathrm{PPV} + \mathrm{NPV} -1
\]&lt;/p&gt;
&lt;hr&gt;
&lt;table class="wikitable" align="center" style="text-align:center; border:none; background:transparent;"&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td colspan="2" style="border:none;"&gt;&lt;/td&gt;
&lt;td colspan="2" style="background:#eeeebb;"&gt;&lt;b&gt;predicted condition&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:none;"&gt;&lt;/td&gt;
&lt;td style="background:#dddddd;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Statistical_population" title="Statistical population"&gt;total population&lt;/a&gt;&lt;/td&gt;
&lt;td style="background:#ffffcc;"&gt;prediction positive&lt;/td&gt;
&lt;td style="background:#ddddaa;"&gt;prediction negative&lt;/td&gt;
&lt;td style="background:#eeeecc;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Prevalence" title="Prevalence"&gt;Prevalence&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ condition positive&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ total population&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td rowspan="2" style="background:#bbeeee;"&gt;&lt;b&gt;true&lt;br&gt;
condition&lt;/b&gt;&lt;/td&gt;
&lt;td style="background:#ccffff;"&gt;condition&lt;br&gt;
positive&lt;/td&gt;
&lt;td style="background:#ccffcc;"&gt;&lt;span style="color:#006600;"&gt;&lt;b&gt;&lt;a href="https://en.wikipedia.org/wiki/True_positive" class="mw-redirect" title="True positive"&gt;True Positive (TP)&lt;/a&gt;&lt;/b&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#eedddd;"&gt;&lt;span style="color:#cc0000;"&gt;&lt;b&gt;&lt;a href="https://en.wikipedia.org/wiki/False_Negative" class="mw-redirect" title="False Negative"&gt;False Negative (FN)&lt;/a&gt;&lt;/b&gt;&lt;/span&gt;&lt;br&gt;
(&lt;a href="https://en.wikipedia.org/wiki/Type_II_error" class="mw-redirect" title="Type II error"&gt;type II error&lt;/a&gt;)&lt;/td&gt;
&lt;td style="background:#eeffcc;"&gt;&lt;a href="https://en.wikipedia.org/wiki/True_Positive_Rate" class="mw-redirect" title="True Positive Rate"&gt;True Positive Rate (TPR)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Sensitivity_(tests)" class="mw-redirect" title="Sensitivity (tests)"&gt;Sensitivity&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Recall_(information_retrieval)" class="mw-redirect" title="Recall (information retrieval)"&gt;Recall&lt;/a&gt;, Probability of Detection &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ TP&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ condition positive&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#ffeecc;"&gt;&lt;a href="https://en.wikipedia.org/wiki/False_Negative_Rate" class="mw-redirect" title="False Negative Rate"&gt;False Negative Rate (FNR)&lt;/a&gt;, Miss Rate &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ FN&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ condition positive&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="background:#aadddd;"&gt;condition&lt;br&gt;
negative&lt;/td&gt;
&lt;td style="background:#ffdddd;"&gt;&lt;span style="color:#cc0000;"&gt;&lt;b&gt;&lt;a href="https://en.wikipedia.org/wiki/False_Positive" class="mw-redirect" title="False Positive"&gt;False Positive (FP)&lt;/a&gt;&lt;/b&gt;&lt;/span&gt;&lt;br&gt;
(&lt;a href="https://en.wikipedia.org/wiki/Type_I_error" class="mw-redirect" title="Type I error"&gt;Type I error&lt;/a&gt;)&lt;/td&gt;
&lt;td style="background:#bbeebb;"&gt;&lt;span style="color:#006600;"&gt;&lt;b&gt;&lt;a href="https://en.wikipedia.org/wiki/True_negative" class="mw-redirect" title="True negative"&gt;True Negative (TN)&lt;/a&gt;&lt;/b&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#eeddbb;"&gt;&lt;a href="https://en.wikipedia.org/wiki/False_Positive_Rate" class="mw-redirect" title="False Positive Rate"&gt;False Positive Rate (FPR)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Information_retrieval" title="Information retrieval"&gt;&lt;span class="nowrap"&gt;Fall-out&lt;/span&gt;&lt;/a&gt;, Probability of False Alarm &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ FP&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ condition negative&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#ddeebb;"&gt;&lt;a href="https://en.wikipedia.org/wiki/True_Negative_Rate" class="mw-redirect" title="True Negative Rate"&gt;True Negative Rate (TNR)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Specificity_(tests)" class="mw-redirect" title="Specificity (tests)"&gt;Specificity&lt;/a&gt; (SPC) &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ TN&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ condition negative&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:none;"&gt;&lt;/td&gt;
&lt;td rowspan="2" style="background:#cceecc;border-top:solid grey;border-right:solid grey"&gt;&lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision" title="Accuracy and precision"&gt;Accuracy&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ TP + Σ TN&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ total population&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#ccffee;border-top:solid grey;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Positive_Predictive_Value" class="mw-redirect" title="Positive Predictive Value"&gt;Positive Predictive Value (PPV)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Precision_(information_retrieval)" class="mw-redirect" title="Precision (information retrieval)"&gt;Precision&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ TP&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ prediction positive&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#eeddee;border-bottom:solid grey;"&gt;&lt;a href="https://en.wikipedia.org/wiki/False_omission_rate" class="mw-redirect" title="False omission rate"&gt;False Omission Rate (FOR)&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ FN&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ prediction negative&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#eeeeee;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Positive_likelihood_ratio" class="mw-redirect" title="Positive likelihood ratio"&gt;Positive Likelihood Ratio &lt;span class="nowrap"&gt;(LR+)&lt;/span&gt;&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;TPR&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;FPR&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td rowspan="2" style="background:#dddddd;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Diagnostic_odds_ratio" title="Diagnostic odds ratio"&gt;Diagnostic Odds Ratio (DOR)&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;LR+&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;LR−&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:none;"&gt;&lt;/td&gt;
&lt;td style="background:#cceeff;border-top:solid grey;"&gt;&lt;a href="https://en.wikipedia.org/wiki/False_Discovery_Rate" class="mw-redirect" title="False Discovery Rate"&gt;False Discovery Rate (FDR)&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ FP&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ prediction positive&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#aaddcc;border-bottom:solid grey;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Negative_Predictive_Value" class="mw-redirect" title="Negative Predictive Value"&gt;Negative Predictive Value (NPV)&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;Σ TN&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;Σ prediction negative&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="background:#cccccc;"&gt;&lt;a href="https://en.wikipedia.org/wiki/Negative_likelihood_ratio" class="mw-redirect" title="Negative likelihood ratio"&gt;Negative Likelihood Ratio &lt;span class="nowrap"&gt;(LR−)&lt;/span&gt;&lt;/a&gt; &lt;span style="font-size:118%;white-space:nowrap;"&gt;= &lt;span class="texhtml"&gt;&lt;span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em;"&gt;FNR&lt;/span&gt;&lt;span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;"&gt;TNR&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/machine-learning/14-evaluation_metrics/</guid><pubDate>Wed, 19 Apr 2017 23:04:44 GMT</pubDate></item><item><title>Validation</title><link>https://chow1026.github.io/course-notes/machine-learning/13-validation/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt;, sometimes called &lt;strong&gt;rotation estimation&lt;/strong&gt;, is a &lt;a href="https://en.wikipedia.org/wiki/Model_validation" title="Model Validation"&gt;model validation&lt;/a&gt; technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset).[4] The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc.&lt;/p&gt;
&lt;p&gt;One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.&lt;/p&gt;
&lt;p&gt;One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique.    &lt;/p&gt;
&lt;p&gt;In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.    &lt;/p&gt;
&lt;h3&gt;Common types of cross-validation&lt;/h3&gt;
&lt;p&gt;Two types of cross-validation can be distinguished, exhaustive and non-exhaustive cross-validation.&lt;/p&gt;
&lt;h4&gt;Exhaustive cross-validation&lt;/h4&gt;
&lt;p&gt;Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.&lt;/p&gt;
&lt;h5&gt;Leave-p-out cross-validation&lt;/h5&gt;
&lt;p&gt;Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set.&lt;/p&gt;
&lt;p&gt;LpO cross-validation requires training and validating the model \( C_{n}^{p} \) \( C_{n}^{p} \) times, where n is the number of observations in the original sample, and where \( C_{n}^{p} \) is the &lt;a href="https://en.wikipedia.org/wiki/Binomial_coefficient" title="Binomial Coefficient"&gt;binomial coefficient&lt;/a&gt;. For \( p \) &amp;gt; 1 and for even moderately large \( n \), LpO CV can become computationally infeasible. For example, with \( n \) = 100 and \( p \) = 30 = 30 percent of 100 (as suggested above), \( C_{100}^{30} \approx 3\times 10^{25} \).  &lt;/p&gt;
&lt;h5&gt;Leave-one-out cross-validation&lt;/h5&gt;
&lt;p&gt;Leave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with \( p \) = 1. The process looks similar to &lt;a href="https://en.wikipedia.org/wiki/Jackknife_resampling" title="Jackknife Resampling"&gt;jackknife&lt;/a&gt;, however with cross-validation you compute a statistic on the left-out sample(s), while with jackknifing you compute a statistic from the kept samples only.&lt;/p&gt;
&lt;p&gt;LOO cross-validation does not have the same problem of excessive compute time as general LpO cross-validation because \( C_{n}^{1}=n \).&lt;/p&gt;
&lt;h4&gt;Non-exhaustive cross-validation&lt;/h4&gt;
&lt;p&gt;Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-p-out cross-validation.&lt;/p&gt;
&lt;h5&gt;k-fold cross-validation&lt;/h5&gt;
&lt;p&gt;In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the \( k \) subsamples, a single subsample is retained as the validation data for testing the model, and the remaining \( k \) − 1 subsamples are used as training data. The cross-validation process is then repeated \( k \) times (the folds), with each of the k subsamples used exactly once as the validation data. The \( k \) results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter.&lt;/p&gt;
&lt;p&gt;For example, setting \( k \) = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets \( d_0 \) and \( d_1 \), so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on \( d_0 \) and test on \( d_1 \), followed by training on \( d_1 \) and testing on \( d_0 \).&lt;/p&gt;
&lt;p&gt;When \( k \) = \( n \) (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.&lt;/p&gt;
&lt;p&gt;In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.&lt;/p&gt;
&lt;h5&gt;Holdout method&lt;/h5&gt;
&lt;p&gt;In the holdout method, we randomly assign data points to two sets \( d_0 \) and \( d_1 \), usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train on \( d_0 \) and test on \( d_1 \).&lt;/p&gt;
&lt;p&gt;In typical cross-validation, multiple runs are aggregated together; in contrast, the holdout method, in isolation, involves a single run. While the holdout method can be framed as "the simplest kind of cross-validation", many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation.&lt;/p&gt;
&lt;h5&gt;Repeated random sub-sampling validation&lt;/h5&gt;
&lt;p&gt;This method, also known as Monte Carlo cross-validation, randomly splits the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.&lt;/p&gt;
&lt;p&gt;As the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation.&lt;/p&gt;
&lt;p&gt;In a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data.&lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/machine-learning/13-validation/</guid><pubDate>Wed, 19 Apr 2017 23:04:10 GMT</pubDate></item><item><title>Principle Component Analysis</title><link>https://chow1026.github.io/course-notes/machine-learning/12-pca/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;Principal component analysis (PCA) is a statistical procedure that uses an &lt;a href="https://en.wikipedia.org/wiki/Orthogonal_transformation" title="Orthogonal Transformation"&gt;orthogonal transformation&lt;/a&gt; to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation). The number of principal components is less than or equal to the smaller of the number of original variables or the number of observations. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is &lt;a href="https://en.wikipedia.org/wiki/Orthogonal" title="Orthogonal"&gt;orthogonal&lt;/a&gt; to the preceding components. The resulting vectors are an uncorrelated &lt;a href="https://en.wikipedia.org/wiki/Orthogonal_basis_set" title="Orthogonal Basis Set"&gt;orthogonal basis set&lt;/a&gt;. PCA is sensitive to the relative scaling of the original variables.&lt;/p&gt;
&lt;p&gt;PCA is mostly used as a tool in &lt;a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" title="Exploratory Data Analysis"&gt;exploratory data analysis&lt;/a&gt; and for making &lt;a href="https://en.wikipedia.org/wiki/Predictive_modeling" title="Predictive Model"&gt;predictive models&lt;/a&gt;. PCA can be done by &lt;a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" title="Eigenvalue Decomposition"&gt;eigenvalue decomposition&lt;/a&gt; of a data &lt;a href="https://en.wikipedia.org/wiki/Covariance" title="Covariance"&gt;covariance&lt;/a&gt; (or &lt;a href="https://en.wikipedia.org/wiki/Correlation" title="Correlation"&gt;correlation&lt;/a&gt;) matrix or &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" title="Singular Value Decomposition"&gt;singular value decomposition&lt;/a&gt; of a &lt;a href="https://en.wikipedia.org/wiki/Data_matrix_(multivariate_statistics)" title="Data Matrix"&gt;data matrix&lt;/a&gt;, usually after mean centering (and normalizing or using Z-scores) the data matrix for each attribute. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).&lt;/p&gt;
&lt;p&gt;PCA is the simplest of the true &lt;a href="https://en.wikipedia.org/wiki/Eigenvectors" title="Eigenvectors"&gt;eigenvector&lt;/a&gt;-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.&lt;/p&gt;
&lt;p&gt;PCA is closely related to &lt;a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor Analysis"&gt;factor analysis&lt;/a&gt;. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.&lt;/p&gt;
&lt;p&gt;PCA is also related to &lt;a href="https://en.wikipedia.org/wiki/Canonical_correlation" title="Canonical Correlation"&gt;canonical correlation analysis&lt;/a&gt; (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new &lt;a href="https://en.wikipedia.org/wiki/Orthogonal_coordinate_system" title="Orthogonal Coordinate System"&gt;orthogonal coordinate system&lt;/a&gt; that optimally describes variance in a single dataset.&lt;/p&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/machine-learning/12-pca/</guid><pubDate>Wed, 19 Apr 2017 23:00:11 GMT</pubDate></item><item><title>Feature Selection</title><link>https://chow1026.github.io/course-notes/machine-learning/11-feature_selection/</link><dc:creator>cHoWy</dc:creator><description>&lt;div&gt;&lt;p&gt;In machine learning and statistics, &lt;strong&gt;feature selection&lt;/strong&gt;, also known as &lt;strong&gt;variable selection&lt;/strong&gt;, &lt;strong&gt;attribute selection&lt;/strong&gt; or &lt;strong&gt;variable subset selection&lt;/strong&gt;, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simplification of models to make them easier to interpret by researchers/users,     &lt;/li&gt;
&lt;li&gt;shorter training times,      &lt;/li&gt;
&lt;li&gt;to avoid the &lt;a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" title="Curse of Dimensionality"&gt;curse of dimensionality&lt;/a&gt;,      &lt;/li&gt;
&lt;li&gt;enhanced generalization by reducing overfitting (formally, reduction of variance)      &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The central premise when using a feature selection technique is that the data contains many features that are either &lt;em&gt;redundant&lt;/em&gt; or &lt;em&gt;irrelevant&lt;/em&gt;, and can thus be removed without incurring much loss of information. &lt;em&gt;Redundant&lt;/em&gt; or &lt;em&gt;irrelevant&lt;/em&gt; features are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.&lt;/p&gt;
&lt;p&gt;Feature selection techniques should be distinguished from &lt;a href="https://en.wikipedia.org/wiki/Feature_extraction" title="Feature Extraction"&gt;feature extraction&lt;/a&gt;. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.&lt;/p&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods.     &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model.      &lt;/li&gt;
&lt;li&gt;Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set. Common measures include the &lt;a href="https://en.wikipedia.org/wiki/Mutual_information" title="Mutual Information"&gt;mutual information&lt;/a&gt;, the &lt;a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" title="Pointwise Mutual Information"&gt;pointwise mutual information&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" title="Pearson Product Moment Correlation Coefficient"&gt;Pearson product-moment correlation coefficient&lt;/a&gt;, inter/intra class distance or the scores of &lt;a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing" title="Significance Test"&gt;significance tests&lt;/a&gt; for each class/feature combinations.  Filters are usually less computationally intensive than wrappers, but they produce a feature set which is not tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than the set from a wrapper, usually giving lower prediction performance than a wrapper. However the feature set doesn't contain the assumptions of a prediction model, and so is more useful for exposing the relationships between the features. Many filters provide a feature ranking rather than an explicit best feature subset, and the cut off point in the ranking is chosen via &lt;a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" title="Cross Validation"&gt;cross-validation&lt;/a&gt;. Filter methods have also been used as a preprocessing step for wrapper methods, allowing a wrapper to be used on larger problems.      &lt;/li&gt;
&lt;li&gt;Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process. The exemplar of this approach is the &lt;a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" title="Lasso"&gt;LASSO&lt;/a&gt; method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are 'selected' by the &lt;a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" title="Lasso"&gt;LASSO&lt;/a&gt; algorithm. Improvements to the &lt;a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" title="Lasso"&gt;LASSO&lt;/a&gt; include Bolasso which bootstraps samples, and FeaLect which scores all the features based on combinatorial analysis of regression coefficients. One other popular approach is the Recursive Feature Elimination algorithm, commonly used with Support Vector Machines to repeatedly construct a model and remove features with low weights. These approaches tend to be between filters and wrappers in terms of computational complexity.       &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In traditional statistics, the most popular form of feature selection is &lt;a href="https://en.wikipedia.org/wiki/Stepwise_regression" title="Stepwise Regression"&gt;stepwise regression&lt;/a&gt;, which is a wrapper technique. It is a &lt;a href="https://en.wikipedia.org/wiki/Greedy_algorithm" title="Greedy Algorithm"&gt;greedy algorithm&lt;/a&gt; that adds the best feature (or deletes the worst feature) at each round. The main control issue is deciding when to stop the algorithm. In machine learning, this is typically done by &lt;a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" title="Cross Validation"&gt;cross-validation&lt;/a&gt;. In statistics, some criteria are optimized. This leads to the inherent problem of nesting. More robust methods have been explored, such as &lt;a href="https://en.wikipedia.org/wiki/Branch_and_bound" title="Branch and Bound"&gt;branch and bound&lt;/a&gt; and piecewise linear network.&lt;/p&gt;
&lt;h3&gt;Subset selection&lt;/h3&gt;
&lt;p&gt;Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into Wrappers, Filters and Embedded. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to Wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in and specific to a model.&lt;/p&gt;
&lt;p&gt;Many popular search approaches use &lt;a href="https://en.wikipedia.org/wiki/Hill_climbing" title="Hill Climbing"&gt;greedy hill climbing&lt;/a&gt;, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features. Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset. The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a program's maximum allowed run time has been surpassed, etc.&lt;/p&gt;
&lt;p&gt;Alternative search-based techniques are based on targeted projection pursuit which finds low-dimensional projections of the data that score highly: the features that have the largest projections in the lower-dimensional space are then selected.&lt;/p&gt;
&lt;p&gt;Search approaches include:       &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exhaustive    &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Best-first_search" title="Best First"&gt;Best first&lt;/a&gt;       &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Simulated_annealing" title="Simulated Annealing"&gt;Simulated annealing&lt;/a&gt;       &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Genetic_algorithm" title="Genetic Algorithm"&gt;Genetic algorithm&lt;/a&gt;       &lt;/li&gt;
&lt;li&gt;Greedy forward selection        &lt;/li&gt;
&lt;li&gt;Greedy backward elimination              &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Particle_swarm_optimization" title="Particle Swarm Optimization"&gt;Particle swarm optimization&lt;/a&gt;         &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Targeted_projection_pursuit" title="Targeted Projection Pursuit"&gt;Targeted projection pursuit&lt;/a&gt;        &lt;/li&gt;
&lt;li&gt;Scatter Search        &lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Variable_Neighborhood_Search" title="Variable Neighborhood Search"&gt;Variable Neighborhood Search&lt;/a&gt;      &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two popular filter metrics for classification problems are &lt;a href="https://en.wikipedia.org/wiki/Correlation" title="Correlation"&gt;correlation&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Mutual_information" title="Mutual Information"&gt;mutual information&lt;/a&gt;, although neither are true metrics or 'distance measures' in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual 'distance' – they should rather be regarded as 'scores'. These scores are computed between a candidate feature (or set of features) and the desired output category. There are, however, true metrics that are a simple function of the mutual information;[15] see here.&lt;/p&gt;
&lt;p&gt;Other available filter metrics include:      &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Class separability       &lt;ul&gt;
&lt;li&gt;Error probability      &lt;/li&gt;
&lt;li&gt;Inter-class distance      &lt;/li&gt;
&lt;li&gt;Probabilistic distance      &lt;/li&gt;
&lt;li&gt;Entropy      &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Consistency-based feature selection      &lt;/li&gt;
&lt;li&gt;Correlation-based feature selection      &lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><guid>https://chow1026.github.io/course-notes/machine-learning/11-feature_selection/</guid><pubDate>Wed, 19 Apr 2017 22:59:28 GMT</pubDate></item></channel></rss>
<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH ·  =^..^= MEH (old posts, page 3) </title>
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://chow1026.github.io/index-3.html">
<link rel="icon" href="images/favicon.png" sizes="64x64">
<link rel="icon" href="images/icon_512x512.png" sizes="512x512">
<link rel="prev" href="index-4.html" type="text/html">
<link rel="next" href="index-2.html" type="text/html">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chow1026.github.io/">
                      <h1 id="brand"><a href="https://chow1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="course-notes/index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="links/index.html">Links</a>
        <a class="sidebar-nav-item" href="books/index.html">Books</a>
        <a class="sidebar-nav-item" href="archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2017 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-15/" class="u-url">Inferential Statistics - Regressions</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-08T15:00:58+08:00" title="2016-11-08 15:00">2016-11-08 15:00</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Linear Regressions</h3>
<p>Line of best fit forms a line to help us:  <br>
- describe data  <br>
- make predictions     </p>
<p><strong>Observed Y vs Expected Y</strong>   <br><strong>Expected Y</strong> \( \hat{y} \) are the ones calculated/predicted based on the best fit regression line.  <br><strong>Observed Y</strong> \( y \) are collected data or real-life data.  <br><strong>Residual</strong> is the difference between Observed value and Expected value.  </p>
<p>A few ways to find the line of best fit:  <br>
- Find a line to minimize the sum of residuals.   The problem with this approach is sometimes, negative and positive residuals cancel each other out.     <br>
- Find a line that minimize the sum of <strong>absolute</strong> residuals.  <br>
- Find a line that minimize the sum of squared residuals, \( \sum{(y - \hat{y})} \)</p>
<p>When we use calculus to determine the slope, b:<br>
\[
  b = \frac{\sum{(y_i - \bar{y})(x_i - \bar{x})}}{\sum{(x_i - \bar{x})^2}} \\
  = r (\frac{S_y}{S_x}) \\
  \text{where } r \text{ is Pearson's Correlation Coefficient and }\\
   S \text{ are standard deviations of } x \text{ and } y.
\]</p>
<p>We have decided to symbolize the regression line by y = bx + a, where b represents the slope and a represents the y-intercept.
Since \( b = r(\frac{S_y}{S_x}) \), we can also symbolize the regression line like this:  <br>
\[ y = r(\frac{S_y}{S_x})x + a \]</p>
<p><strong>Pearson's Correlation Coefficient \( r \)</strong>  <br>
A high \(r\) value indicates a strong correlation. This could contribute to high \(r^2 \) value, which indicates the percentage of differences in Y is due to differences in X.  </p>
<p><strong>Standard Error of Estimates</strong>  <br>
Standard error of estimates: <br>
\[
SE = \sqrt{ \frac{\sum (y - \hat{y})^2}{N-2}}
\]</p>
<p><strong>Factors that Affect the Regressions</strong> <br>
- outliers affect the value of r, correlation Coefficient
- outliers also affect the linear regression line
-</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-14/" class="u-url">Inferential Statistics - Correlations</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-07T13:33:58+08:00" title="2016-11-07 13:33">2016-11-07 13:33</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p><strong>Relationships</strong>   <br>
How one variable is related to the other?</p>
<p><strong>Variable X and Y</strong>    <br>
X is often referred to as the predictor, explanatory, independent variable.      </p>
<p>Y is often referred as the outcome, response, dependent variable.  </p>
<p>Scatterplot is a popular/ most common way to show relationship of X and Y variables.      </p>
<p><strong>Strong Relationships</strong>    <br>
Strong relationships usually have less scattered plots.  If we draw an eclipse surrounding the data points, the smaller the ratio of minor axis to the major axis, the stronger the relationship is.  </p>
<p><strong>Direction of Relationships</strong>       <br>
Positively related - Y responses in the same direction as X changes; Negatively related - Y responses in opposite directions of X changes.</p>
<p><strong>Correlation Coefficient, (r)</strong>    <br>
Also known as Pierson's r.  </p>
<p>r is a fraction, with the covariance of x and y (how much do they vary together) as the numerator, and the product of standard deviation of x  and standard deviation of y as the denominator.<br>
\[
  r = \frac{cov(x,y)}{ S_x * S_y } = \frac{cov_{x,y}}{ S_x * S_y }
\]  <br>
r measures the strength of a relationship, by measuring how closely the data falls along a straight line.      </p>
<p>Even though r is a ratio, it is not interpreted as a percentage.  However, \(r^2\) is a percentage of the variation in y explained by variation in x.  \(r^2\) is called the coefficient determination.       </p>
<p><strong>True Correlation of Population, rho, \( \rho \)</strong>     <br>
We usually perform hypothesis testing with t-tests.
\[
  H_{0} : \rho = 0 \\
  H_{1} : \rho \gt 0 \\
  H_{1} : \rho \lt 0 \\
  H_{1} : \rho \ne 0
\]</p>
<p><strong>Causation vs Correlations</strong>    <br><strong>Causation</strong> - One variable caused another to happen.     </p>
<p><strong>Correlation</strong> - There is a relationship between two variables.  But there are lots of lurking variables.  For example, there are two variables X and Y.  They could have a relationship because both of them are influenced by variable A, or Y is influenced by X <strong>through</strong> variable A.  In this case, variable A is called the mediating variable.  </p>
<p>To make a causal statement: <br>
     - the independent variable would have to occur BEFORE the dependent variable.<br>
     - have to rule out other lurking variables too</p>
<p><strong>Fallacies</strong>  <br>
Ambiguous Temporal Precedence - we don't know which variable happens first.   <br>
Third variable problem    <br>
Post-hoc fallacy      </p>
<p><strong>t value and Correlation Coefficient, (r)</strong>   <br>
t value and r:
\[
  t = \frac{r  \sqrt{N-2}}{\sqrt{1-r^2}}
\]
where \(df\) is \( N - 2 \)</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-13/" class="u-url">Inferential Statistics - ANOVA Continued.</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-11-03T21:55:35+08:00" title="2016-11-03 21:55">2016-11-03 21:55</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>One of the popular Multiple Comparison Tests is Tukey's Honestly Significant Differences.    <br><strong>Tukey's Honestly Significant Differences (HSD)</strong>  <br>
Tukey's HSD is calculated as the following:
\[
\text{Tukey's HSD} = q * \sqrt{\frac{MS_{within}}{n}} \\
q \text{ is looked up with } df_{within} \text{ and } k \text{, the number of treatments/sample groups}   \\
n \text{ is the number of samples in one sample group}
\]   </p>
<p>If the mean difference between/among treatments are greater than Tukey's HSD, the difference is significant.    </p>
<p>Note this is very similar to Z test and T test. For Z tests, the margin of error is:
\[
\text{Margin of Error} = z * \frac{\sigma}{\sqrt{n}}
\]</p>
<p>Whereas for t tests, the margin of error is:
\[
\text{Margin of Error} = t * \frac{s}{\sqrt{n}}
\]</p>
<p><strong>Cohen's D for Multiple Comparisons</strong>  <br>
For normal comparisons, Cohen's D is calculated by
\[
\text{Cohen's D, } d = \frac{\bar{X_1} - \bar{X_2}}{SD_{pooled}}
\]   </p>
<p>In multiple comparisons, Cohen's D is calculated by
\[
\text{Cohen's D, } d = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{MS_{within}}}
\]</p>
<p>Cohen's D is calculated per pair comparisons.  </p>
<p><strong>\( \mathbf{\eta ^ 2} \)</strong>  <br>
\( \eta ^2 \) is defined as the proportion of total variance that is due to between-group differences (explained variation).  </p>
<p>\[
\eta ^2 = \frac{SS_{between}}{SS_{total}} \\
      = \frac{SS_{between}}{SS_{between} + SS_{within}}
\]    </p>
<p><strong>Reporting Reports of Anova</strong>  <br>
We report the results of F Statistics as the following:   <br>
\[
   F(df_{between}, df_{within}) = 27 \quad p &lt; 0.05 \quad \eta ^2 = 0.90 \quad \\
   \text{p is estimated, by hand} \\
   F(df_{between}, df_{within}) = 27 \quad p = 0.001 \quad \eta ^2 = 0.90 \quad \\
   \text{exact p value calculated by software} \\
\]</p>
<p><strong>ANOVA for Groups with Different Sample Sizes</strong>  <br>
Grand mean
\[
  \text{Grand mean, } \bar{X_G} = \frac{ \sum_{j=0}^k n_j (\bar{x_j}) }{\sum_{j=0}^k n_j } \\
  n_j \text{ is sample size for each sample} \\<br>
  k \text{ is number of sample groups}      \\
\]    </p>
<p>SS (Sum of Squares) Between   <br>
\[
  \text{sum of squares, } SS_{between} = \sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 \\
  n_j \text{ is sample size for each sample}   \\
\]    </p>
<p>SS (Sum of Squares) Within   <br>
\[
  \text{sum of squares, } SS_{within} = \sum_{i=0}^N (x_i - \bar{x_k})^2 \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]    </p>
<p>DF (Degress of Freedom) Between   <br>
\[
  \text{degrees of freedom, } df_{between} = k - 1 \\
  k \text{ is number of sample groups}    \\
\]    </p>
<p>DF (Degress of Freedom) Within   <br>
\[
  \text{degrees of freedom, } df_{within} = N - k \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]    </p>
<p>MS (Mean Squares) Between   <br>
\[
  \text{Mean square, } MS_{between} = \frac{SS_{between}}{df_{between}}  \\
  = \frac{\sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2}{k - 1}
\]    </p>
<p>MS (Mean Squares) Within   <br>
\[
\text{Mean square, } MS_{within} = \frac{SS_{within}}{df_{within}}  \\
= \frac{\sum_{i=0}^N (x_i - \bar{x_k})^2}{N - k}
\]    </p>
<p>F Stats
\[
\text{F Statistics, } F = \frac{MS_{between}}{MS_{within}}  \\
= \frac{\sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 / (k - 1)}{\sum_{i=0}^N (x_i - \bar{x_k})^2 / (N - k)}
\]   </p>
<p><strong>\( \mathbf{\eta ^ 2} \)</strong><br>
\[
\eta ^2 = \frac{SS_{between}}{SS_{total}} \\
      = \frac{SS_{between}}{SS_{between} + SS_{within}}
\]  </p>
<p><strong>ANOVA Power</strong>  <br>
Increase POWER in order to avoid Type II statistical error where we fail to reject the null when there is a treatment effect.   </p>
<p>In the case of drug testing, we want to:  <br>
- test <strong>more people</strong>  <br>
- give each drug to <strong>very similar</strong> groups of people  <br>
- test with a <strong>strong</strong> dosage    </p>
<p><strong>ANOVA Assumptions &amp; Conclusions</strong>   <br>
We assume:  <br>
- <strong>Normality</strong>: the population of which our samples are from are all normally distributed.  <br>
- <strong>Homogeneity of Variance</strong>: the samples come from populations that have equal amount of variability.   <br>
- <strong>Independence of Observations</strong>: The results found from one samples won't affect the others.   </p>
<p>We could have the following exceptions:  <br>
- violate the normality if the sample is large  <br>
- violate the homogeneity of variance if:  <br>
        - almost equal sample sizes  <br>
        - ratio of any two variances doesn't exceed 4     <br>
-</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/intro-data-analysis/1-data_analysis_process/" class="u-url">Intro to DA - Data Analysis Process</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-14T13:57:55+08:00" title="2016-10-14 13:57">2016-10-14 13:57</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>Data Analysis Process</h2>
<ol>
<li>
<strong>Question</strong>  <br>
    Problem begins with a question you want answer or problem you wanna solve.  For example:    <ul>
<li>Characters of students who pass projects</li>
<li>How can I stock stores with products that most people want to buy?</li>
</ul>
</li>
<li>
<strong>Wrangler</strong>    <ul>
<li>Data Acquisition   </li>
<li>Data Cleaning   </li>
</ul>
</li>
<li>
<strong>Explore</strong>   <ul>
<li>Building intuition</li>
<li>Finding patterns</li>
</ul>
</li>
<li>
<strong>Draw Conclusions</strong>   <br>
    This requires statistics and machine learning (that is beyond the scope of this course)<ul>
<li>Draw Conclusion =&gt; Users less likely to XXX</li>
<li>Make Prediction =&gt; Predict what products a user would like</li>
</ul>
</li>
<li>
<strong>Communicate</strong>  <ul>
<li>Blog post, paper, email, powerpoint, in-person conversation</li>
</ul>
</li>
</ol>
<p>The steps aren't always sequential.  We might go back and forth between data wrangling and exploration.  Along the process, one may go back to the question to further refine the question.  And data acquisition might occur before the question arises.</p>
<h2>Data Wrangling</h2>
<p>Data Types: <br>
Usually data acquired are in JSON or CSV (comma separated value) format.    </p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-12/" class="u-url">Inferential Statistics - One-Way ANOVA</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-14T10:25:32+08:00" title="2016-10-14 10:25">2016-10-14 10:25</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In lesson 11, we learned to perform t-tests for two independent samples.  However, in real statistical studies, there are a lot of times we need to compare more than two independent samples.  When we have \( n \) independent samples, the number of t-tests we need to perform are:
<!-- {n+1 \choose 2k} == \binom{n+1}{2k} -->
\[
  \text{no of t-tests with n samples} = \binom{n}{2} = \frac{n!}{2!(n-2)!}
\]</p>
<p>However, the same concept of t-test applies here.  Remember t is defined by a function of the distance apart from each other and the variability of each sample.
\[
  t_{statistic} = \frac{ \bar{X_1} - \bar{X_2}}{ \sqrt{ \frac{ S_p^2 }{ n_1 } + \frac{ S_p^2 }{ n_2 }}}
\]
When we compare 3 or more samples, we compare distance/variability between means (as numerator) and some kind of sample error (as the denominator)</p>
<p><strong>Think about this:</strong> <br>
Q: How can we compare three or more samples? <br>
A: Find the average squared deviation of each sample means.</p>
<p>Q: Will the <strong>Grand Mean</strong>, mean of the sample means be the same as the mean of all values in each sample? <br>
A: Sometimes.  Only when the sample sizes are equal for each sample, that the Grand Mean will be the same.</p>
<p><strong>Between Group Variability</strong> <br>
Between group variability is the variability between/among samples.   <br>
Q: What conclusions can we draw from the deviation of each sample mean from the mean of the means? <br>
A: The smaller the distance between sample means, the less likely the population means will defer significantly.  Vice versa the greater the distance between sample means, the more likely population means will differ significantly.  </p>
<p><strong>Within Group Variability</strong> <br>
Within group variability is the variability of the individual samples within a sample.  <br>
The greater the variability of each individual sample, the less likely population means will differ significantly.  (thinner, non overlapping normal distribution.) <br>
The smaller the variability of each individual sample, the more likely population means will differ significantly.  (wider, overlapping normal distribution)</p>
<p>Since we are analyzing the variabilities between samples and within samples, we call this Analysis of Variability (ANOVA).  We have one way ANOVA when we have one independent variable (sometimes called a factor)  </p>
<p><strong>F Statistics</strong> <br>
F statistics is the ratio of between group variability (numerator) to the within group variability (denominator)
If the between group variability is big, it constitutes to big F statistics, which results in rejecting the null hypothesis and accepting the alternative hypothesis.<br>
Whereas if the within group variability is large, it makes the F statistics small, which results in accepting the null hypothesis and rejecting the alternative hypothesis.</p>
<p>\[
F = \frac{ \text{between group variability}}{ \text{within group variability}} \\<br>
  = \frac{ \sum_{j=0}^k n_j (\bar{x_j} - \bar{x_G})^2 / (k-1)}{\sum_{i=0}^N  (x_i - \bar{x_k})^2 / (N-k) } \\
  n_j \text{ is sample size for each sample}   \\
  = \frac{ n \sum_{j=0}^k (\bar{x_j} - \bar{x_G})^2 / (k-1)}{\sum_{i=0}^N (x_i - \bar{x_k})^2 / (N-k) } \text{when sample size is same for all samples}  \\
  k \text{ is number of sample groups}      \\
  N \text{ is total number of all samples of each sample group}     \\
\]</p>
<p>F can also be formulated as
\[
F = \frac{SS_{between} / df_{between}}{SS_{within} / df_{within}} \text{ where } SS \text{ stands for Sum of Squares }      \\
  = \frac{MS_{between}}{MS_{within}} \text{ where } MS \text{ stands for Mean Square }
\]</p>
<p>Note that \(df_{between}\) is \( k - 1 \) while \(df_{within}\) is \( N - k \)</p>
<p>If we add \(df_{between}\) and \(df_{within}\) up, we get \( N - 1\) which is the total degree of freedom \(df_{total}\)
\[
df_{total} = df_{between} + df_{within} \\
= N - 1
\]</p>
<p>Similarly, the total variation \(SS_{total}\) is the sum of \(SS_{between}\) and \(SS_{within}\).<br>
\[
SS_{total} = SS_{between} + SS_{within} \\
= \sum{(x_i - \bar{x_G}) ^ 2}
\]</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-11/" class="u-url">Inferential Statistics - T-Test, Independent Samples</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-10-11T12:35:53+08:00" title="2016-10-11 12:35">2016-10-11 12:35</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p><strong>Dependent Samples (Repeated Measures)</strong> deals with within-subject designs.<br>
Types of Dependent Samples:</p>
<ul>
<li>Two conditions (a control group and a treatment group, OR two groups with two types of treatment)</li>
<li>Longitudinal (same subject group measured two different points in time)</li>
<li>Pre-test, Post-test (subject group measure before and after treatment)</li>
</ul>
<p>Pros:</p>
<ul>
<li>Controls for individual differences:<ul>
<li>Use fewer subjects</li>
<li>More cost-effective</li>
<li>Less time-consuming</li>
<li>Less expensive</li>
</ul>
</li>
</ul>
<p>Cons:</p>
<ul>
<li>Carry-over effect:<ul>
<li>The second measurement might be affected by the first treatment</li>
</ul>
</li>
<li>The order in which treatments were given might influence the results</li>
</ul>
<h2>Independent Samples:</h2>
<p>Independent Samples deals with between-subject designs.
Types of Independent Samples:</p>
<ul>
<li>Experimental</li>
<li>Observational</li>
</ul>
<p>Pros:</p>
<ul>
<li>Control for carry-over effect:<ul>
<li>The second measurement less likely be affected by the first treatment</li>
</ul>
</li>
<li>The order in which treatments no longer influence the results</li>
</ul>
<p>Cons:</p>
<ul>
<li>Little/no control for individual differences:</li>
<li>Need more subjects</li>
<li>Less cost-effective</li>
<li>More time-consuming</li>
<li>More expensive</li>
</ul>
<h2>Two-Sample Tests</h2>
<p>Considering two independent normally distributed samples collected, when we subtract those two data, we get a new dataset.<br>
\[
    N ( \mu_{1}, \sigma_{1} ) - N ( \mu_{2}, \sigma_{2} ) = N ( \mu_{1} - \mu_{2}, \sqrt{ \sigma_{1}^2 + \sigma_{2}^2 } )
\]</p>
<p>The standard deviation, \( SD \) would be:
\[
    SD = \sqrt{ SD_{1}^2 + SD_{2}^ 2 }
\]</p>
<p>The standard error, \( SE \) would be:
\[
    SE = \sqrt{ \frac{SD_{1}^2}{n_{1}} + \frac{SD_{2}^2}{n_2}}
\]</p>
<p>The t statistic, \( t_{statistic} \) would be:
\[
    t_{statistic} = \frac{ (\bar{x_{1}} - \bar{x_{2}} ) - (\mu_{1} - \mu_{2})}{ SE }
\]</p>
<p>The degree of freedom, \( df \) would be:
\[
    df = (n_{1} - 1) + (n_{2} - 1) = n_{1} + n_{2} - 2
\]
or the smaller value between
\(
(n_{1} - 1) \text{ and } (n_{2} - 1)
\).</p>
<h2>Pooled Variance</h2>
<p>Pooled variance is a method for estimating variance
given several different samples taken in different circumstances where the mean may vary
between samples but the true variance is assumed to remain the same. The pooled variance is
computed by using
The Pooled Variance, \( S_{p}^2 \) would be:
\[
    S_{p}^2 = \frac{(SS_{x} + SS_{y})}{df_{x} + df_{y}}
\]
where \( SS_{x} = \sum (x_{i} - \bar{x})^ 2 \)
and \( SS_{y} = \sum (y_{i} - \bar{y})^ 2 \)</p>
<p>The Standard Error, \( SE_{(\bar{x}-\bar{y})} \), using Pooled Variance is:</p>
<p>\[
    SE_{(\bar{x}-\bar{y})} = \sqrt{ \frac{S_{p}^2}{n_x} + \frac{S_{p}^2}{n_y} }
\]</p>
<p>The full t statistic \( t_{statistic} \) formula would be:
\[
    t_{statistic} = \frac{ (\bar{x} - \bar{y}) - \delta }{ SE_{(\bar{x}-\bar{y})} }
\]
\(\bar{x} - \bar{y} \) is called the observed difference.
The \( \delta \)expected_diff is derived from the Null Hypothesis, \( H_0 \), when
\[
    H_0: \mu_x - \mu_y = \delta
\]
There are lots of cases where \( \delta \) was assumed 0.  </p>
<p>When we use Pooled Variance, we hold the following assumptions:
1. X and Y should be two random samples from two independent populations. <br>
2. Populations that X and Y come from, should be approximately normal.  This is less important when sample size is really large ( &gt;30).<br>
3. Sample data can be used to estimate population variances.
4. Population variances should be roughly equal.</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/inferential-statistics/lesson-10/" class="u-url">Inferential Statistics - T-Test, Dependent Samples</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-09-29T13:09:03+08:00" title="2016-09-29 13:09">2016-09-29 13:09</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>T-Test</h2>
<p>Z-test works when we know the population parameters such as \( \mu \) and \( \sigma \).  For any samples drawn from this population, the samples would form a sampling distribution that is a normal distribution, with:
\[
    mean = \mu  \quad\text{where M is mean of sample means}\\
    SD = \frac{\sigma}{\sqrt{n}} \quad\text{where n is sample size}
\]
\( SD \) is also known as Standard Error \( SE \) which is used for Z score calculation.</p>
<p>For any sample mean, \( M \), we can determine where it falls on this sampling distribution by standardizing, aka, finding the z-score \( Z \), given its formula below:
\[
    Z = \frac{M - \mu}{ SE }
\]</p>
<p>However, much of the times, with sample data, we don't know population \( \mu \) and population standard deviation \( \sigma \). We only have samples which we must use to draw all our conclusions.  </p>
<p>When working with samples, we usually estimate the population standard deviation using the sample standard deviation with Bassel's correction.<br>
\[
  S =  \sqrt{ \frac{ \sum_1^n{ (x_{i} - \bar{x}) ^ 2 }} { n-1 } } \\
  where S is estimated population standard deviation, n is sample size
\]</p>
<p>Without population parameters and with only sample data, we end up with a new distribution that is more prone to error, called the t-distribution. The t-distribution is more spread out and thicker in the tails.  </p>
<p>Same principals applies to t-distribution.  When n increases:</p>
<ul>
<li>the t-distribution approaches a normal distribution</li>
<li>the t-distribution gets skinner tails</li>
<li>S, the estimated population standard deviation, gets closer to the real population standard deviation \( \mu \).  </li>
</ul>
<p>With t-distribution and t-test, we can determine:</p>
<ol>
<li>How different a sample mean is from a population mean</li>
<li>How different two sample means are from each other.  These two samples can be either:<ul>
<li>dependent</li>
<li>independent</li>
</ul>
</li>
</ol>
<h2>Degrees of Freedom</h2>
<p>t-distribution are defined by degrees of freedom, \(df\), which generally is \( n - 1 \) for single dimension data.  The \(df\) for 2D or 3D sample is \[df = (n-1)^d\;\text{where power d is the number of dimension}\]
Degrees of freedom are the number of pieces of information that can be freely varied, without violating any given restrictions.  It is pieces of independent information to estimate another piece of information. As the degrees of freedom increases, it better approximates the normal distribution.
\( n - 1 \) is also known as the effective sample size.  As shown above, it is used to estimate the population standard deviation with Basel's correction.  </p>
<h2>Hypothesis Testing with t-statistics</h2>
<p>Like the z-test, if the t-statistic falls far from the mean, where t-statistic is 0, we reject the null. To do so, we compare the sample mean to population mean, by calculating t-statistic.</p>
<p>For one sample t-test, the t-statistic is:
\[
    t = \frac{\bar{x} - \mu_{0}}{S/\sqrt{n}} \text{where:}\\
    \bar{x}\text{ is sample mean}\\
    \mu_{0}\text{ is population mean}\\
    S \text{ is standard error, aka. standard deviation of sample}\\
    n \text{ is the sample size}
\]
The sample mean, \(\bar{x}\) is also the point estimate for the population mean.
t-statistics increases when:</p>
<ul>
<li>a larger difference between \(\bar{x}\) and \(\mu_{0}\)</li>
<li>larger \( n \)</li>
</ul>
<p>For hypothesis testing:</p>
<ul>
<li>The <strong>larger</strong> the value of \(\bar{x}\), the stronger the evidence that \(\mu \gt \mu_{0}\).</li>
<li>The <strong>smaller</strong> the value of \(\bar{x}\), the stronger the evidence that \(\mu \lt \mu_{0}\).</li>
<li>The further the value of \(\bar{x}\) from \(\mu_{0}\) in either direction, the stronger the evidence that \(\mu \neq \mu_{0}\).</li>
</ul>
<p>We reject the null hypothesis if the t-statistic is less than or greater than the t-critical value, at a given \( \alpha \) level.</p>
<h2>P-Value</h2>
<p>For one-tailed test, the P-value is the probability above the t-statistic if it is positive, and below the t-statistic if it is negative.  For two tailed test, the p-value will be the sum of probability on both ends.  We reject the null hypothesis when the p-value is less than the \( \alpha \) level.</p>
<p>After calculating the t-statistic, we go to <a href="http://www.graphpad.com/quickcalcs/pValue1/" title="P-Value with t and DF">GraphPad</a> to get the exact P-value. There are also other calculators on <a href="http://www.graphpad.com/" title="GraphPad Calculators">GraphPad</a> that are worth checking out.  </p>
<h2>Cohen's d</h2>
<p>Cohen's d is another common measure of effect size, when comparing means, named after Jacobs Cohen.  It is a <strong>standardized mean difference</strong> that measures the distance between 2 means in standard deviation units.
\[
    Cohen's d = \frac{\bar{x} - \mu_{0}}{S} \\
    where\;\bar{x}\text{ is sample mean, }\mu_{0}\text{ is population mean, and }S\text{ is sample standard deviation}
\]</p>
<h2>Confidence Interval</h2>
<p>Confidence interval is the interval where the population mean will probably lie.<br>
At a given confidence level, or alpha level, we first determine the t-critical value.<br>
Confidence interval for a two-tailed test is:
\[
  \begin{align}
    CI &amp; = M \pm t_{critical, \alpha} \cdot SE_{sample} \\
    &amp; = M \pm t_{critical, \alpha} \cdot \frac{S}{\sqrt{n}}
  \end{align}
\]</p>
<h2>Margin of Error</h2>
<p>Margin of Error is one-half width of the confidence interval.  CI's upper bound is sample mean, \(M + t_{critical} \cdot \frac{S}{\sqrt{n}} \) plus margin of error; whereas CI's lower bound is sample mean, \( M - t_{critical} \cdot \frac{S}{\sqrt{n}} \).  Therefore:
\[
  Margin\;of\;Error = t_{critical} \cdot \frac{S}{\sqrt{n}}
\]</p>
<h2>Dependent t-tests</h2>
<p>Dependent samples are generated when the same subject takes the test twice.  This is a within subject design.  Examples:</p>
<ul>
<li>when same subject is applied two conditions</li>
<li>subject is given a pre-test and post-test</li>
<li>longitudinal study (development over time)</li>
</ul>
<p>The within-subject designs generate paired data.  Then we look at the difference between these two sets of data, \( D_{i} \)</p>
<h2>Types of Designs</h2>
<ol>
<li>Repeated measures design (eg errors on two types of keyboards)
\[ H_{0}: \mu_{1} = \mu_{2} \]</li>
<li>Longitudinal design
\[ H_{0}: \mu_{time1} = \mu_{time2} \]</li>
<li>Pre-test vs Post-test
\[ H_{0}: \mu_{pre} = \mu_{post} \]</li>
</ol>
<h2>Effect Size</h2>
<p>In experimental studies, Effect Size refers to the size of a treatment effect.  In non-experimental studies, Effect Size may refer to the strength of the relationships between the variables.  </p>
<p>In the Z-test or one sample t-test, the mean difference is \( \bar{x} - \mu \).  Mean differences is great when we variables with easy to understand meanings.</p>
<h3>Types of Effect Size Measures</h3>
<p>There are many Effect Size measures, but they all fall into two main groups:</p>
<ol>
<li>
<p>Difference Measures:</p>
<ul>
<li>Mean difference</li>
<li>standardized difference<ul>
<li>Cohen's d (in SD units)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Correlation Measures:</p>
<ul>
<li>\( r^2 \): the proportion (or %) of variation of one variable that is related to ("explained by") another variable.<br>
    \[ r^2 = \frac{ t^2 }{ t^2 + df } \\
        where\; t \text{ is t statistics, not } t_{critical} \text{ and } df \text{ is degree of freedom}
    \]</li>
<li>\( r^2 \) is also known as the coefficient of determination.</li>
</ul>
</li>
</ol>
<h2>Statistical Significance</h2>
<p>Statistical Significant means:
- we reject the null
- results are not likely due to chance (sampling error)</p>
<h2>Meaningfulness of Results</h2>
<ul>
<li>What was being measured?  Do/Does the variable(s) has any practical, social, theoretical importance?</li>
<li>Effect Size: Small effect size doesn't necessarily mean the results have lower importance, and vice versa, large effect size doesn't necessarily means the results have greater importance.  </li>
<li>Can we rule out random chance/sampling errors?</li>
<li>Can we rule out alternative explainations? (lurking variables)</li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="books/python-design-patterns/02-singleton-design-pattern/" class="u-url">Python Design Patterns - The Singleton Design Pattern</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-09-27T12:06:02+08:00" title="2016-09-27 12:06">2016-09-27 12:06</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>Singleton Design Pattern</h2>
<h2>Real-World Example of Singleton Pattern</h2>
<h2>Singleton Pattern Implementation in Python</h2>
<h2>The Monostate(Borg) Pattern</h2>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="books/python-design-patterns/01-intro/" class="u-url">Python Design Patterns - Introduction</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-09-26T16:22:03+08:00" title="2016-09-26 16:22">2016-09-26 16:22</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h2>Understanding Object-Oriented Programming</h2>
<h3>Objects</h3>
<p>Objects are described as:</p>
<ul>
<li>They represent entities in your application under development</li>
<li>Entities interact among themselves to solve real-world problems</li>
</ul>
<h3>Classes</h3>
<p>Classes help represent real-world entities by:</p>
<ul>
<li>Classes define objects in attributes and behaviors. Attributes are data members and behaviors are manifested by the member functions</li>
<li>Classes consist of constructors that provide the initial state of these objects</li>
<li>Classes are like templates and hence can be easily reused.</li>
</ul>
<h3>Methods</h3>
<p>Methods are described as following:</p>
<ul>
<li>They represent the behavior of the objects.</li>
<li>Methods work on attributes and also implement the desired functionality.</li>
</ul>
<pre class="code literal-block"><span></span>    <span class="k">class</span> <span class="nc">Person</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">age</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">age</span> <span class="o">=</span> <span class="n">age</span>

      <span class="k">def</span> <span class="nf">get_person</span><span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="k">return</span> <span class="s2">"&lt;Person (</span><span class="si">%s</span><span class="s2">, </span><span class="si">%s</span><span class="s2">)&gt;"</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">age</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">Person</span><span class="p">(</span><span class="s2">"John Doe"</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Type of Object:"</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="s2">"Memory Address:"</span> <span class="p">,</span> <span class="nb">id</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
</pre>


<h2>Major Aspects of Object-Oriented Programming</h2>
<h3>Encapsulation</h3>
<p>Key Features are:</p>
<ul>
<li>an object's behavior is kept hidden from the outside world or objects kept their state information private.</li>
<li>Clients can't change the object's internal state by directly acting on them; rather clients request the object by sending messages. Based on the type of requests, objects may respond by changing their internal state using special member functions such as <code>get</code> or <code>set</code>
</li>
<li>In Python, the concept of encapsulation isn't implicit, as it doesn't have keywords such as public, private, and protected that are required to support encapsulation. Accessibility is on the other hand made private by prefixing <code>__</code> in the variable or function name.</li>
</ul>
<h3>Polymorphism</h3>
<p>Key Features are:</p>
<ul>
<li>There are two types:<ul>
<li>An object provides different implementations of the method based on input parameters.</li>
<li>The same interface can be used by objects of different types.</li>
</ul>
</li>
<li>In Python, polymorphism is built in for the language.</li>
<li>Examples:<ul>
<li>The + operator can act on two integers to add them or can work with strings to concatenate them.</li>
<li>Strings, tuples and lists can all be accessed with an integer index. I.E, <code>s(1)</code>, <code>t(2)</code> or <code>l(-1)</code>.</li>
</ul>
</li>
</ul>
<h3>Inheritance</h3>
<ul>
<li>Inheritance indicates that one class derives (most of its) functionality from the parent class.</li>
<li>Inheritance is described as an option to reuse functionality defined in the base class and allow independent extensions of the original software implementation.</li>
<li>Inheritance creates hierarchy via the relationships among objects of different classes. Python, unlike Java, supports multiple inheritance (inheriting multiple base classes).</li>
</ul>
<h3>Abstraction</h3>
<p>Key Features are:</p>
<ul>
<li>It provides you with a simple interface to the clients, where the client can interact with the class object and call methods defined in the interface.  </li>
<li>It abstracts the complexity of internal classes with an interface so that the client needs not aware of the internal implementation.  </li>
</ul>
<pre class="code literal-block"><span></span>    <span class="k">class</span> <span class="nc">Adder</span><span class="p">:</span>
      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sum</span> <span class="o">+=</span> <span class="n">value</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">Adder</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">99</span><span class="p">):</span>
      <span class="n">acc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">sum</span><span class="p">)</span>
</pre>


<h3>Composition</h3>
<p>Key Points:</p>
<ul>
<li>It is a way to combine objects or classes into more complex data structures or software implementation</li>
<li>In composition, an object is used to call member functions other modules thereby making base functionality available across modules without inheritance.  </li>
</ul>
<pre class="code literal-block"><span></span>    <span class="k">class</span> <span class="nc">A</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">a1</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">print</span><span class="p">(</span><span class="s2">"a1"</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">B</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
      <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">print</span><span class="p">(</span><span class="s2">"b"</span><span class="p">)</span>
      <span class="n">A</span><span class="p">()</span><span class="o">.</span><span class="n">a1</span><span class="p">()</span>

    <span class="n">objectB</span> <span class="o">=</span> <span class="n">B</span><span class="p">()</span>
    <span class="n">objectB</span><span class="o">.</span><span class="n">b</span><span class="p">()</span>
</pre>


<h2>Object-Oriented Design Principles</h2>
<h3>The Open/Close Principle</h3>
<p>The open/close principle states that <em>classes or objects and methods should be open for extension but closed for modifications.</em>  </p>
<p>For example, the open/close principle is manifested in a case where a user has to create a class implementation by extending the abstract base class to implement the required behavior instead of changing the abstract class.</p>
<p>The advantages of this design principle are:</p>
<ul>
<li>Existing classes are changed and hence the chances of regression are less.</li>
<li>It also helps maintain backward compatibility for the previous code.</li>
</ul>
<h3>The Inversion of Control Principle</h3>
<p>The inversion of control principle states that <em>high-level modules shouldn't be dependent on low-level modules; they should both be dependent on abstractions.  Details should depend on abstractions but not the other way around.</em></p>
<p>The principle suggests that any two modules shouldn't be dependent on each other in a tight way but should be decoupled with an abstraction layer in between.  It also suggests that the details of the class should represent the abstractions. In some cases, where the philosophy gets inverted and implementation details itself decide the abstraction, which should be avoided.  </p>
<p>Advantages of the inversion of control principles are as follow:</p>
<ul>
<li>The tight coupling of modules is more prevalent and hence no complexity/rigidity in the system.  </li>
<li>As there is a clear abstraction layer between dependent modules (provided by a hook or parameter), it's easy to deal with dependencies across modules in a better way.  </li>
</ul>
<h3>The Interface Segregation Principle</h3>
<p>The interface segregation principle states, <em>clients should not be forced to depend on interfaces they don't use.</em></p>
<p>It reminds developers to develop methods that relate to the functionality.  If there is any method that is not related to the interface, the class dependent on the interface has to implement it unnecessarily.  For example: a <code>Pizza</code> interface shouldn't have a method called <code>add_chicken()</code>.  The <code>VegePizza</code> class based on <code>Pizza</code> shouldn't be forced to implement this method.  </p>
<p>Advantages of this design principle are:</p>
<ul>
<li>It forces developers to write thin interfaces and have methods that are specific to the interface</li>
<li>It helps you not to populate by adding unintentional methods</li>
</ul>
<h3>The Single Responsibility Principle</h3>
<p>The single responsibility principle states, <em>a class should have only one reason to change.</em>  </p>
<p>When a class is developed, it should cater to the given functionality well.  If it is serving two functionalities, it is better to split them.  It refers to functionality as a reason to change.  For example, a class can undergo changes because of the difference in behavior expected in it, but if a class is changed for two reasons (ie changes in two functionalities), then the class should definitely be split.  </p>
<p>Advantages of this design principle:</p>
<ul>
<li>Whenever there is a change in one functionality, this particular class needs to change, and nothing else.</li>
<li>Additionally, if a class has multiple functionalities, the dependent classes will have to undergo changes for multiple reason, which gets avoided.  </li>
</ul>
<h3>The Single Responsibility Principle</h3>
<p>The substitution principle states that <em>derived classes must be able to completely substitute the base classes.</em>  </p>
<p>It says when developers write derived classes, they should extend the base classes.  The derived class should be as close to the base class as possible so much so that the derived class itself should replace the base class without any code changes.  </p>
<h2>The Concept of Design Patterns</h2>
<p>Design patterns were first introduced by <strong>GoF</strong> (<strong>Gang of Four</strong>), where they mentioned them as being solutions to given problems.  The book, <a href="http://bla" title="Design Patterns by Gang of Four">Design Patterns: Elements of Reusable Object-Oriented Software</a>, covers software engineering solutions to the commonly occurring problems in software design.  There are 23 design patterns first identified, and first implemented to the Java programming language.  <em>Design patterns are discoveries, and not invention in themselves.</em>  </p>
<p>The key features of design patterns are:</p>
<ul>
<li>They are language-neutral and can be implemented across multiple languages</li>
<li>They are dynamic, as new patterns get introduced every now and then</li>
<li>They are open for customization and hence useful for developers</li>
</ul>
<p>One would also find:</p>
<ul>
<li>It is a panacea to all design problemsthat a developer has had so far</li>
<li>It's an extraordinary, specially clever way to solving a problem</li>
<li>Many experts in software development world agree to these solutions</li>
<li>There's something repeatable about the design, hence the word pattern</li>
</ul>
<p>Completeness in a software solution refers to many factors such as design, scalability, reuse, memory utilization and etc.  </p>
<h3>Advantages of Design Patterns</h3>
<p>The advantages of design patterns are:
- They are reusable across multiple projects
- The architectural level of problems can be solved.
- They are time-tested and well-proven, which is the experience of developers and architects
- They have reliability and dependence</p>
<h3>Taxonomy of Design Patterns</h3>
<p>Not every piece of code/design can be termed as a design patterns.<br>
- <strong>Snippet</strong>: Some code in some language for a certain purpose, for example, DB connectivity in Python can be a code Snippet
- <strong>Design</strong>: A better solution to solve a particular problem.
- <strong>Standard</strong>: A way to solve some kind of problems, which can be very generic and applicable to a situation at hand.
- <strong>Pattern</strong>: A time-tested, efficient and scalable solution that will resolve entire class of known issues.  </p>
<h3>Context - The Applicability of Design Patterns</h3>
<p>To use design patterns efficiently, application developers must be aware of the context where design patterns apply.  Context are classified into the following categories:</p>
<ul>
<li>
<strong>Participants</strong>: They are classes that are used in design patterns.  Classes play different roles to accomplish multiple goals in the pattern.  </li>
<li>
<strong>Non-functional Requirements</strong>: Requirements such as memory optimization, usablility and performance fall under this category.  These factors impact the complete software solution and are thus critical.</li>
<li>
<strong>Trade-offs</strong>: Not all design patterns fit in (application development) as it is, and trade-offs are necessary.  These are decisions
that you take while using a design pattern in an application.  </li>
<li>
<strong>Results</strong>: Design patterns can have a negative impact on other parts of the code if the context is not appropriate.  Developers should understand the consequences and use of design patterns.  </li>
</ul>
<h2>Patterns for Dynamic Languages</h2>
<p>Python is a dynamic language.  The dynamic nature of Python can be represented as follows:</p>
<ul>
<li>Types or classes are objects at runtime.</li>
<li>Variables can have type as a value and can be modified at runtime.  For example, a = 5 and a = "John", the a variable is assigned at runtime and type also gets changed.</li>
<li>Dynamic languages have more flexibility in terms of of class restrictions.</li>
<li>In Python, polymorphism is built into the language, there is no keywords such as <code>private</code> and <code>protected</code> and everything is public by default.  </li>
<li>Represents a case where design patterns can be easily implemented in dynamic languages.</li>
</ul>
<h2>Classifying Patterns</h2>
<p>The GoF generally classified 23 patterns under three main categories:</p>
<ul>
<li>
<p>Creational Pattern</p>
<ul>
<li>They work on the basis of how objects can be created</li>
<li>They isolate the details of object creation</li>
<li>Code is independent of the type of object to be created</li>
<li>An example of a creation pattern is the Singleton pattern.</li>
</ul>
</li>
<li>
<p>Structural Pattern</p>
<ul>
<li>They design the structure of objects and classes so that they can compose to archive larger results</li>
<li>The focus is on simplifying the structure and identifying the relationship between classes and objects</li>
<li>They focus on class inheritance and composition</li>
<li>An example of a behavioral pattern is the Adapter pattern.</li>
</ul>
</li>
<li>
<p>Behavioral Pattern</p>
<ul>
<li>They are concerned with the interaction among objects and responsibility of objects</li>
<li>Objects should be able to interact and still be loosely coupled.  </li>
</ul>
</li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="books/design-patterns/01-intro/" class="u-url">Introduction</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2016-09-26T16:20:47+08:00" title="2016-09-26 16:20">2016-09-26 16:20</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>Write your post here.</p>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-4.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-2.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>

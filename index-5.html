<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title> =^..^= MEH ·  =^..^= MEH (old posts, page 5) </title>
<link href="assets/css/rst.css" rel="stylesheet" type="text/css">
<link href="assets/css/poole.css" rel="stylesheet" type="text/css">
<link href="assets/css/hyde.css" rel="stylesheet" type="text/css">
<link href="assets/css/code.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<link rel="canonical" href="https://chow1026.github.io/index-5.html">
<link rel="icon" href="images/favicon.png" sizes="64x64">
<link rel="icon" href="images/icon_512x512.png" sizes="512x512">
<link rel="prev" href="." type="text/html">
<link rel="next" href="index-4.html" type="text/html">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            displayAlign: 'center', // Change this to 'left' to left equations.
            "HTML-CSS": {
                styles: {'.MathJax_Display': {"margin": 0}}
            }
        });
    </script><script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
</head>
<body class="test">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="sidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://chow1026.github.io/">
                      <h1 id="brand"><a href="https://chow1026.github.io/" title=" =^..^= MEH" rel="home">

        <span id="blog-title"> =^..^= MEH</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead"></p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="posts/index.html">Articles</a>
        <a class="sidebar-nav-item" href="course-notes/index.html">Course Notes</a>
        <a class="sidebar-nav-item" href="links/index.html">Links</a>
        <a class="sidebar-nav-item" href="books/index.html">Books</a>
        <a class="sidebar-nav-item" href="archives/archives.html">Archives</a>
        <a class="sidebar-nav-item" href="tags.html">Tags</a>
    
    
    </nav><footer id="footer"><p class="footer">
              <span class="icon_row">

                <a href="mailto:chowy1026@gmail.com">
                  <img class="social_icon" src="images/envelope1.png" title="email" width="24"></a> ·
                <!--<a href="">
                  <img class="social_icon" src="/images/twitter-black-shape1.png" title="twitter" width="24" /></a> &middot;-->
                <a href="https://github.com/chowy1026/">
                  <img class="social_icon" src="images/github-character1.png" title="github" width="24"></a>
              </span>
              <br><br><span class="copyright">
              
Contents © 2017 by <a href="mailto:chowy1026@gmail.com">cHoWy</a> ·
Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a> <br><a href="http://hyde.getpoole.com" target="_blank">Hyde</a> theme by <a href="https://twitter.com/mdo" target="_blank">@mdo</a>

            </span>
            </p>
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
    
<div class="post">
    <article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/09-feature_scaling/" class="u-url">Feature Scaling</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-20T06:53:53+08:00" title="2017-04-20 06:53">2017-04-20 06:53</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Introduction</h3>
<p>Feature scaling is a method used to standardize the range of independent variables or features of data. In <a href="https://en.wikipedia.org/wiki/Data_processing" title="Data Processing">data processing</a>, it is also known as data normalization and is generally performed during the data preprocessing step.</p>
<p>Since the range of values of raw data varies widely, in some <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning">machine learning</a> algorithms, objective functions will not work properly without <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)" title="Normalization">normalization</a>. For example, the majority of <a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Classifiers">classifiers</a> calculate the distance between two points by the <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean Distance">Euclidean distance</a>. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.</p>
<p>Another reason why feature scaling is applied is that <a href="https://en.wikipedia.org/wiki/Gradient_descent" title="Gradient Descent">gradient descent</a> converges much faster with feature scaling than without it.</p>
<h3>Methods</h3>
<h4>Rescaling</h4>
<p>The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula is given as:</p>
<p>\( x' = \frac{x- \text{min}(x)} {{\text{max}}(x)-{\text{min}}(x)} \)</p>
<p>where \(  x \) is an original value, \( x' \) is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).</p>
<h4>Standardization</h4>
<p>In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple <a href="https://en.wikipedia.org/wiki/Dimensions" title="Dimensions">dimensions</a>. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., <a href="https://en.wikipedia.org/wiki/Support_vector_machine" title="Support Vector Machine">support vector machines</a>, <a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic Regression">logistic regression</a>, and <a href="https://en.wikipedia.org/wiki/Neural_network" title="Neural Network">neural networks</a>). This is typically done by calculating <a href="https://en.wikipedia.org/wiki/Standard_score" title="Standard Score">standard scores</a>. The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.</p>
<p>\( x'= \frac{x-{\bar {x}}}{\sigma } \)</p>
<p>Where \(  x \) is the original feature vector, \( \bar {x} \) is the mean of that feature vector, and \( \sigma \)  is its standard deviation.</p>
<h4>Scaling to unit length</h4>
<p>Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the <a href="https://en.wikipedia.org/wiki/Euclidean_length" title="Euclidean Length">Euclidean length</a> of the vector. In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or <a href="https://en.wikipedia.org/wiki/Taxicab_Geometry" title="Taxicab Geometry">Taxicab Geometry</a>) of the feature vector:</p>
<p>\(  x'= \frac {x}{||x||} \)
This is especially important if in the following learning steps the Scalar Metric is used as a distance measure.</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/08-unsupervised_learning-clustering/" class="u-url">Unsupervised Learning - Clustering</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-20T06:53:05+08:00" title="2017-04-20 06:53">2017-04-20 06:53</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Unsupervised Learning</h3>
<p><strong>Unsupervised machine learning</strong> is the machine learning task of inferring a function to describe hidden structure from "unlabeled" data (a classification or categorization is not included in the observations). Since the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm—which is one way of distinguishing <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised Learning">unsupervised learning</a> from <a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised Learning">supervised learning</a> and <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforced Learning">reinforcement learning</a>.</p>
<p>A central case of unsupervised learning is the problem of <a href="https://en.wikipedia.org/wiki/Density_estimation" title="Density Estimation">density estimation</a> in statistics, though unsupervised learning encompasses many other problems (and solutions) involving summarizing and explaining key features of the data.</p>
<p>Approaches to unsupervised learning include:</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Data_clustering" title="Data Clustering">Clustering</a></strong>    <br>
- <a href="https://en.wikipedia.org/wiki/K-means" title="K Means">k-means</a>    <br>
- <a href="https://en.wikipedia.org/wiki/Mixture_models" title="Mixture Models">mixture models</a>    <br>
- <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierachical Clustering">hierarchical clustering</a>      <br><strong><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly Detection">Anomaly detection</a></strong>     <br><strong><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Neural Network">Neural Networks</a></strong>   <br>
- <a href="https://en.wikipedia.org/wiki/Hebbian_Learning" title="Hebbian Learning">Hebbian Learning</a>  <br>
- <a href="https://en.wikipedia.org/wiki/Generative_Adversarial_Networks" title="Generative Adversarial Networks">Generative Adversarial Networks</a>  <br><strong>Approaches for learning <a href="https://en.wikipedia.org/wiki/Latent_variable_model" title="Latent Variable Model">latent variable models</a></strong> such as          <br>
- <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation Maximization Algorithm">Expectation-Maximization Algorithm</a> (EM)
- <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)" title="Method of Moments">Method of moments</a>
- <a href="https://en.wikipedia.org/wiki/Blind_signal_separation" title="Blind Signal Separation">Blind signal separation</a> techniques, e.g.,
    - <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal Component Analysis (PCA)">Principal component analysis</a>,     <br>
    - <a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent Component Analysis">Independent component analysis</a>,     <br>
    - <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-Negative Matrix Factorization">Non-negative matrix factorization</a>,     <br>
    - <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" title="Singular Value Decomposition">Singular value decomposition</a>.        </p>
<h3>Clustering</h3>
<p><strong>Cluster analysis</strong> or <strong>Clustering</strong> is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory <a href="https://en.wikipedia.org/wiki/Data_mining" title="Data Mining">data mining</a>, and a common technique for statistical data analysis, used in many fields, including <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine Learning">machine learning</a>, <a href="https://en.wikipedia.org/wiki/Pattern_recognition" title="Pattern Recognition">pattern recognition</a>, <a href="https://en.wikipedia.org/wiki/Image_analysis" title="Image Analysis">image analysis</a>, <a href="https://en.wikipedia.org/wiki/Information_retrieval" title="Information Retrieval">information retrieval</a>, <a href="https://en.wikipedia.org/wiki/Bioinformatics" title="Bio-Informatics">bioinformatics</a>, <a href="https://en.wikipedia.org/wiki/Data_compression" title="Data Compression">data compression</a>, and <a href="https://en.wikipedia.org/wiki/Computer_graphics" title="Computer Graphics">computer graphics</a>.</p>
<p>Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a <a href="https://en.wikipedia.org/wiki/Multi-objective_optimization" title="Multi-Objective Optimization">multi-objective optimization</a> problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.</p>
<p>Besides the term clustering, there are a number of terms with similar meanings, including <em>automatic classification</em>, <em>numerical taxonomy</em>, <em>botryology</em> (from Greek βότρυς "grape") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.</p>
<p>The notion of a "cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these "cluster models" is key to understanding the differences between the various algorithms.</p>
<p>Typical cluster models include:          </p>
<ul>
<li>Connectivity models: for example, <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierachical Clustering">hierarchical clustering</a> builds models based on distance connectivity.    </li>
<li>Centroid models: for example, the <a href="https://en.wikipedia.org/wiki/K-means_algorithm" title="K Means Algorithm">k-means algorithm</a> represents each cluster by a single mean vector.      </li>
<li>Distribution models: clusters are modeled using statistical distributions, such as <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" title="Multivariate Normal Distribution">multivariate normal distributions</a> used by the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation Maximization Algorithm">Expectation-Maximization Algorithm</a>.         </li>
<li>Density models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.        </li>
<li>Subspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.       </li>
<li>Group models: some algorithms do not provide a refined model for their results and just provide the grouping information.         </li>
<li>Graph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.        </li>
</ul>
<p>A "clustering" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:         </p>
<ul>
<li>Hard clustering: each object belongs to a cluster or not         </li>
<li>Soft clustering (also: <a href="https://en.wikipedia.org/wiki/Fuzzy_clustering" title="Fuzzy Clustering">fuzzy clustering</a>): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster)       </li>
</ul>
<p>There are also finer distinctions possible, for example:        </p>
<ul>
<li>Strict partitioning clustering: each object belongs to exactly one cluster      </li>
<li>Strict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers       </li>
<li>Overlapping clustering (also: alternative clustering, multi-view clustering): objects may belong to more than one cluster; usually involving hard clusters        </li>
<li>Hierarchical clustering: objects that belong to a child cluster also belong to the parent cluster        </li>
<li>
<a href="https://en.wikipedia.org/wiki/Subspace_clustering" title="Subspace Clustering">Subspace clustering</a>: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap        </li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/07-outliers/" class="u-url">Outliers</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-20T06:51:18+08:00" title="2017-04-20 06:51">2017-04-20 06:51</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.</p>
<p>Outliers can occur by chance in any distribution, but they often indicate either <a href="https://en.wikipedia.org/wiki/Measurement_error" title="Measurement Error">measurement error</a> or that the population has a <a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution" title="Heavy Tailed Distribution">heavy-tailed distribution</a>. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture Model">mixture model</a>.</p>
<p>In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition).</p>
<p>Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations.</p>
<p>Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 °C, the median of the data will be between 20 and 25 °C but the mean temperature will be between 35.5 and 40 °C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as "a typical sample", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set.</p>
<p>Estimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally more precise estimator.</p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/05-datasets_and_questions/" class="u-url">Datasets and Questions</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-20T06:28:08+08:00" title="2017-04-20 06:28">2017-04-20 06:28</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<h3>Accuracy and Training Set Size</h3>
<p>The larger the training data set the higher to chance of obtaining higher accuracy.  If the training set isn't big enough, accuracy could be low.  However, the accuracy increment by dataset size would plateau off at some point, as adding more data points won't improve accuracy much.  </p>
<p>In real life, there might be cases where datasets are small in size, and it is not possible to attain more data.  But most data projects, it is always good to start with question like this : how does accuracy change with the number of training events especially if we have the power to go out and obtain more data (data points).  </p>
<p>Datasets with incomplete information will also lead to low accuracy.  </p>
<h3>Types of Data</h3>
<ul>
<li>Numerical      </li>
<li>Salary Info     </li>
<li>Number of email sent      </li>
<li>Categorical     </li>
<li>Job title      </li>
<li>Time Series       </li>
<li>Time Stamp on emails      </li>
<li>Text      </li>
<li>Content on emails      </li>
<li>To/From of emails      </li>
<li>Other     </li>
</ul>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/06-supervised_learning-regression/" class="u-url">Supervised Learning - Regression</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-20T06:19:50+08:00" title="2017-04-20 06:19">2017-04-20 06:19</time></span>
        </div>
    </header><div class="e-content entry-content">
    <div>
<p>In <a href="https://en.wikipedia.org/wiki/Statistical_model" title="Statistical Model">statistical modeling</a>, <strong>regression analysis</strong> is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a <a href="https://en.wikipedia.org/wiki/Dependent_variable" title="Dependent Variable">dependent variable</a> and one or more <a href="https://en.wikipedia.org/wiki/Independent_variable" title="Independent Variable">independent variables</a> (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the <a href="https://en.wikipedia.org/wiki/Conditional_expectation" title="Conditional Expectation">conditional expectation</a> of the dependent variable given the independent variables – that is, the <a href="https://en.wikipedia.org/wiki/Average_value" title="Average Value">average value</a> of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a <a href="https://en.wikipedia.org/wiki/Quantile" title="Quantile">quantile</a>, or other <a href="https://en.wikipedia.org/wiki/Location_parameter" title="Location Parameter">location parameter</a> of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the <strong>regression function</strong>. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a <a href="https://en.wikipedia.org/wiki/Probability_distribution" title="Probability Distribution">probability distribution</a>. A related but distinct approach is necessary condition analysis (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is necessary but not sufficient for a given value of the dependent variable.</p>
<p>Regression analysis is widely used for <a href="https://en.wikipedia.org/wiki/Prediction" title="Prediction">prediction</a> and <a href="https://en.wikipedia.org/wiki/Forecasting" title="Forecasting">forecasting</a>, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer <a href="https://en.wikipedia.org/wiki/Causality" title="Casual Relationships">causal relationships</a> between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable;[2] for example, <a href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation" title="Correlation is Not Causation">correlation does not imply causation</a>.</p>
<p>Many techniques for carrying out regression analysis have been developed. Familiar methods such as <a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear Regression">linear regression</a> and <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" title="Ordinary Least Squares">ordinary least squares</a> regression are <a href="https://en.wikipedia.org/wiki/Parametric_statistics" title="Parametric Statistics">parametric</a>, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. <a href="https://en.wikipedia.org/wiki/Nonparametric_regression" title="Non-Parametric Regression">Nonparametric regression</a> refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.</p>
<p>The performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results.</p>
<p>In a narrower sense, regression may refer specifically to the estimation of continuous response variables, as opposed to the discrete response variables used in <a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical Classification">classification</a>. The case of a continuous output variable may be more specifically referred to as <strong>metric regression</strong> to distinguish it from related problems.</p>
<h3>Linear Regression</h3>
<p>In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable \(y\) and one or more explanatory variables (or independent variables) denoted \(X\). The case of one explanatory variable is called <a href="https://en.wikipedia.org/wiki/Simple_linear_regression" title="Simple Linear Regression">simple linear regression</a>. For more than one explanatory variable, the process is called <em>multiple linear regression</em>. (This term is distinct from <a href="https://en.wikipedia.org/wiki/General_linear_model" title="Multivariate Regression">multivariate linear regression</a>, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)</p>
<p>In linear regression, the relationships are modeled using <a href="https://en.wikipedia.org/wiki/Linear_predictor_function" title="Linear Predictor Function">linear predictor functions</a> whose unknown model parameters are <a href="https://en.wikipedia.org/wiki/Estimation_theory" title="Estimation Theory">estimated</a> from the data. Such models are called <a href="https://en.wikipedia.org/wiki/Linear_model" title="Linear Model">linear models</a>. Most commonly, the <a href="https://en.wikipedia.org/wiki/Conditional_expectation" title="Conditional Expectation">conditional</a> mean of \(y\) given the value of \(X\) is assumed to be an <a href="https://en.wikipedia.org/wiki/Affine_transformation" title="Affine Transformation">affine function</a> of \(X\); less commonly, the <a href="https://en.wikipedia.org/wiki/Median" title="Median">median</a> or some other quantile of the conditional distribution of \(y\) given \(X\) is expressed as a linear function of \(X\). Like all forms of regression analysis, linear regression focuses on the <a href="https://en.wikipedia.org/wiki/Conditional_probability_distribution" title="Conditional Probability Distribution">conditional probability distribution</a> of \(y\) given \(X\), rather than on the <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution" title="Joint Probability Distribution">joint probability distribution</a> of \(y\) and \(X\), which is the domain of <a href="https://en.wikipedia.org/wiki/Multivariate_analysis" title="Multivariate Analysis">multivariate analysis</a>.</p>
<p>Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.</p>
<p>Linear regression has many practical uses. Most applications fall into one of the following two broad categories:</p>
<ul>
<li>If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of \(y\) and \(X\) values. After developing such a model, if an additional value of \(X\) is then given without its accompanying value of \(y\), the fitted model can be used to make a prediction of the value of \(y\).         </li>
<li>Given a variable \(y\) and a number of variables \(X_1, \, \ldots, \, X_p \) that may be related to \(y\), linear regression analysis can be applied to quantify the strength of the relationship between \(y\) and the \(X_j\), to assess which \(X_j\) may have no relationship with \(y\) at all, and to identify which subsets of the \(X_j\) contain redundant information about \(y\).        </li>
</ul>
<p>Linear regression models are often fitted using the <a href="https://en.wikipedia.org/wiki/Least_squares" title="Least Square">least squares</a> approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)" title="Mathematical Norm">norm</a> (as with <a href="https://en.wikipedia.org/wiki/Least_absolute_deviations" title="Least Absolute Deviation">least absolute deviations</a> regression), or by minimizing a penalized version of the least squares <a href="https://en.wikipedia.org/wiki/Loss_function" title="Loss Function">loss function</a> as in <a href="https://en.wikipedia.org/wiki/Ridge_regression" title="Ridge Regression">ridge regression</a> (L2-norm penalty) and <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" title="Lasso">lasso</a> (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous.</p>
<h4>Introduction</h4>
<p>Given a data set
\(  \left\{y_{i},   \, x_{i1}, \, \ldots, \,   x_{ip}  \right\} _{i=0}^{n} \)
of  \( n \) statistical units, a linear regression model assumes that the relationship between the dependent variable \( y_{i} \) and the <em>p-vector</em> of regressors \( x_{i} \) is linear. This relationship is modeled through a disturbance term or error variable \( \epsilon_i \)— an unobserved random variable that adds noise to the linear relationship between the dependent variable and regressors.      </p>
<p>Thus the model takes the form
\[ y_{i} = \beta _{0}1 + \beta _{1}x _{i1} + \cdots + \beta _{p} x _{ip} + \varepsilon _{i} = \mathbf{x} _{i}^{ \top }{\boldsymbol{\beta}} + \varepsilon _{i},\qquad i=1,\ldots ,n, <br>
\]  </p>
<p>where T denotes the transpose, so that \( \mathbf{x} _{i}^{ \top }{\boldsymbol{\beta}} \) is the inner product between vectors \( \mathbf{x} _{i} \) and \( \boldsymbol{\beta} \).</p>
<h4>Assumptions</h4>
<p>Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Some methods are general enough that they can relax multiple assumptions at once, and in other cases this can be achieved by combining different extensions. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.       </p>
<p>The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" title="Ordinary Least Squares">ordinary least squares</a>):        </p>
<ul>
<li>
<strong>Weak exogeneity</strong>. This essentially means that the predictor variables x can be treated as fixed values, rather than <a href="https://en.wikipedia.org/wiki/Random_variable" title="Random Variables">random variables</a>. This means, for example, that the predictor variables are assumed to be error-free—that is, not contaminated with measurement errors. Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult <a href="https://en.wikipedia.org/wiki/Errors-in-variables_model" title="Errors in Variable Model">errors-in-variables models</a>.      </li>
<li>
<strong>Linearity</strong>. This means that the mean of the response variable is a <a href="https://en.wikipedia.org/wiki/Linear_combination" title="Linear Combination">linear combination</a> of the parameters (regression coefficients) and the predictor variables. Note that this assumption is much less restrictive than it may at first seem. Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. This trick is used, for example, in <a href="https://en.wikipedia.org/wiki/Polynomial_regression" title="Polynomial Regression">polynomial regression</a>, which uses linear regression to fit the response variable as an arbitrary <a href="https://en.wikipedia.org/wiki/Polynomial" title="Polynomial">polynomial</a> function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method. In fact, models such as polynomial regression are often "too powerful", in that they tend to overfit the data. As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. Common examples are <a href="https://en.wikipedia.org/wiki/Ridge_regression" title="Ridge Regression">ridge regression</a> and <a href="https://en.wikipedia.org/wiki/Lasso_regression" title="Lasso Regression">lasso regression</a>. <a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression" title="Bayesian Linear Regression">Bayesian linear regression</a> can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, <a href="https://en.wikipedia.org/wiki/Ridge_regression" title="Ridge Regression">ridge regression</a> and <a href="https://en.wikipedia.org/wiki/Lasso_regression" title="Lasso Regression">lasso regression</a> can both be viewed as special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)       </li>
<li>
<strong>Constant variance</strong> (a.k.a. <strong><a href="https://en.wikipedia.org/wiki/Homoscedasticity" title="Homoscedasticity">homoscedasticity</a></strong>). This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variables can vary over a wide scale. In order to determine for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a "fanning effect" between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predicting outcome. Error will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be \$100,000 may easily have an actual income of \$80,000 or \$120,000 (a standard deviation of around \$20,000), while another person with a predicted income of \$10,000 is unlikely to have the same \$20,000 standard deviation, which would imply their actual income would vary anywhere between -\$10,000 and \$30,000. (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. <a href="https://en.wikipedia.org/wiki/Weighted_least_squares" title="Weighted Least Squares">weighted least squares</a> and <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors" title="Heteroscedasticity Consistent Standard Errors">heteroscedasticity-consistent standard errors</a>) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).          </li>
<li>
<strong>Independence of errors</strong>. This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. generalized least squares) are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.</li>
<li>
<strong>Lack of multicollinearity</strong> in the predictors. For standard least squares estimation methods, the design matrix \( X \) must have full column rank \( p \); otherwise, we have a condition known as <a href="https://en.wikipedia.org/wiki/Multicollinearity" title="Multicollinearity">multicollinearity</a> in the predictor variables. This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of multicollinearity, the parameter vector \( \beta \) will be non-identifiable—it has no unique solution. At most we will be able to identify some of the parameters, i.e. narrow down its value to some linear subspace of \( R^p \). See partial least squares regression. Methods for fitting linear models with multicollinearity have been developed; some require additional assumptions such as "effect sparsity"—that a large fraction of the effects are exactly zero.      <br>
Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem—and in fact it's quite normal when handling categorically valued predictors to introduce a separate indicator variable predictor for each possible category, which inevitably introduces multicollinearity.</li>
</ul>
<p>Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:   <br>
- The statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.     <br>
- The arrangement, or probability distribution of the predictor variables \( x \) has a major influence on the precision of estimates of \( \beta \). Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of \( \beta \).        </p>
</div>
    </div>
    </article><article class="post h-entry post-text"><header><h1 class="post-title p-name"><a href="course-notes/machine-learning/04-supervised_learning-k_nearest_neighbors/" class="u-url">Supervised Learning - K Nearest Neighbors (KNN)</a></h1>
        <div class="metadata">
            <span class="post-date dateline"><time class="published dt-published" datetime="2017-04-04T14:24:22+08:00" title="2017-04-04 14:24">2017-04-04 14:24</time></span>
        </div>
    </header><div class="e-content entry-content">
    <p>Write your post here.</p>
    </div>
    </article>
</div>
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="." rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-4.html" rel="next">Older posts</a>
            </li>
        </ul></nav><script>var disqus_shortname="chowygit";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>

    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73098247-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
